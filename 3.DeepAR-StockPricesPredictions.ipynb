{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Stock Prices Predictions with Machine Learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Notebook will contain the modeling phases needed to predict stock prices using a deep learning model.\n",
    "The stocks analyzed will be the following:\n",
    "* IBM\n",
    "* AAPL (Apple Inc.)\n",
    "* AMZN (Amazon Inc.)\n",
    "* GOOGL (Alphabet Inc.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data must be prepared in order to be processed by DeepAR model:\n",
    "* Train/test set split\n",
    "* Save Data locally\n",
    "* Upload to S3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Data Locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'stock_deepar'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir_csv = os.path.join(data_dir, 'csv') # The folder we will use for storing data\n",
    "if not os.path.exists(data_dir_csv): # Make sure that the folder exists\n",
    "    os.makedirs(data_dir_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "interval ='D'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IBM\n",
    "df_ibm_train.to_csv(os.path.join(data_dir_csv, 'ibm_train.csv'), header=True, index=True)\n",
    "df_ibm_test.to_csv(os.path.join(data_dir_csv, 'ibm_test.csv'), header=True, index=True)\n",
    "df_ibm_valid.to_csv(os.path.join(data_dir_csv, 'ibm_valid.csv'), header=True, index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apple Inc.\n",
    "df_aapl_train.to_csv(os.path.join(data_dir_csv, 'aapl_train.csv'), header=True, index=True)\n",
    "df_aapl_test.to_csv(os.path.join(data_dir_csv, 'aapl_test.csv'), header=True, index=True)\n",
    "df_aapl_valid.to_csv(os.path.join(data_dir_csv, 'aapl_valid.csv'), header=True, index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Amazon.com\n",
    "df_amzn_train.to_csv(os.path.join(data_dir_csv, 'amzn_train.csv'), header=True, index=True)\n",
    "df_amzn_test.to_csv(os.path.join(data_dir_csv, 'amzn_test.csv'), header=True, index=True)\n",
    "df_amzn_valid.to_csv(os.path.join(data_dir_csv, 'amzn_valid.csv'), header=True, index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alphabet Inc.\n",
    "df_googl_train.to_csv(os.path.join(data_dir_csv, 'googl_train.csv'), header=True, index=True)\n",
    "df_googl_test.to_csv(os.path.join(data_dir_csv, 'googl_test.csv'), header=True, index=True)\n",
    "df_googl_valid.to_csv(os.path.join(data_dir_csv, 'googl_valid.csv'), header=True, index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSON serialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to feed DeepAR model, JSON files must be prepared from data.\n",
    "I'll dispose two kind of JSON inputs:\n",
    "* one with \"dynamic features\", to use a DeepAR API terminology: all dataset features except for target column and related one ('Adj Close', 'Close');\n",
    "* one without \"dynamic features: only 'Adj Close' column will be fed to DeepAR model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSON files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I'm going to convert data to JSON file format, in order to feed the DeepAR model correctly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As already announced, I will create two kind of time series, one with a list of dynamic features `dyn_feat`and the other one with only the target column (`Adj Close`) time series. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing train/test dataframe lists to iterate on them\n",
    "dfs_train = [df_ibm_train, df_aapl_train, df_amzn_train, df_googl_train]\n",
    "dfs_test = [df_ibm_test, df_aapl_test, df_amzn_test, df_googl_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating local storage path:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir_json = os.path.join(data_dir, 'json')\n",
    "if not os.path.exists(data_dir_json): # Make sure that the folder exists\n",
    "    os.makedirs(data_dir_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Serializing data to json files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "from source_deepar.deepar_utils import ts2json_serialize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset with the `Adj Close` time series alone:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir_json_train = os.path.join(data_dir_json, 'train') # The folder we will use for storing data\n",
    "if not os.path.exists(data_dir_json_train): # Make sure that the folder exists\n",
    "    os.makedirs(data_dir_json_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df, m in zip(dfs_train, mnemonics):\n",
    "    ts2json_serialize(df, data_dir_json_train, m+'.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir_json_test = os.path.join(data_dir_json, 'test') # The folder we will use for storing data\n",
    "if not os.path.exists(data_dir_json_test): # Make sure that the folder exists\n",
    "    os.makedirs(data_dir_json_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df, m in zip(dfs_test, mnemonics):\n",
    "    ts2json_serialize(df, data_dir_json_test, m+'.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset containing dynamic features:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir_json_dyn_feat = os.path.join(data_dir_json, 'w_dyn_feat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir_json_train_dyn_feat = os.path.join(data_dir_json_dyn_feat, 'train') # The folder we will use for storing data\n",
    "if not os.path.exists(data_dir_json_train_dyn_feat): # Make sure that the folder exists\n",
    "    os.makedirs(data_dir_json_train_dyn_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df, m in zip(dfs_train, mnemonics):\n",
    "    ts2json_serialize(df, data_dir_json_train_dyn_feat, m+'.json', dyn_feat=['Open'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir_json_test_dyn_feat = os.path.join(data_dir_json_dyn_feat, 'test') # The folder we will use for storing data\n",
    "if not os.path.exists(data_dir_json_test_dyn_feat): # Make sure that the folder exists\n",
    "    os.makedirs(data_dir_json_test_dyn_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df, m in zip(dfs_test, mnemonics):\n",
    "    ts2json_serialize(df, data_dir_json_test_dyn_feat, m+'.json', dyn_feat=['Open'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir_json_test_dyn_feat = os.path.join(data_dir_json_dyn_feat, 'test') # The folder we will use for storing data\n",
    "if not os.path.exists(data_dir_json_test_dyn_feat): # Make sure that the folder exists\n",
    "    os.makedirs(data_dir_json_test_dyn_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df, m in zip(dfs_test, mnemonics):\n",
    "    ts2json_serialize(df, data_dir_json_test_dyn_feat, m+'.json', dyn_feat=['Open'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uploading data to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "# Define IAM role and session\n",
    "role = sagemaker.get_execution_role()\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "#Define training data location\n",
    "s3_data_key = 'stock_deepar/train_artifacts'\n",
    "s3_bucket = sagemaker_session.default_bucket()\n",
    "interval = 'D' #Use D or H\n",
    "s3_output_path = \"s3://{}/{}/{}/output\".format(s3_bucket, s3_data_key, interval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *unique* train/test prefixes\n",
    "train_prefix   = '{}/{}'.format(data_dir_json, 'train')\n",
    "test_prefix    = '{}/{}'.format(data_dir_json, 'test')\n",
    "train_prefix_dyn_feat   = '{}/{}'.format(data_dir_json_dyn_feat, 'train')\n",
    "test_prefix_dyn_feat    = '{}/{}'.format(data_dir_json_dyn_feat, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_train = sagemaker_session.upload_data(path=data_dir_json_train, bucket=s3_bucket, key_prefix=train_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_test = sagemaker_session.upload_data(path=data_dir_json_test, bucket=s3_bucket, key_prefix=test_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_train_dyn_feat = sagemaker_session.upload_data(path=data_dir_json_train, bucket=s3_bucket, key_prefix=train_prefix_dyn_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_test_dyn_feat = sagemaker_session.upload_data(path=data_dir_json_test, bucket=s3_bucket, key_prefix=test_prefix_dyn_feat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set DeepAR specific hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "from source_deepar import deepar_utils\n",
    "\n",
    "# setting target columns\n",
    "target_column = 'Adj Close'\n",
    "    \n",
    "    \n",
    "hyperparameters = {\n",
    "    \"prediction_length\": str(prediction_length[1]), #number of time-steps model is trained to predict, always generates forecasts with this length\n",
    "    \"context_length\": str(context_length[1]), #number of time-points that the model gets to see before making the prediction, should be about same as the prediction_length\n",
    "    \"time_freq\": interval, #granularity of the time series in the dataset\n",
    "    \"epochs\": \"200\", #maximum number of passes over the training data\n",
    "    \"early_stopping_patience\": \"40\", #training stops when no progress is made within the specified number of epochs\n",
    "    \"num_layers\": \"2\", #number of hidden layers in the RNN, typically range from 1 to 4    \n",
    "    \"num_cells\": \"40\", #number of cells to use in each hidden layer of the RNN, typically range from 30 to 100\n",
    "    \"mini_batch_size\": \"128\", #size of mini-batches used during training, typically values range from 32 to 512\n",
    "    \"learning_rate\": \"1e-3\", #learning rate used in training. Typical values range from 1e-4 to 1e-1\n",
    "    \"dropout_rate\": \"0.1\", # dropout rate to use for regularization, typically less than 0.2. \n",
    "    \"likelihood\": \"gaussian\" #noise model used for uncertainty estimates - gaussian/beta/negative-binomial/student-T/deterministic-L1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The method get_image_uri has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "Defaulting to the only supported framework/algorithm version: 1. Ignoring framework/algorithm version: latest.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model will be trained using container image : 495149712605.dkr.ecr.eu-central-1.amazonaws.com/forecasting-deepar:1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define IAM role and session\n",
    "role = sagemaker.get_execution_role()\n",
    "session = sagemaker.Session()\n",
    "\n",
    "#Obtain container image URI for SageMaker-DeepAR algorithm, based on region\n",
    "region = session.boto_region_name\n",
    "image_name = sagemaker.amazon.amazon_estimator.get_image_uri(region, \"forecasting-deepar\", \"latest\")\n",
    "print(\"Model will be trained using container image : {}\".format(image_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimator Instantiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train_instance_count has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "train_instance_type has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.estimator import Estimator\n",
    "\n",
    "# instantiate a DeepAR estimator\n",
    "estimator = Estimator(image_uri=image_name,\n",
    "                      sagemaker_session=sagemaker_session,\n",
    "                      image_name=image_name,\n",
    "                      role=role,\n",
    "                      train_instance_count=1,\n",
    "                      train_instance_type='ml.c4.xlarge',\n",
    "                      output_path=s3_output_path,\n",
    "                      hyperparameters=hyperparameters\n",
    "                      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Job Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creation of a training job with stand alone time series (no dynamic features provided). Run only if no model has already been trained before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# train and test channels\n",
    "data_channels = {\n",
    "    \"train\": input_data_train,\n",
    "    \"test\": input_data_test\n",
    "}\n",
    "\n",
    "# fit the estimator\n",
    "estimator.fit(inputs=data_channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Existing Model Instantiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instatiation of a model from existing training artifacts (run only if a model has already been trained before)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = sagemaker.model.Model(\n",
    "    model_data='{}/{}/model.tar.gz'.format(s3_output_path, 'forecasting-deepar-2021-03-07-20-20-06-397/output'),\n",
    "    image_uri= image_name,\n",
    "    #image=image_name,  # example path for the semantic segmentation in eu-west-1\n",
    "    role=role)  # your role here; could be different name\n",
    "\n",
    "#trainedmodel.deploy(initial_instance_count=1, instance_type='ml.c4.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy and Create a Predictor\n",
    "\n",
    "Now that we have trained a model, we can use it to perform predictions by deploying it to a predictor endpoint.\n",
    "\n",
    "Remember to **delete the endpoint** at the end of this notebook. A cell at the very bottom of this notebook will be provided, but it is always good to keep, front-of-mind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name = 'DeepAR-ml-spp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a predictor\n",
    "\n",
    "from sagemaker.predictor import json_serializer, json_deserializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "update_endpoint is a no-op in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
      "Wall time: 7.87 µs\n",
      "-----------------!"
     ]
    }
   ],
   "source": [
    "# run it once, then update the endpoint\n",
    "%time\n",
    "\n",
    "predictor = estimator.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.t2.medium',\n",
    "    endpoint_name=endpoint_name,\n",
    "    #content_type=\"application/json\" # specify that it will accept/produce JSON\n",
    "    update_endpoint=True,\n",
    "    serializer=json_serializer,\n",
    "    deserializer=json_deserializer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update endpoint if needed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------!CPU times: user 281 ms, sys: 20 ms, total: 301 ms\n",
      "Wall time: 8min 32s\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "\n",
    "# update an endpoint\n",
    "\n",
    "predictor = estimator.update_endpoint(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.t2.medium',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "from source_deepar.deepar_utils import DeepARPredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The class RealTimePredictor has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    }
   ],
   "source": [
    "json_predictor = DeepARPredictor(endpoint_name=endpoint_name, sagemaker_session=sagemaker_session)\n",
    "json_predictor.set_prediction_parameters(interval, prediction_length[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Predictions\n",
    "\n",
    "According to the [inference format](https://docs.aws.amazon.com/sagemaker/latest/dg/deepar-in-formats.html) for DeepAR, the `predictor` expects to see input data in a JSON format, with the following keys:\n",
    "* **instances**: A list of JSON-formatted time series that should be forecast by the model.\n",
    "* **configuration** (optional): A dictionary of configuration information for the type of response desired by the request.\n",
    "\n",
    "Within configuration the following keys can be configured:\n",
    "* **num_samples**: An integer specifying the number of samples that the model generates when making a probabilistic prediction.\n",
    "* **output_types**: A list specifying the type of response. We'll ask for **quantiles**, which look at the list of num_samples generated by the model, and generate [quantile estimates](https://en.wikipedia.org/wiki/Quantile) for each time point based on these values.\n",
    "* **quantiles**: A list that specified which quantiles estimates are generated and returned in the response.\n",
    "\n",
    "\n",
    "Below is an example of what a JSON query to a DeepAR model endpoint might look like.\n",
    "\n",
    "```\n",
    "{\n",
    " \"instances\": [\n",
    "  { \"start\": \"2009-11-01 00:00:00\", \"target\": [4.0, 10.0, 50.0, 100.0, 113.0] },\n",
    "  { \"start\": \"1999-01-30\", \"target\": [2.0, 1.0] }\n",
    " ],\n",
    " \"configuration\": {\n",
    "  \"num_samples\": 50,\n",
    "  \"output_types\": [\"quantiles\"],\n",
    "  \"quantiles\": [\"0.5\", \"0.9\"]\n",
    " }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JSON Prediction Request\n",
    "\n",
    "The code below accepts a **list** of time series as input and some configuration parameters. It then formats that series into a JSON instance and converts the input into an appropriately formatted JSON_input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "from source_deepar.deepar_utils import series_to_json_obj\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_predictor_input(input_ts, target_column='Adj Close', dyn_feat=[], start=None, num_samples=50, quantiles=['0.1', '0.5', '0.9']):\n",
    "    '''Accepts a list of input time series and produces a formatted input.\n",
    "       :input_ts: An list of input time series.\n",
    "       :num_samples: Number of samples to calculate metrics with.\n",
    "       :quantiles: A list of quantiles to return in the predicted output.\n",
    "       :return: The JSON-formatted input.\n",
    "       '''\n",
    "    # request data is made of JSON objects (instances)\n",
    "    # and an output configuration that details the type of data/quantiles we want\n",
    "    \n",
    "    instances = []\n",
    "    for k in range(len(input_ts)):\n",
    "        # get JSON objects for input time series\n",
    "        instances.append(__series_to_json_obj(input_ts[k], target_column=target_column, dyn_feat=dyn_feat, start=start))\n",
    "\n",
    "    # specify the output quantiles and samples\n",
    "    '''configuration = {\"num_samples\": num_samples, \n",
    "                     \"output_types\": [\"quantiles\"], \n",
    "                     \"quantiles\": quantiles}\n",
    "\n",
    "    request_data = {\"instances\": instances, \n",
    "                    \"configuration\": configuration}\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    configuration = {\n",
    "            \"num_samples\": num_samples,\n",
    "            \"output_types\": [\"quantiles\"],\n",
    "            \"quantiles\": quantiles\n",
    "    }\n",
    "\n",
    "    request_data = {\n",
    "            \"instances\": [instances],\n",
    "            \"configuration\": configuration\n",
    "    }\n",
    "    \n",
    "    json_request = json.dumps(request_data)\n",
    "    \n",
    "    return json_request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get a Prediction\n",
    "\n",
    "We can then use this function to get a prediction for a formatted time series!\n",
    "\n",
    "In the next cell, I'm getting an input time series and known target, and passing the formatted input into the predictor endpoint to get a resultant prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__series_to_json_obj' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-210-52035371c673>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# get formatted input time series\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mjson_input_ts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson_predictor_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# get the prediction from the predictor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-209-6f5208150f8d>\u001b[0m in \u001b[0;36mjson_predictor_input\u001b[0;34m(input_ts, target_column, dyn_feat, start, num_samples, quantiles)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;31m# get JSON objects for input time series\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0minstances\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__series_to_json_obj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_column\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_column\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdyn_feat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdyn_feat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# specify the output quantiles and samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name '__series_to_json_obj' is not defined"
     ]
    }
   ],
   "source": [
    "# get all input and target (test) time series\n",
    "# input_ts = [df_ibm_train, df_aapl_train, df_amzn_train, df_googl_train]\n",
    "# target_ts = [df_ibm_test, df_aapl_test, df_amzn_test, df_googl_test]\n",
    "\n",
    "input_ts = [df_ibm_train]\n",
    "target_ts = [df_ibm_test]\n",
    "\n",
    "# get formatted input time series\n",
    "json_input_ts = json_predictor_input(input_ts)\n",
    "\n",
    "# get the prediction from the predictor\n",
    "json_prediction = predictor.predict(json_input_ts)\n",
    "\n",
    "#print(json_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting IBM Stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all input and target (test) time series\n",
    "# input_ts = [df_ibm_train, df_aapl_train, df_amzn_train, df_googl_train]\n",
    "# target_ts = [df_ibm_test, df_aapl_test, df_amzn_test, df_googl_test]\n",
    "\n",
    "input_ts = [df_ibm_train]\n",
    "target_ts = [df_ibm_test]\n",
    "\n",
    "# get the prediction from the predictor\n",
    "json_prediction = json_predictor.predict(input_ts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[                   0.1         0.9         0.5\n",
      "2021-01-07  121.732735  128.752777  125.387764\n",
      "2021-01-08  121.970581  129.155243  125.847977\n",
      "2021-01-09  120.897179  127.914078  124.735115\n",
      "2021-01-10  119.990166  128.461517  124.985794\n",
      "2021-01-11  120.159508  128.049881  124.527077\n",
      "2021-01-12  120.097336  129.601776  125.042023\n",
      "2021-01-13  119.840149  129.563629  124.619591\n",
      "2021-01-14  119.033905  128.656128  123.579094\n",
      "2021-01-15  119.223236  128.350433  123.499306\n",
      "2021-01-16  117.730324  128.371826  123.341614\n",
      "2021-01-17  117.874954  129.328674  123.687904\n",
      "2021-01-18  118.195808  130.068359  123.000916\n",
      "2021-01-19  118.532639  129.857071  123.103737\n",
      "2021-01-20  118.725182  131.174072  123.732292\n",
      "2021-01-21  117.408356  129.416428  123.031570\n",
      "2021-01-22  116.937096  130.180725  123.215073\n",
      "2021-01-23  117.220276  128.982590  122.722649\n",
      "2021-01-24  117.340263  130.219025  122.956421\n",
      "2021-01-25  117.912949  129.854523  123.160736\n",
      "2021-01-26  117.112442  129.628860  122.515289]\n"
     ]
    }
   ],
   "source": [
    "print(json_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ground truth:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date\n",
      "2021-01-07    127.289062\n",
      "2021-01-08    126.835121\n",
      "2021-01-11    126.884468\n",
      "2021-01-12    127.506165\n",
      "2021-01-13    125.246353\n",
      "2021-01-14    127.269318\n",
      "2021-01-15    126.696968\n",
      "2021-01-19    127.318665\n",
      "2021-01-20    128.364685\n",
      "2021-01-21    129.913971\n",
      "2021-01-22    117.045937\n",
      "2021-01-25    117.016335\n",
      "2021-01-26    120.874763\n",
      "2021-01-27    120.855034\n",
      "2021-01-28    118.496552\n",
      "2021-01-29    117.539337\n",
      "2021-02-01    118.950485\n",
      "2021-02-02    117.864990\n",
      "2021-02-03    117.549210\n",
      "2021-02-04    119.424149\n",
      "Name: Adj Close, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(df_ibm_test.iloc[-(prediction_length[1]):]['Adj Close'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting Apple Stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all input and target (test) time series\n",
    "# input_ts = [df_ibm_train, df_aapl_train, df_amzn_train, df_googl_train]\n",
    "# target_ts = [df_ibm_test, df_aapl_test, df_amzn_test, df_googl_test]\n",
    "\n",
    "input_ts = [df_aapl_train]\n",
    "target_ts = [df_aapl_test]\n",
    "\n",
    "# get the prediction from the predictor\n",
    "json_prediction = json_predictor.predict(input_ts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[                   0.1         0.9         0.5\n",
      "2021-01-07  124.966141  133.710434  129.518921\n",
      "2021-01-08  124.867119  133.773071  129.647171\n",
      "2021-01-09  124.385757  133.364563  129.237198\n",
      "2021-01-10  123.272110  134.095047  129.878265\n",
      "2021-01-11  124.112465  133.994354  129.640106\n",
      "2021-01-12  124.415009  136.922028  130.901855\n",
      "2021-01-13  124.572411  136.825378  130.627472\n",
      "2021-01-14  124.404770  136.993866  130.291321\n",
      "2021-01-15  124.076569  136.886169  130.670990\n",
      "2021-01-16  124.076393  138.153107  131.160660\n",
      "2021-01-17  123.285065  139.048508  131.765427\n",
      "2021-01-18  124.370094  140.550064  130.926865\n",
      "2021-01-19  124.946037  140.130737  130.881851\n",
      "2021-01-20  125.189308  143.258240  132.550827\n",
      "2021-01-21  123.472794  139.811050  131.557358\n",
      "2021-01-22  123.124748  141.913605  131.499939\n",
      "2021-01-23  123.020920  140.186432  131.297318\n",
      "2021-01-24  123.628662  141.317886  131.838501\n",
      "2021-01-25  124.427841  141.728821  131.928314\n",
      "2021-01-26  123.563614  140.581131  130.547806]\n"
     ]
    }
   ],
   "source": [
    "print(json_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ground truth:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date\n",
      "2021-01-07    130.724655\n",
      "2021-01-08    131.852966\n",
      "2021-01-11    128.787552\n",
      "2021-01-12    128.607819\n",
      "2021-01-13    130.694702\n",
      "2021-01-14    128.717667\n",
      "2021-01-15    126.950294\n",
      "2021-01-19    127.639267\n",
      "2021-01-20    131.832993\n",
      "2021-01-21    136.665771\n",
      "2021-01-22    138.862503\n",
      "2021-01-25    142.706757\n",
      "2021-01-26    142.946396\n",
      "2021-01-27    141.848038\n",
      "2021-01-28    136.885452\n",
      "2021-01-29    131.763107\n",
      "2021-02-01    133.939850\n",
      "2021-02-02    134.788589\n",
      "2021-02-03    133.740158\n",
      "2021-02-04    137.184998\n",
      "Name: Adj Close, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(df_aapl_test.iloc[-(prediction_length[1]):]['Adj Close'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting Amazon.com Stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all input and target (test) time series\n",
    "# input_ts = [df_ibm_train, df_aapl_train, df_amzn_train, df_googl_train]\n",
    "# target_ts = [df_ibm_test, df_aapl_test, df_amzn_test, df_googl_test]\n",
    "\n",
    "input_ts = [df_amzn_train]\n",
    "target_ts = [df_amzn_test]\n",
    "\n",
    "# get the prediction from the predictor\n",
    "json_prediction = json_predictor.predict(input_ts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[                    0.1          0.9          0.5\n",
      "2021-01-07  3103.875244  3255.646973  3183.352783\n",
      "2021-01-08  3110.760742  3264.066895  3173.780029\n",
      "2021-01-09  3069.394775  3263.605957  3176.585938\n",
      "2021-01-10  3092.879883  3275.094971  3180.134033\n",
      "2021-01-11  3082.147461  3280.021729  3184.632812\n",
      "2021-01-12  3102.078125  3302.175781  3204.151123\n",
      "2021-01-13  3086.716309  3294.481445  3196.488770\n",
      "2021-01-14  3105.305908  3311.101074  3207.810059\n",
      "2021-01-15  3096.487305  3314.914551  3200.523926\n",
      "2021-01-16  3106.058105  3331.040771  3217.055420\n",
      "2021-01-17  3134.722656  3364.406738  3226.018311\n",
      "2021-01-18  3088.201660  3335.793457  3208.609375\n",
      "2021-01-19  3115.896484  3347.588135  3238.604248\n",
      "2021-01-20  3116.237061  3363.819824  3226.713379\n",
      "2021-01-21  3105.863281  3385.596436  3241.212646\n",
      "2021-01-22  3106.853271  3377.485840  3250.418213\n",
      "2021-01-23  3085.061279  3390.304688  3255.830566\n",
      "2021-01-24  3090.060791  3393.720215  3255.502930\n",
      "2021-01-25  3096.748535  3376.555420  3239.009033\n",
      "2021-01-26  3095.596680  3384.260498  3238.775391]\n"
     ]
    }
   ],
   "source": [
    "print(json_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ground truth:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date\n",
      "2021-01-07    3162.159912\n",
      "2021-01-08    3182.699951\n",
      "2021-01-11    3114.209961\n",
      "2021-01-12    3120.830078\n",
      "2021-01-13    3165.889893\n",
      "2021-01-14    3127.469971\n",
      "2021-01-15    3104.250000\n",
      "2021-01-19    3120.760010\n",
      "2021-01-20    3263.379883\n",
      "2021-01-21    3306.989990\n",
      "2021-01-22    3292.229980\n",
      "2021-01-25    3294.000000\n",
      "2021-01-26    3326.129883\n",
      "2021-01-27    3232.580078\n",
      "2021-01-28    3237.620117\n",
      "2021-01-29    3206.199951\n",
      "2021-02-01    3342.879883\n",
      "2021-02-02    3380.000000\n",
      "2021-02-03    3312.530029\n",
      "2021-02-04    3331.000000\n",
      "Name: Adj Close, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(df_amzn_test.iloc[-(prediction_length[1]):]['Adj Close'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting Alphabet Inc. Stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all input and target (test) time series\n",
    "# input_ts = [df_ibm_train, df_aapl_train, df_amzn_train, df_googl_train]\n",
    "# target_ts = [df_ibm_test, df_aapl_test, df_amzn_test, df_googl_test]\n",
    "\n",
    "input_ts = [df_googl_train]\n",
    "target_ts = [df_googl_test]\n",
    "\n",
    "# get the prediction from the predictor\n",
    "json_prediction = json_predictor.predict(input_ts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[                    0.1          0.9          0.5\n",
      "2021-01-07  1679.881470  1763.668213  1723.757568\n",
      "2021-01-08  1684.970093  1769.465698  1719.822632\n",
      "2021-01-09  1659.624878  1767.636597  1720.335693\n",
      "2021-01-10  1678.443115  1780.066162  1726.919434\n",
      "2021-01-11  1668.081299  1780.199463  1725.360107\n",
      "2021-01-12  1679.628540  1790.985962  1736.021240\n",
      "2021-01-13  1668.636108  1784.489014  1732.065796\n",
      "2021-01-14  1674.419678  1790.535400  1731.337158\n",
      "2021-01-15  1667.183228  1792.080200  1726.906128\n",
      "2021-01-16  1671.154297  1798.102905  1732.709717\n",
      "2021-01-17  1692.093384  1824.358154  1742.937134\n",
      "2021-01-18  1670.530518  1810.012817  1737.217773\n",
      "2021-01-19  1687.705811  1820.018921  1757.901855\n",
      "2021-01-20  1689.093750  1829.603882  1751.361206\n",
      "2021-01-21  1681.372559  1843.392700  1761.610718\n",
      "2021-01-22  1690.414062  1843.050659  1773.397705\n",
      "2021-01-23  1680.970825  1855.756714  1781.634155\n",
      "2021-01-24  1692.762085  1867.024048  1786.429688\n",
      "2021-01-25  1696.905029  1861.865112  1783.089966\n",
      "2021-01-26  1700.337891  1870.259399  1786.662964]\n"
     ]
    }
   ],
   "source": [
    "print(json_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ground truth:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date\n",
      "2021-01-07    1774.339966\n",
      "2021-01-08    1797.829956\n",
      "2021-01-11    1756.290039\n",
      "2021-01-12    1737.430054\n",
      "2021-01-13    1747.250000\n",
      "2021-01-14    1730.920044\n",
      "2021-01-15    1727.619995\n",
      "2021-01-19    1784.469971\n",
      "2021-01-20    1880.069946\n",
      "2021-01-21    1884.150024\n",
      "2021-01-22    1892.560059\n",
      "2021-01-25    1894.280029\n",
      "2021-01-26    1907.949951\n",
      "2021-01-27    1818.939941\n",
      "2021-01-28    1853.199951\n",
      "2021-01-29    1827.359985\n",
      "2021-02-01    1893.069946\n",
      "2021-02-02    1919.119995\n",
      "2021-02-03    2058.879883\n",
      "2021-02-04    2053.629883\n",
      "Name: Adj Close, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(df_googl_test.iloc[-(prediction_length[1]):]['Adj Close'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get a Prediction\n",
    "\n",
    "We can then use this function to get a prediction for a formatted time series!\n",
    "\n",
    "In the next cell, I'm getting an input time series and known target, and passing the formatted input into the predictor endpoint to get a resultant prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date\n",
       "2019-12-27    126.834549\n",
       "2019-12-30    124.527962\n",
       "2019-12-31    125.681244\n",
       "2020-01-02    126.975204\n",
       "2020-01-03    125.962540\n",
       "Name: Adj Close, dtype: float64"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ibm_valid['Adj Close'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all input and target (test) time series\n",
    "input_ts = df_ibm_valid['Adj Close']\n",
    "target_ts = time_series\n",
    "\n",
    "# get formatted input time series\n",
    "json_input_ts = json_predictor_input(input_ts)\n",
    "\n",
    "# get the prediction from the predictor\n",
    "json_prediction = predictor.predict(json_input_ts)\n",
    "\n",
    "#print(json_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete the Endpoint\n",
    "\n",
    "Try your code out on different time series. You may want to tweak your DeepAR hyperparameters and see if you can improve the performance of this predictor.\n",
    "\n",
    "When you're done with evaluating the predictor (any predictor), make sure to delete the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: delete the endpoint\n",
    "predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
