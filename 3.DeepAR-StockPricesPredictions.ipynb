{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Stock Prices Predictions with Machine Learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Notebook will contain the modeling phases needed to predict stock prices using a deep learning model.\n",
    "The stocks analyzed will be the following:\n",
    "* IBM\n",
    "* AAPL (Apple Inc.)\n",
    "* AMZN (Amazon Inc.)\n",
    "* GOOGL (Alphabet Inc.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data must be prepared in order to be processed by DeepAR model:\n",
    "* Train/test set split\n",
    "* Save Data locally\n",
    "* Upload to S3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Data Locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'stock_deepar'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir_csv = os.path.join(data_dir, 'csv') # The folder we will use for storing data\n",
    "if not os.path.exists(data_dir_csv): # Make sure that the folder exists\n",
    "    os.makedirs(data_dir_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "interval ='D'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IBM\n",
    "df_ibm_train.to_csv(os.path.join(data_dir_csv, 'ibm_train.csv'), header=True, index=True)\n",
    "df_ibm_test.to_csv(os.path.join(data_dir_csv, 'ibm_test.csv'), header=True, index=True)\n",
    "df_ibm_valid.to_csv(os.path.join(data_dir_csv, 'ibm_valid.csv'), header=True, index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apple Inc.\n",
    "df_aapl_train.to_csv(os.path.join(data_dir_csv, 'aapl_train.csv'), header=True, index=True)\n",
    "df_aapl_test.to_csv(os.path.join(data_dir_csv, 'aapl_test.csv'), header=True, index=True)\n",
    "df_aapl_valid.to_csv(os.path.join(data_dir_csv, 'aapl_valid.csv'), header=True, index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Amazon.com\n",
    "df_amzn_train.to_csv(os.path.join(data_dir_csv, 'amzn_train.csv'), header=True, index=True)\n",
    "df_amzn_test.to_csv(os.path.join(data_dir_csv, 'amzn_test.csv'), header=True, index=True)\n",
    "df_amzn_valid.to_csv(os.path.join(data_dir_csv, 'amzn_valid.csv'), header=True, index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alphabet Inc.\n",
    "df_googl_train.to_csv(os.path.join(data_dir_csv, 'googl_train.csv'), header=True, index=True)\n",
    "df_googl_test.to_csv(os.path.join(data_dir_csv, 'googl_test.csv'), header=True, index=True)\n",
    "df_googl_valid.to_csv(os.path.join(data_dir_csv, 'googl_valid.csv'), header=True, index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSON serialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to feed DeepAR model, JSON files must be prepared from data.\n",
    "I'll dispose two kind of JSON inputs:\n",
    "* one with \"dynamic features\", to use a DeepAR API terminology: all dataset features except for target column and related one ('Adj Close', 'Close');\n",
    "* one without \"dynamic features: only 'Adj Close' column will be fed to DeepAR model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSON files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I'm going to convert data to JSON file format, in order to feed the DeepAR model correctly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As already announced, I will create two kind of time series, one with a list of dynamic features `dyn_feat`and the other one with only the target column (`Adj Close`) time series. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing train/test dataframe lists to iterate on them\n",
    "dfs_train = [df_ibm_train, df_aapl_train, df_amzn_train, df_googl_train]\n",
    "dfs_test = [df_ibm_test, df_aapl_test, df_amzn_test, df_googl_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating local storage path:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir_json = os.path.join(data_dir, 'json')\n",
    "if not os.path.exists(data_dir_json): # Make sure that the folder exists\n",
    "    os.makedirs(data_dir_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Serializing data to json files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "from source_deepar.deepar_utils import ts2json_serialize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset with the `Adj Close` time series alone:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir_json_train = os.path.join(data_dir_json, 'train') # The folder we will use for storing data\n",
    "if not os.path.exists(data_dir_json_train): # Make sure that the folder exists\n",
    "    os.makedirs(data_dir_json_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df, m in zip(dfs_train, mnemonics):\n",
    "    ts2json_serialize(df, data_dir_json_train, m+'.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir_json_test = os.path.join(data_dir_json, 'test') # The folder we will use for storing data\n",
    "if not os.path.exists(data_dir_json_test): # Make sure that the folder exists\n",
    "    os.makedirs(data_dir_json_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df, m in zip(dfs_test, mnemonics):\n",
    "    ts2json_serialize(df, data_dir_json_test, m+'.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset containing dynamic features:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir_json_dyn_feat = os.path.join(data_dir_json, 'w_dyn_feat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir_json_train_dyn_feat = os.path.join(data_dir_json_dyn_feat, 'train') # The folder we will use for storing data\n",
    "if not os.path.exists(data_dir_json_train_dyn_feat): # Make sure that the folder exists\n",
    "    os.makedirs(data_dir_json_train_dyn_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df, m in zip(dfs_train, mnemonics):\n",
    "    ts2json_serialize(df, data_dir_json_train_dyn_feat, m+'.json', dyn_feat=['Open'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir_json_test_dyn_feat = os.path.join(data_dir_json_dyn_feat, 'test') # The folder we will use for storing data\n",
    "if not os.path.exists(data_dir_json_test_dyn_feat): # Make sure that the folder exists\n",
    "    os.makedirs(data_dir_json_test_dyn_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df, m in zip(dfs_test, mnemonics):\n",
    "    ts2json_serialize(df, data_dir_json_test_dyn_feat, m+'.json', dyn_feat=['Open'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir_json_test_dyn_feat = os.path.join(data_dir_json_dyn_feat, 'test') # The folder we will use for storing data\n",
    "if not os.path.exists(data_dir_json_test_dyn_feat): # Make sure that the folder exists\n",
    "    os.makedirs(data_dir_json_test_dyn_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df, m in zip(dfs_test, mnemonics):\n",
    "    ts2json_serialize(df, data_dir_json_test_dyn_feat, m+'.json', dyn_feat=['Open'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uploading data to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "# Define IAM role and session\n",
    "role = sagemaker.get_execution_role()\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "#Define training data location\n",
    "s3_data_key = 'stock_deepar/train_artifacts'\n",
    "s3_bucket = sagemaker_session.default_bucket()\n",
    "interval = 'D' #Use D or H\n",
    "s3_output_path = \"s3://{}/{}/{}/output\".format(s3_bucket, s3_data_key, interval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *unique* train/test prefixes\n",
    "train_prefix   = '{}/{}'.format(data_dir_json, 'train')\n",
    "test_prefix    = '{}/{}'.format(data_dir_json, 'test')\n",
    "train_prefix_dyn_feat   = '{}/{}'.format(data_dir_json_dyn_feat, 'train')\n",
    "test_prefix_dyn_feat    = '{}/{}'.format(data_dir_json_dyn_feat, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_train = sagemaker_session.upload_data(path=data_dir_json_train, bucket=s3_bucket, key_prefix=train_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_test = sagemaker_session.upload_data(path=data_dir_json_test, bucket=s3_bucket, key_prefix=test_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_train_dyn_feat = sagemaker_session.upload_data(path=data_dir_json_train, bucket=s3_bucket, key_prefix=train_prefix_dyn_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_test_dyn_feat = sagemaker_session.upload_data(path=data_dir_json_test, bucket=s3_bucket, key_prefix=test_prefix_dyn_feat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DeepAR is the model of choice of this project.\n",
    "This model expects input data to be already test-train splitted.\n",
    "A big part of the model design has to be done looking close at data.\n",
    "More specifically, defining these two hyperparameters about the data:\n",
    "* Context length\n",
    "* Prediction length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the length of the time series future predictions in days. It will be conveniently set to 5 days (exactly a week of trading hours) because a shorter interval would be of little significance.\n",
    "A longer interval could be interesting from an application point of view, but it can be challenging in terms of model performances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Context length can be either:\n",
    "* designed on patterns or seasonality observed in the data, if any is present;\n",
    "* chosen as a fixed value. This will be my choice, and it will be the same as the moving average window, in order to have a good reference metrics, applicable to both this model and the benchmark model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To explore this second option, we will refer to what we've found during the EDA stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ncovariate_columns = list(df_ibm.columns)\\ncovariate_columns.remove('Close')\\ncovariate_columns.remove('Adj Close')\\n\""
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "covariate_columns = list(df_ibm.columns)\n",
    "covariate_columns.remove('Close')\n",
    "covariate_columns.remove('Adj Close')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "from source_deepar import deepar_utils\n",
    "\n",
    "# setting target columns\n",
    "target_column = 'Adj Close'\n",
    "\n",
    "# retrieving covariate columns\n",
    "'''covariate_columns = list(df_ibm.columns)\n",
    "covariate_columns.remove('Close')\n",
    "covariate_columns.remove('Adj Close')\n",
    "'''\n",
    "train_test_split = 0.9\n",
    "num_test_windows = 4\n",
    "    \n",
    "    \n",
    "hyperparameters = {\n",
    "    \"prediction_length\": str(prediction_length[0]), #number of time-steps model is trained to predict, always generates forecasts with this length\n",
    "    \"context_length\": str(context_length[0]), #number of time-points that the model gets to see before making the prediction, should be about same as the prediction_length\n",
    "    \"time_freq\": interval, #granularity of the time series in the dataset\n",
    "    \"epochs\": \"200\", #maximum number of passes over the training data\n",
    "    \"early_stopping_patience\": \"40\", #training stops when no progress is made within the specified number of epochs\n",
    "    \"num_layers\": \"2\", #number of hidden layers in the RNN, typically range from 1 to 4    \n",
    "    \"num_cells\": \"40\", #number of cells to use in each hidden layer of the RNN, typically range from 30 to 100\n",
    "    \"mini_batch_size\": \"128\", #size of mini-batches used during training, typically values range from 32 to 512\n",
    "    \"learning_rate\": \"1e-3\", #learning rate used in training. Typical values range from 1e-4 to 1e-1\n",
    "    \"dropout_rate\": \"0.1\", # dropout rate to use for regularization, typically less than 0.2. \n",
    "    \"likelihood\": \"gaussian\" #noise model used for uncertainty estimates - gaussian/beta/negative-binomial/student-T/deterministic-L1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The method get_image_uri has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "Defaulting to the only supported framework/algorithm version: 1. Ignoring framework/algorithm version: latest.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model will be trained using container image : 495149712605.dkr.ecr.eu-central-1.amazonaws.com/forecasting-deepar:1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define IAM role and session\n",
    "role = sagemaker.get_execution_role()\n",
    "session = sagemaker.Session()\n",
    "\n",
    "#Obtain container image URI for SageMaker-DeepAR algorithm, based on region\n",
    "region = session.boto_region_name\n",
    "image_name = sagemaker.amazon.amazon_estimator.get_image_uri(region, \"forecasting-deepar\", \"latest\")\n",
    "print(\"Model will be trained using container image : {}\".format(image_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimator Instantiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train_instance_count has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "train_instance_type has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.estimator import Estimator\n",
    "\n",
    "# instantiate a DeepAR estimator\n",
    "estimator = Estimator(image_uri=image_name,\n",
    "                      sagemaker_session=sagemaker_session,\n",
    "                      image_name=image_name,\n",
    "                      role=role,\n",
    "                      train_instance_count=1,\n",
    "                      train_instance_type='ml.c4.xlarge',\n",
    "                      output_path=s3_output_path,\n",
    "                      hyperparameters=hyperparameters\n",
    "                      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Job Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creation of a training job with stand alone time series (no dynamic features provided)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-06 21:15:30 Starting - Starting the training job...\n",
      "2021-03-06 21:15:54 Starting - Launching requested ML instancesProfilerReport-1615065330: InProgress\n",
      "......\n",
      "2021-03-06 21:16:54 Starting - Preparing the instances for training.........\n",
      "2021-03-06 21:18:22 Downloading - Downloading input data\n",
      "2021-03-06 21:18:22 Training - Downloading the training image..\u001b[34mArguments: train\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:18:40 INFO 139741071263552] Reading default configuration from /opt/amazon/lib/python2.7/site-packages/algorithm/resources/default-input.json: {u'num_dynamic_feat': u'auto', u'dropout_rate': u'0.10', u'mini_batch_size': u'128', u'test_quantiles': u'[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]', u'_tuning_objective_metric': u'', u'_num_gpus': u'auto', u'num_eval_samples': u'100', u'learning_rate': u'0.001', u'num_cells': u'40', u'num_layers': u'2', u'embedding_dimension': u'10', u'_kvstore': u'auto', u'_num_kv_servers': u'auto', u'cardinality': u'auto', u'likelihood': u'student-t', u'early_stopping_patience': u''}\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:18:40 INFO 139741071263552] Reading provided configuration from /opt/ml/input/config/hyperparameters.json: {u'dropout_rate': u'0.1', u'learning_rate': u'1e-3', u'num_cells': u'40', u'prediction_length': u'5', u'epochs': u'200', u'time_freq': u'D', u'context_length': u'10', u'num_layers': u'2', u'mini_batch_size': u'128', u'likelihood': u'gaussian', u'early_stopping_patience': u'40'}\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:18:40 INFO 139741071263552] Final configuration: {u'dropout_rate': u'0.1', u'test_quantiles': u'[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]', u'_tuning_objective_metric': u'', u'num_eval_samples': u'100', u'learning_rate': u'1e-3', u'num_layers': u'2', u'epochs': u'200', u'embedding_dimension': u'10', u'num_cells': u'40', u'_num_kv_servers': u'auto', u'mini_batch_size': u'128', u'likelihood': u'gaussian', u'num_dynamic_feat': u'auto', u'cardinality': u'auto', u'_num_gpus': u'auto', u'prediction_length': u'5', u'time_freq': u'D', u'context_length': u'10', u'_kvstore': u'auto', u'early_stopping_patience': u'40'}\u001b[0m\n",
      "\u001b[34mProcess 1 is a worker.\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:18:40 INFO 139741071263552] Detected entry point for worker worker\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:18:40 INFO 139741071263552] Using early stopping with patience 40\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:18:40 INFO 139741071263552] [cardinality=auto] `cat` field was NOT found in the file `/opt/ml/input/data/train/.ipynb_checkpoints/AAPL-checkpoint.json` and will NOT be used for training.\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:18:40 INFO 139741071263552] [num_dynamic_feat=auto] `dynamic_feat` field was NOT found in the file `/opt/ml/input/data/train/.ipynb_checkpoints/AAPL-checkpoint.json` and will NOT be used for training.\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:18:40 INFO 139741071263552] Training set statistics:\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:18:40 INFO 139741071263552] Real time series\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:18:40 INFO 139741071263552] number of time series: 8\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:18:40 INFO 139741071263552] number of observations: 63344\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:18:40 INFO 139741071263552] mean target length: 7918\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:18:40 INFO 139741071263552] min/mean/max target: 0.0386135652661/111.688109026/2039.51000977\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:18:40 INFO 139741071263552] mean abs(target): 111.688109026\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:18:40 INFO 139741071263552] contains missing values: no\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:18:40 INFO 139741071263552] Small number of time series. Doing 160 passes over dataset with prob 1.0 per epoch.\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:18:40 INFO 139741071263552] Test set statistics:\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:18:40 INFO 139741071263552] Real time series\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:18:40 INFO 139741071263552] number of time series: 6\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:18:40 INFO 139741071263552] number of observations: 4338\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:18:40 INFO 139741071263552] mean target length: 723\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:18:40 INFO 139741071263552] min/mean/max target: 32.4180755615/509.281847914/3531.44995117\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:18:40 INFO 139741071263552] mean abs(target): 509.281847914\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:18:40 INFO 139741071263552] contains missing values: no\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:18:40 INFO 139741071263552] nvidia-smi took: 0.0252649784088 secs to identify 0 gpus\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:18:40 INFO 139741071263552] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:18:40 INFO 139741071263552] Create Store: local\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"get_graph.time\": {\"count\": 1, \"max\": 46.62299156188965, \"sum\": 46.62299156188965, \"min\": 46.62299156188965}}, \"EndTime\": 1615065520.32181, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1615065520.274339}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:18:40 INFO 139741071263552] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"initialize.time\": {\"count\": 1, \"max\": 117.72012710571289, \"sum\": 117.72012710571289, \"min\": 117.72012710571289}}, \"EndTime\": 1615065520.392198, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1615065520.321881}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:18:42 INFO 139741071263552] Epoch[0] Batch[0] avg_epoch_loss=6.206621\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:18:42 INFO 139741071263552] #quality_metric: host=algo-1, epoch=0, batch=0 train loss <loss>=6.20662069321\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:18:42 INFO 139741071263552] Epoch[0] Batch[5] avg_epoch_loss=4.894959\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:18:42 INFO 139741071263552] #quality_metric: host=algo-1, epoch=0, batch=5 train loss <loss>=4.89495929082\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:18:42 INFO 139741071263552] Epoch[0] Batch [5]#011Speed: 3028.88 samples/sec#011loss=4.894959\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:18:42 INFO 139741071263552] processed a total of 1264 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"epochs\": {\"count\": 1, \"max\": 200, \"sum\": 200.0, \"min\": 200}, \"update.time\": {\"count\": 1, \"max\": 2157.4909687042236, \"sum\": 2157.4909687042236, \"min\": 2157.4909687042236}}, \"EndTime\": 1615065522.549887, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1615065520.392289}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:18:42 INFO 139741071263552] #throughput_metric: host=algo-1, train throughput=585.82959604 records/second\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:18:42 INFO 139741071263552] #progress_metric: host=algo-1, completed 0 % of epochs\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:18:42 INFO 139741071263552] #quality_metric: host=algo-1, epoch=0, train loss <loss>=4.46551446915\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:18:42 INFO 139741071263552] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:18:42 INFO 139741071263552] Saved checkpoint to \"/opt/ml/model/state_e576524f-8ebe-4401-a31f-0adae683cafc-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 10.337114334106445, \"sum\": 10.337114334106445, \"min\": 10.337114334106445}}, \"EndTime\": 1615065522.560994, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1615065522.549976}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:18:44 INFO 139741071263552] Epoch[1] Batch[0] avg_epoch_loss=4.027817\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:18:44 INFO 139741071263552] #quality_metric: host=algo-1, epoch=1, batch=0 train loss <loss>=4.0278172493\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:18:44 INFO 139741071263552] Epoch[1] Batch[5] avg_epoch_loss=3.724548\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:18:44 INFO 139741071263552] #quality_metric: host=algo-1, epoch=1, batch=5 train loss <loss>=3.72454841932\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:18:44 INFO 139741071263552] Epoch[1] Batch [5]#011Speed: 2828.08 samples/sec#011loss=3.724548\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:18:44 INFO 139741071263552] processed a total of 1266 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2292.6158905029297, \"sum\": 2292.6158905029297, \"min\": 2292.6158905029297}}, \"EndTime\": 1615065524.853727, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1615065522.561049}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:18:44 INFO 139741071263552] #throughput_metric: host=algo-1, train throughput=552.178234773 records/second\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:18:44 INFO 139741071263552] #progress_metric: host=algo-1, completed 1 % of epochs\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:18:44 INFO 139741071263552] #quality_metric: host=algo-1, epoch=1, train loss <loss>=3.59128820896\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:18:44 INFO 139741071263552] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:18:44 INFO 139741071263552] Saved checkpoint to \"/opt/ml/model/state_dbed6064-e434-43a5-8fde-a15f2e4d92d7-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 11.606931686401367, \"sum\": 11.606931686401367, \"min\": 11.606931686401367}}, \"EndTime\": 1615065524.865946, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1615065524.853811}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:18:46 INFO 139741071263552] Epoch[2] Batch[0] avg_epoch_loss=3.163183\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:18:46 INFO 139741071263552] #quality_metric: host=algo-1, epoch=2, batch=0 train loss <loss>=3.16318297386\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:18:46 INFO 139741071263552] Epoch[2] Batch[5] avg_epoch_loss=3.198915\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:18:46 INFO 139741071263552] #quality_metric: host=algo-1, epoch=2, batch=5 train loss <loss>=3.19891520341\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:18:46 INFO 139741071263552] Epoch[2] Batch [5]#011Speed: 2952.06 samples/sec#011loss=3.198915\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:18:46 INFO 139741071263552] Epoch[2] Batch[10] avg_epoch_loss=2.778717\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:18:46 INFO 139741071263552] #quality_metric: host=algo-1, epoch=2, batch=10 train loss <loss>=2.27447822094\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:18:46 INFO 139741071263552] Epoch[2] Batch [10]#011Speed: 2935.12 samples/sec#011loss=2.274478\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:18:46 INFO 139741071263552] processed a total of 1285 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2029.5310020446777, \"sum\": 2029.5310020446777, \"min\": 2029.5310020446777}}, \"EndTime\": 1615065526.89561, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1615065524.866004}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:18:46 INFO 139741071263552] #throughput_metric: host=algo-1, train throughput=633.112827861 records/second\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:18:46 INFO 139741071263552] #progress_metric: host=algo-1, completed 1 % of epochs\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:18:46 INFO 139741071263552] #quality_metric: host=algo-1, epoch=2, train loss <loss>=2.77871657502\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:18:46 INFO 139741071263552] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:18:46 INFO 139741071263552] Saved checkpoint to \"/opt/ml/model/state_52ee91cb-0a0c-43df-8204-845673b61d21-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 7.701873779296875, \"sum\": 7.701873779296875, \"min\": 7.701873779296875}}, \"EndTime\": 1615065526.903981, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1615065526.895694}\n",
      "\u001b[0m\n",
      "\n",
      "2021-03-06 21:18:56 Training - Training image download completed. Training in progress.\u001b[34m[03/06/2021 21:18:48 INFO 139741071263552] Epoch[3] Batch[0] avg_epoch_loss=2.830589\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:18:48 INFO 139741071263552] #quality_metric: host=algo-1, epoch=3, batch=0 train loss <loss>=2.8305888176\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:18:48 INFO 139741071263552] Epoch[3] Batch[5] avg_epoch_loss=2.720482\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:18:48 INFO 139741071263552] #quality_metric: host=algo-1, epoch=3, batch=5 train loss <loss>=2.72048163414\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:18:48 INFO 139741071263552] Epoch[3] Batch [5]#011Speed: 2947.95 samples/sec#011loss=2.720482\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:18:48 INFO 139741071263552] processed a total of 1253 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1976.0611057281494, \"sum\": 1976.0611057281494, \"min\": 1976.0611057281494}}, \"EndTime\": 1615065528.880203, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1615065526.904043}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:18:48 INFO 139741071263552] #throughput_metric: host=algo-1, train throughput=634.036460098 records/second\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:18:48 INFO 139741071263552] #progress_metric: host=algo-1, completed 2 % of epochs\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:18:48 INFO 139741071263552] #quality_metric: host=algo-1, epoch=3, train loss <loss>=2.61310516596\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:18:48 INFO 139741071263552] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:18:48 INFO 139741071263552] Saved checkpoint to \"/opt/ml/model/state_17cb7c53-3d86-4d7f-926d-6adb95a942f2-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 8.807897567749023, \"sum\": 8.807897567749023, \"min\": 8.807897567749023}}, \"EndTime\": 1615065528.889867, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1615065528.880292}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:18:50 INFO 139741071263552] Epoch[4] Batch[0] avg_epoch_loss=2.401659\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:18:50 INFO 139741071263552] #quality_metric: host=algo-1, epoch=4, batch=0 train loss <loss>=2.40165948868\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:18:50 INFO 139741071263552] Epoch[4] Batch[5] avg_epoch_loss=1.941130\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:18:50 INFO 139741071263552] #quality_metric: host=algo-1, epoch=4, batch=5 train loss <loss>=1.94113037984\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:18:50 INFO 139741071263552] Epoch[4] Batch [5]#011Speed: 2825.88 samples/sec#011loss=1.941130\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:18:50 INFO 139741071263552] processed a total of 1274 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1973.8049507141113, \"sum\": 1973.8049507141113, \"min\": 1973.8049507141113}}, \"EndTime\": 1615065530.863803, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1615065528.889933}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:18:50 INFO 139741071263552] #throughput_metric: host=algo-1, train throughput=645.400212044 records/second\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:18:50 INFO 139741071263552] #progress_metric: host=algo-1, completed 2 % of epochs\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:18:50 INFO 139741071263552] #quality_metric: host=algo-1, epoch=4, train loss <loss>=1.83870414495\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:18:50 INFO 139741071263552] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:18:50 INFO 139741071263552] Saved checkpoint to \"/opt/ml/model/state_c4456928-eba1-4813-8caa-4bd81a0f885a-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 10.627985000610352, \"sum\": 10.627985000610352, \"min\": 10.627985000610352}}, \"EndTime\": 1615065530.875144, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1615065530.863926}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:18:52 INFO 139741071263552] Epoch[5] Batch[0] avg_epoch_loss=1.358301\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:18:52 INFO 139741071263552] #quality_metric: host=algo-1, epoch=5, batch=0 train loss <loss>=1.35830116272\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:18:52 INFO 139741071263552] Epoch[5] Batch[5] avg_epoch_loss=1.339308\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:18:52 INFO 139741071263552] #quality_metric: host=algo-1, epoch=5, batch=5 train loss <loss>=1.33930838108\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:18:52 INFO 139741071263552] Epoch[5] Batch [5]#011Speed: 3179.80 samples/sec#011loss=1.339308\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:18:52 INFO 139741071263552] processed a total of 1255 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1919.3570613861084, \"sum\": 1919.3570613861084, \"min\": 1919.3570613861084}}, \"EndTime\": 1615065532.794616, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1615065530.875198}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:18:52 INFO 139741071263552] #throughput_metric: host=algo-1, train throughput=653.819551644 records/second\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:18:52 INFO 139741071263552] #progress_metric: host=algo-1, completed 3 % of epochs\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:18:52 INFO 139741071263552] #quality_metric: host=algo-1, epoch=5, train loss <loss>=1.39028522968\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:18:52 INFO 139741071263552] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:18:52 INFO 139741071263552] Saved checkpoint to \"/opt/ml/model/state_1bc9cc03-2e3d-4165-afcf-6067921279f0-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 11.152982711791992, \"sum\": 11.152982711791992, \"min\": 11.152982711791992}}, \"EndTime\": 1615065532.806367, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1615065532.794708}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:18:54 INFO 139741071263552] Epoch[6] Batch[0] avg_epoch_loss=1.222942\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:18:54 INFO 139741071263552] #quality_metric: host=algo-1, epoch=6, batch=0 train loss <loss>=1.2229424715\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:18:54 INFO 139741071263552] Epoch[6] Batch[5] avg_epoch_loss=1.520550\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:18:54 INFO 139741071263552] #quality_metric: host=algo-1, epoch=6, batch=5 train loss <loss>=1.52055035035\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:18:54 INFO 139741071263552] Epoch[6] Batch [5]#011Speed: 3067.44 samples/sec#011loss=1.520550\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:18:54 INFO 139741071263552] processed a total of 1271 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2029.2620658874512, \"sum\": 2029.2620658874512, \"min\": 2029.2620658874512}}, \"EndTime\": 1615065534.835742, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1615065532.806422}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:18:54 INFO 139741071263552] #throughput_metric: host=algo-1, train throughput=626.29808721 records/second\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:18:54 INFO 139741071263552] #progress_metric: host=algo-1, completed 3 % of epochs\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:18:54 INFO 139741071263552] #quality_metric: host=algo-1, epoch=6, train loss <loss>=1.35902838111\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:18:54 INFO 139741071263552] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:18:54 INFO 139741071263552] Saved checkpoint to \"/opt/ml/model/state_b8c6adbe-ca82-4e4a-8922-6a58044856e3-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 10.524988174438477, \"sum\": 10.524988174438477, \"min\": 10.524988174438477}}, \"EndTime\": 1615065534.846924, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1615065534.835828}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:18:56 INFO 139741071263552] Epoch[7] Batch[0] avg_epoch_loss=1.303202\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:18:56 INFO 139741071263552] #quality_metric: host=algo-1, epoch=7, batch=0 train loss <loss>=1.30320239067\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:18:56 INFO 139741071263552] Epoch[7] Batch[5] avg_epoch_loss=1.156164\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:18:56 INFO 139741071263552] #quality_metric: host=algo-1, epoch=7, batch=5 train loss <loss>=1.15616428852\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:18:56 INFO 139741071263552] Epoch[7] Batch [5]#011Speed: 3126.16 samples/sec#011loss=1.156164\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:18:56 INFO 139741071263552] processed a total of 1278 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1964.8141860961914, \"sum\": 1964.8141860961914, \"min\": 1964.8141860961914}}, \"EndTime\": 1615065536.811867, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1615065534.846987}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:18:56 INFO 139741071263552] #throughput_metric: host=algo-1, train throughput=650.382654749 records/second\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:18:56 INFO 139741071263552] #progress_metric: host=algo-1, completed 4 % of epochs\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:18:56 INFO 139741071263552] #quality_metric: host=algo-1, epoch=7, train loss <loss>=1.2005491972\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:18:56 INFO 139741071263552] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:18:56 INFO 139741071263552] Saved checkpoint to \"/opt/ml/model/state_fa82f003-d0c0-4fc4-ac5b-757f0ea31ab1-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 11.096954345703125, \"sum\": 11.096954345703125, \"min\": 11.096954345703125}}, \"EndTime\": 1615065536.82365, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1615065536.812009}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:18:58 INFO 139741071263552] Epoch[8] Batch[0] avg_epoch_loss=0.585075\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:18:58 INFO 139741071263552] #quality_metric: host=algo-1, epoch=8, batch=0 train loss <loss>=0.585074663162\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:18:58 INFO 139741071263552] Epoch[8] Batch[5] avg_epoch_loss=0.949280\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:18:58 INFO 139741071263552] #quality_metric: host=algo-1, epoch=8, batch=5 train loss <loss>=0.949280450741\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:18:58 INFO 139741071263552] Epoch[8] Batch [5]#011Speed: 3090.47 samples/sec#011loss=0.949280\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:18:58 INFO 139741071263552] Epoch[8] Batch[10] avg_epoch_loss=1.134559\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:18:58 INFO 139741071263552] #quality_metric: host=algo-1, epoch=8, batch=10 train loss <loss>=1.35689370632\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:18:58 INFO 139741071263552] Epoch[8] Batch [10]#011Speed: 2817.20 samples/sec#011loss=1.356894\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:18:58 INFO 139741071263552] processed a total of 1336 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2090.968132019043, \"sum\": 2090.968132019043, \"min\": 2090.968132019043}}, \"EndTime\": 1615065538.914731, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1615065536.823704}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:18:58 INFO 139741071263552] #throughput_metric: host=algo-1, train throughput=638.900306932 records/second\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:18:58 INFO 139741071263552] #progress_metric: host=algo-1, completed 4 % of epochs\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:18:58 INFO 139741071263552] #quality_metric: host=algo-1, epoch=8, train loss <loss>=1.13455920328\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:18:58 INFO 139741071263552] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:18:58 INFO 139741071263552] Saved checkpoint to \"/opt/ml/model/state_6c8e4bd6-9984-4553-9fde-472b60a8d384-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 11.209964752197266, \"sum\": 11.209964752197266, \"min\": 11.209964752197266}}, \"EndTime\": 1615065538.926497, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1615065538.914819}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:00 INFO 139741071263552] Epoch[9] Batch[0] avg_epoch_loss=1.219409\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:00 INFO 139741071263552] #quality_metric: host=algo-1, epoch=9, batch=0 train loss <loss>=1.21940886974\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:00 INFO 139741071263552] Epoch[9] Batch[5] avg_epoch_loss=1.048287\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:00 INFO 139741071263552] #quality_metric: host=algo-1, epoch=9, batch=5 train loss <loss>=1.0482866168\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:00 INFO 139741071263552] Epoch[9] Batch [5]#011Speed: 3082.38 samples/sec#011loss=1.048287\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:00 INFO 139741071263552] processed a total of 1243 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1960.6308937072754, \"sum\": 1960.6308937072754, \"min\": 1960.6308937072754}}, \"EndTime\": 1615065540.887251, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1615065538.926557}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:00 INFO 139741071263552] #throughput_metric: host=algo-1, train throughput=633.935279388 records/second\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:00 INFO 139741071263552] #progress_metric: host=algo-1, completed 5 % of epochs\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:00 INFO 139741071263552] #quality_metric: host=algo-1, epoch=9, train loss <loss>=1.01328081489\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:00 INFO 139741071263552] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:00 INFO 139741071263552] Saved checkpoint to \"/opt/ml/model/state_236404ba-1092-4dc9-9861-60f41f1702ed-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 11.445045471191406, \"sum\": 11.445045471191406, \"min\": 11.445045471191406}}, \"EndTime\": 1615065540.899333, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1615065540.887345}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:02 INFO 139741071263552] Epoch[10] Batch[0] avg_epoch_loss=1.594375\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:02 INFO 139741071263552] #quality_metric: host=algo-1, epoch=10, batch=0 train loss <loss>=1.5943748951\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:02 INFO 139741071263552] Epoch[10] Batch[5] avg_epoch_loss=1.011081\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:02 INFO 139741071263552] #quality_metric: host=algo-1, epoch=10, batch=5 train loss <loss>=1.0110809505\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:02 INFO 139741071263552] Epoch[10] Batch [5]#011Speed: 2365.18 samples/sec#011loss=1.011081\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:03 INFO 139741071263552] processed a total of 1264 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2274.96600151062, \"sum\": 2274.96600151062, \"min\": 2274.96600151062}}, \"EndTime\": 1615065543.174414, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1615065540.899387}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:03 INFO 139741071263552] #throughput_metric: host=algo-1, train throughput=555.581431827 records/second\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:03 INFO 139741071263552] #progress_metric: host=algo-1, completed 5 % of epochs\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:03 INFO 139741071263552] #quality_metric: host=algo-1, epoch=10, train loss <loss>=1.08570528626\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:03 INFO 139741071263552] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:05 INFO 139741071263552] Epoch[11] Batch[0] avg_epoch_loss=1.112823\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:05 INFO 139741071263552] #quality_metric: host=algo-1, epoch=11, batch=0 train loss <loss>=1.11282324791\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:05 INFO 139741071263552] Epoch[11] Batch[5] avg_epoch_loss=0.870570\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:05 INFO 139741071263552] #quality_metric: host=algo-1, epoch=11, batch=5 train loss <loss>=0.870570083459\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:05 INFO 139741071263552] Epoch[11] Batch [5]#011Speed: 2176.11 samples/sec#011loss=0.870570\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:05 INFO 139741071263552] processed a total of 1218 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2419.198989868164, \"sum\": 2419.198989868164, \"min\": 2419.198989868164}}, \"EndTime\": 1615065545.594523, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1615065543.174502}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:05 INFO 139741071263552] #throughput_metric: host=algo-1, train throughput=503.445788687 records/second\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:05 INFO 139741071263552] #progress_metric: host=algo-1, completed 6 % of epochs\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:05 INFO 139741071263552] #quality_metric: host=algo-1, epoch=11, train loss <loss>=1.07383230925\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:05 INFO 139741071263552] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:07 INFO 139741071263552] Epoch[12] Batch[0] avg_epoch_loss=1.156825\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:07 INFO 139741071263552] #quality_metric: host=algo-1, epoch=12, batch=0 train loss <loss>=1.15682530403\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:07 INFO 139741071263552] Epoch[12] Batch[5] avg_epoch_loss=1.026982\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:07 INFO 139741071263552] #quality_metric: host=algo-1, epoch=12, batch=5 train loss <loss>=1.02698152264\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:07 INFO 139741071263552] Epoch[12] Batch [5]#011Speed: 2952.29 samples/sec#011loss=1.026982\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:07 INFO 139741071263552] processed a total of 1261 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2008.0668926239014, \"sum\": 2008.0668926239014, \"min\": 2008.0668926239014}}, \"EndTime\": 1615065547.603292, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1615065545.594608}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:07 INFO 139741071263552] #throughput_metric: host=algo-1, train throughput=627.930149322 records/second\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:07 INFO 139741071263552] #progress_metric: host=algo-1, completed 6 % of epochs\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:07 INFO 139741071263552] #quality_metric: host=algo-1, epoch=12, train loss <loss>=1.01485698223\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:07 INFO 139741071263552] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:09 INFO 139741071263552] Epoch[13] Batch[0] avg_epoch_loss=1.182796\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:09 INFO 139741071263552] #quality_metric: host=algo-1, epoch=13, batch=0 train loss <loss>=1.18279623985\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:09 INFO 139741071263552] Epoch[13] Batch[5] avg_epoch_loss=1.190968\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:09 INFO 139741071263552] #quality_metric: host=algo-1, epoch=13, batch=5 train loss <loss>=1.19096849362\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:09 INFO 139741071263552] Epoch[13] Batch [5]#011Speed: 2561.35 samples/sec#011loss=1.190968\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:09 INFO 139741071263552] Epoch[13] Batch[10] avg_epoch_loss=1.057720\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:09 INFO 139741071263552] #quality_metric: host=algo-1, epoch=13, batch=10 train loss <loss>=0.897822804749\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:09 INFO 139741071263552] Epoch[13] Batch [10]#011Speed: 2944.83 samples/sec#011loss=0.897823\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:09 INFO 139741071263552] processed a total of 1292 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2169.602870941162, \"sum\": 2169.602870941162, \"min\": 2169.602870941162}}, \"EndTime\": 1615065549.7736, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1615065547.603365}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:09 INFO 139741071263552] #throughput_metric: host=algo-1, train throughput=595.465547814 records/second\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:09 INFO 139741071263552] #progress_metric: host=algo-1, completed 7 % of epochs\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:09 INFO 139741071263552] #quality_metric: host=algo-1, epoch=13, train loss <loss>=1.05772045322\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:09 INFO 139741071263552] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:11 INFO 139741071263552] Epoch[14] Batch[0] avg_epoch_loss=1.028914\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:11 INFO 139741071263552] #quality_metric: host=algo-1, epoch=14, batch=0 train loss <loss>=1.02891433239\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:11 INFO 139741071263552] Epoch[14] Batch[5] avg_epoch_loss=1.073429\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:11 INFO 139741071263552] #quality_metric: host=algo-1, epoch=14, batch=5 train loss <loss>=1.07342938582\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:11 INFO 139741071263552] Epoch[14] Batch [5]#011Speed: 3020.44 samples/sec#011loss=1.073429\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:11 INFO 139741071263552] processed a total of 1257 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1955.596923828125, \"sum\": 1955.596923828125, \"min\": 1955.596923828125}}, \"EndTime\": 1615065551.729843, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1615065549.77369}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:11 INFO 139741071263552] #throughput_metric: host=algo-1, train throughput=642.734996655 records/second\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:11 INFO 139741071263552] #progress_metric: host=algo-1, completed 7 % of epochs\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:11 INFO 139741071263552] #quality_metric: host=algo-1, epoch=14, train loss <loss>=1.07229473591\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:11 INFO 139741071263552] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:13 INFO 139741071263552] Epoch[15] Batch[0] avg_epoch_loss=0.766869\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:13 INFO 139741071263552] #quality_metric: host=algo-1, epoch=15, batch=0 train loss <loss>=0.766869187355\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:13 INFO 139741071263552] Epoch[15] Batch[5] avg_epoch_loss=0.885369\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:13 INFO 139741071263552] #quality_metric: host=algo-1, epoch=15, batch=5 train loss <loss>=0.885369320711\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:13 INFO 139741071263552] Epoch[15] Batch [5]#011Speed: 3139.04 samples/sec#011loss=0.885369\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:13 INFO 139741071263552] Epoch[15] Batch[10] avg_epoch_loss=0.927579\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:13 INFO 139741071263552] #quality_metric: host=algo-1, epoch=15, batch=10 train loss <loss>=0.978229874372\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:13 INFO 139741071263552] Epoch[15] Batch [10]#011Speed: 2522.42 samples/sec#011loss=0.978230\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:13 INFO 139741071263552] processed a total of 1366 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2023.9720344543457, \"sum\": 2023.9720344543457, \"min\": 2023.9720344543457}}, \"EndTime\": 1615065553.754431, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1615065551.729913}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:13 INFO 139741071263552] #throughput_metric: host=algo-1, train throughput=674.874180219 records/second\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:13 INFO 139741071263552] #progress_metric: host=algo-1, completed 8 % of epochs\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:13 INFO 139741071263552] #quality_metric: host=algo-1, epoch=15, train loss <loss>=0.927578663284\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:13 INFO 139741071263552] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:13 INFO 139741071263552] Saved checkpoint to \"/opt/ml/model/state_fdbfaa1d-c7fd-40cb-a9b3-3a1fba4938e1-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 7.793903350830078, \"sum\": 7.793903350830078, \"min\": 7.793903350830078}}, \"EndTime\": 1615065553.762857, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1615065553.754504}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:15 INFO 139741071263552] Epoch[16] Batch[0] avg_epoch_loss=1.026447\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:15 INFO 139741071263552] #quality_metric: host=algo-1, epoch=16, batch=0 train loss <loss>=1.02644717693\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:15 INFO 139741071263552] Epoch[16] Batch[5] avg_epoch_loss=0.893224\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:15 INFO 139741071263552] #quality_metric: host=algo-1, epoch=16, batch=5 train loss <loss>=0.893224169811\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:15 INFO 139741071263552] Epoch[16] Batch [5]#011Speed: 3161.07 samples/sec#011loss=0.893224\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:15 INFO 139741071263552] processed a total of 1240 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1943.061113357544, \"sum\": 1943.061113357544, \"min\": 1943.061113357544}}, \"EndTime\": 1615065555.706035, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1615065553.762916}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:15 INFO 139741071263552] #throughput_metric: host=algo-1, train throughput=638.130868596 records/second\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:15 INFO 139741071263552] #progress_metric: host=algo-1, completed 8 % of epochs\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:15 INFO 139741071263552] #quality_metric: host=algo-1, epoch=16, train loss <loss>=0.841955038905\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:15 INFO 139741071263552] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:15 INFO 139741071263552] Saved checkpoint to \"/opt/ml/model/state_ea450c2d-e8b3-42e7-a222-e85c9d049dc1-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 8.926868438720703, \"sum\": 8.926868438720703, \"min\": 8.926868438720703}}, \"EndTime\": 1615065555.715641, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1615065555.706115}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:17 INFO 139741071263552] Epoch[17] Batch[0] avg_epoch_loss=1.224322\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:17 INFO 139741071263552] #quality_metric: host=algo-1, epoch=17, batch=0 train loss <loss>=1.2243219614\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:17 INFO 139741071263552] Epoch[17] Batch[5] avg_epoch_loss=0.808255\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:17 INFO 139741071263552] #quality_metric: host=algo-1, epoch=17, batch=5 train loss <loss>=0.80825535953\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:17 INFO 139741071263552] Epoch[17] Batch [5]#011Speed: 3061.82 samples/sec#011loss=0.808255\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:17 INFO 139741071263552] processed a total of 1259 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1947.6079940795898, \"sum\": 1947.6079940795898, \"min\": 1947.6079940795898}}, \"EndTime\": 1615065557.66336, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1615065555.715691}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:17 INFO 139741071263552] #throughput_metric: host=algo-1, train throughput=646.393472595 records/second\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:17 INFO 139741071263552] #progress_metric: host=algo-1, completed 9 % of epochs\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:17 INFO 139741071263552] #quality_metric: host=algo-1, epoch=17, train loss <loss>=0.883770272136\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:17 INFO 139741071263552] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:19 INFO 139741071263552] Epoch[18] Batch[0] avg_epoch_loss=1.215919\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:19 INFO 139741071263552] #quality_metric: host=algo-1, epoch=18, batch=0 train loss <loss>=1.21591866016\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:19 INFO 139741071263552] Epoch[18] Batch[5] avg_epoch_loss=0.976242\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:19 INFO 139741071263552] #quality_metric: host=algo-1, epoch=18, batch=5 train loss <loss>=0.976241588593\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:19 INFO 139741071263552] Epoch[18] Batch [5]#011Speed: 3004.23 samples/sec#011loss=0.976242\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:19 INFO 139741071263552] Epoch[18] Batch[10] avg_epoch_loss=1.029513\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:19 INFO 139741071263552] #quality_metric: host=algo-1, epoch=18, batch=10 train loss <loss>=1.09343787432\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:19 INFO 139741071263552] Epoch[18] Batch [10]#011Speed: 2520.50 samples/sec#011loss=1.093438\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:19 INFO 139741071263552] processed a total of 1331 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2072.3929405212402, \"sum\": 2072.3929405212402, \"min\": 2072.3929405212402}}, \"EndTime\": 1615065559.736364, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1615065557.663441}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:19 INFO 139741071263552] #throughput_metric: host=algo-1, train throughput=642.214003591 records/second\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:19 INFO 139741071263552] #progress_metric: host=algo-1, completed 9 % of epochs\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:19 INFO 139741071263552] #quality_metric: host=algo-1, epoch=18, train loss <loss>=1.02951262756\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:19 INFO 139741071263552] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:21 INFO 139741071263552] Epoch[19] Batch[0] avg_epoch_loss=0.890598\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:21 INFO 139741071263552] #quality_metric: host=algo-1, epoch=19, batch=0 train loss <loss>=0.890597879887\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:21 INFO 139741071263552] Epoch[19] Batch[5] avg_epoch_loss=0.964636\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:21 INFO 139741071263552] #quality_metric: host=algo-1, epoch=19, batch=5 train loss <loss>=0.96463561058\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:21 INFO 139741071263552] Epoch[19] Batch [5]#011Speed: 3028.02 samples/sec#011loss=0.964636\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:21 INFO 139741071263552] Epoch[19] Batch[10] avg_epoch_loss=0.861986\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:21 INFO 139741071263552] #quality_metric: host=algo-1, epoch=19, batch=10 train loss <loss>=0.738806414604\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:21 INFO 139741071263552] Epoch[19] Batch [10]#011Speed: 2724.38 samples/sec#011loss=0.738806\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:21 INFO 139741071263552] processed a total of 1329 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2087.156057357788, \"sum\": 2087.156057357788, \"min\": 2087.156057357788}}, \"EndTime\": 1615065561.824071, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1615065559.73645}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:21 INFO 139741071263552] #throughput_metric: host=algo-1, train throughput=636.712925938 records/second\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:21 INFO 139741071263552] #progress_metric: host=algo-1, completed 10 % of epochs\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:21 INFO 139741071263552] #quality_metric: host=algo-1, epoch=19, train loss <loss>=0.861985976046\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:21 INFO 139741071263552] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:23 INFO 139741071263552] Epoch[20] Batch[0] avg_epoch_loss=0.924188\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:23 INFO 139741071263552] #quality_metric: host=algo-1, epoch=20, batch=0 train loss <loss>=0.924188315868\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:23 INFO 139741071263552] Epoch[20] Batch[5] avg_epoch_loss=0.850462\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:23 INFO 139741071263552] #quality_metric: host=algo-1, epoch=20, batch=5 train loss <loss>=0.85046222806\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:23 INFO 139741071263552] Epoch[20] Batch [5]#011Speed: 3120.93 samples/sec#011loss=0.850462\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:23 INFO 139741071263552] processed a total of 1260 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1980.9539318084717, \"sum\": 1980.9539318084717, \"min\": 1980.9539318084717}}, \"EndTime\": 1615065563.805569, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1615065561.824157}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:23 INFO 139741071263552] #throughput_metric: host=algo-1, train throughput=636.007132665 records/second\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:23 INFO 139741071263552] #progress_metric: host=algo-1, completed 10 % of epochs\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:23 INFO 139741071263552] #quality_metric: host=algo-1, epoch=20, train loss <loss>=0.754981178045\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:23 INFO 139741071263552] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:23 INFO 139741071263552] Saved checkpoint to \"/opt/ml/model/state_5073ca85-6fb1-41a3-bb56-bd89c45e1606-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 12.112140655517578, \"sum\": 12.112140655517578, \"min\": 12.112140655517578}}, \"EndTime\": 1615065563.818568, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1615065563.80568}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:25 INFO 139741071263552] Epoch[21] Batch[0] avg_epoch_loss=0.629761\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:25 INFO 139741071263552] #quality_metric: host=algo-1, epoch=21, batch=0 train loss <loss>=0.629761338234\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:25 INFO 139741071263552] Epoch[21] Batch[5] avg_epoch_loss=0.814488\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:25 INFO 139741071263552] #quality_metric: host=algo-1, epoch=21, batch=5 train loss <loss>=0.81448751688\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:25 INFO 139741071263552] Epoch[21] Batch [5]#011Speed: 2991.35 samples/sec#011loss=0.814488\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:25 INFO 139741071263552] processed a total of 1248 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2013.2830142974854, \"sum\": 2013.2830142974854, \"min\": 2013.2830142974854}}, \"EndTime\": 1615065565.832285, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1615065563.818927}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:25 INFO 139741071263552] #throughput_metric: host=algo-1, train throughput=619.844211798 records/second\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:25 INFO 139741071263552] #progress_metric: host=algo-1, completed 11 % of epochs\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:25 INFO 139741071263552] #quality_metric: host=algo-1, epoch=21, train loss <loss>=0.85841422677\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:25 INFO 139741071263552] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:27 INFO 139741071263552] Epoch[22] Batch[0] avg_epoch_loss=0.968777\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:27 INFO 139741071263552] #quality_metric: host=algo-1, epoch=22, batch=0 train loss <loss>=0.968776762486\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:27 INFO 139741071263552] Epoch[22] Batch[5] avg_epoch_loss=1.028521\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:27 INFO 139741071263552] #quality_metric: host=algo-1, epoch=22, batch=5 train loss <loss>=1.02852074305\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:27 INFO 139741071263552] Epoch[22] Batch [5]#011Speed: 3165.17 samples/sec#011loss=1.028521\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:27 INFO 139741071263552] Epoch[22] Batch[10] avg_epoch_loss=1.258293\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:27 INFO 139741071263552] #quality_metric: host=algo-1, epoch=22, batch=10 train loss <loss>=1.53402048349\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:27 INFO 139741071263552] Epoch[22] Batch [10]#011Speed: 2904.28 samples/sec#011loss=1.534020\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:27 INFO 139741071263552] processed a total of 1291 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1992.9418563842773, \"sum\": 1992.9418563842773, \"min\": 1992.9418563842773}}, \"EndTime\": 1615065567.825784, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1615065565.832368}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:27 INFO 139741071263552] #throughput_metric: host=algo-1, train throughput=647.743463826 records/second\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:27 INFO 139741071263552] #progress_metric: host=algo-1, completed 11 % of epochs\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:27 INFO 139741071263552] #quality_metric: host=algo-1, epoch=22, train loss <loss>=1.25829335234\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:27 INFO 139741071263552] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:29 INFO 139741071263552] Epoch[23] Batch[0] avg_epoch_loss=1.072766\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:29 INFO 139741071263552] #quality_metric: host=algo-1, epoch=23, batch=0 train loss <loss>=1.07276570797\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:29 INFO 139741071263552] Epoch[23] Batch[5] avg_epoch_loss=1.040258\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:29 INFO 139741071263552] #quality_metric: host=algo-1, epoch=23, batch=5 train loss <loss>=1.04025790095\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:29 INFO 139741071263552] Epoch[23] Batch [5]#011Speed: 3128.14 samples/sec#011loss=1.040258\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:29 INFO 139741071263552] Epoch[23] Batch[10] avg_epoch_loss=0.568179\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:29 INFO 139741071263552] #quality_metric: host=algo-1, epoch=23, batch=10 train loss <loss>=0.00168466567993\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:29 INFO 139741071263552] Epoch[23] Batch [10]#011Speed: 2831.72 samples/sec#011loss=0.001685\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:29 INFO 139741071263552] processed a total of 1286 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2032.109022140503, \"sum\": 2032.109022140503, \"min\": 2032.109022140503}}, \"EndTime\": 1615065569.858523, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1615065567.825876}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:29 INFO 139741071263552] #throughput_metric: host=algo-1, train throughput=632.800490297 records/second\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:29 INFO 139741071263552] #progress_metric: host=algo-1, completed 12 % of epochs\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:29 INFO 139741071263552] #quality_metric: host=algo-1, epoch=23, train loss <loss>=0.568179157647\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:29 INFO 139741071263552] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:29 INFO 139741071263552] Saved checkpoint to \"/opt/ml/model/state_69e16db7-f6c2-44d4-90af-8cd2686c5c1d-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 11.081933975219727, \"sum\": 11.081933975219727, \"min\": 11.081933975219727}}, \"EndTime\": 1615065569.870158, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1615065569.85861}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:31 INFO 139741071263552] Epoch[24] Batch[0] avg_epoch_loss=0.877527\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:31 INFO 139741071263552] #quality_metric: host=algo-1, epoch=24, batch=0 train loss <loss>=0.877526581287\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:31 INFO 139741071263552] Epoch[24] Batch[5] avg_epoch_loss=0.915873\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:31 INFO 139741071263552] #quality_metric: host=algo-1, epoch=24, batch=5 train loss <loss>=0.915873209635\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:31 INFO 139741071263552] Epoch[24] Batch [5]#011Speed: 3141.75 samples/sec#011loss=0.915873\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:31 INFO 139741071263552] Epoch[24] Batch[10] avg_epoch_loss=1.077604\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:31 INFO 139741071263552] #quality_metric: host=algo-1, epoch=24, batch=10 train loss <loss>=1.27168061137\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:31 INFO 139741071263552] Epoch[24] Batch [10]#011Speed: 2741.98 samples/sec#011loss=1.271681\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:31 INFO 139741071263552] processed a total of 1304 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2050.4729747772217, \"sum\": 2050.4729747772217, \"min\": 2050.4729747772217}}, \"EndTime\": 1615065571.92074, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1615065569.870211}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:31 INFO 139741071263552] #throughput_metric: host=algo-1, train throughput=635.912015954 records/second\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:31 INFO 139741071263552] #progress_metric: host=algo-1, completed 12 % of epochs\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:31 INFO 139741071263552] #quality_metric: host=algo-1, epoch=24, train loss <loss>=1.07760384679\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:31 INFO 139741071263552] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:33 INFO 139741071263552] Epoch[25] Batch[0] avg_epoch_loss=0.978096\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:33 INFO 139741071263552] #quality_metric: host=algo-1, epoch=25, batch=0 train loss <loss>=0.978095948696\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:33 INFO 139741071263552] Epoch[25] Batch[5] avg_epoch_loss=0.856219\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:33 INFO 139741071263552] #quality_metric: host=algo-1, epoch=25, batch=5 train loss <loss>=0.85621906817\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:33 INFO 139741071263552] Epoch[25] Batch [5]#011Speed: 3059.68 samples/sec#011loss=0.856219\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:33 INFO 139741071263552] processed a total of 1234 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1945.3279972076416, \"sum\": 1945.3279972076416, \"min\": 1945.3279972076416}}, \"EndTime\": 1615065573.866623, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1615065571.920828}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:33 INFO 139741071263552] #throughput_metric: host=algo-1, train throughput=634.294384433 records/second\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:33 INFO 139741071263552] #progress_metric: host=algo-1, completed 13 % of epochs\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:33 INFO 139741071263552] #quality_metric: host=algo-1, epoch=25, train loss <loss>=0.946751204133\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:33 INFO 139741071263552] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:35 INFO 139741071263552] Epoch[26] Batch[0] avg_epoch_loss=0.522876\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:35 INFO 139741071263552] #quality_metric: host=algo-1, epoch=26, batch=0 train loss <loss>=0.522876024246\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:35 INFO 139741071263552] Epoch[26] Batch[5] avg_epoch_loss=0.694044\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:35 INFO 139741071263552] #quality_metric: host=algo-1, epoch=26, batch=5 train loss <loss>=0.694044108192\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:35 INFO 139741071263552] Epoch[26] Batch [5]#011Speed: 3145.88 samples/sec#011loss=0.694044\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:35 INFO 139741071263552] processed a total of 1252 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1987.4298572540283, \"sum\": 1987.4298572540283, \"min\": 1987.4298572540283}}, \"EndTime\": 1615065575.854672, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1615065573.866718}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:35 INFO 139741071263552] #throughput_metric: host=algo-1, train throughput=629.921555686 records/second\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:35 INFO 139741071263552] #progress_metric: host=algo-1, completed 13 % of epochs\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:35 INFO 139741071263552] #quality_metric: host=algo-1, epoch=26, train loss <loss>=0.682681134343\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:35 INFO 139741071263552] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:37 INFO 139741071263552] Epoch[27] Batch[0] avg_epoch_loss=0.912700\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:37 INFO 139741071263552] #quality_metric: host=algo-1, epoch=27, batch=0 train loss <loss>=0.912699639797\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:37 INFO 139741071263552] Epoch[27] Batch[5] avg_epoch_loss=0.869223\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:37 INFO 139741071263552] #quality_metric: host=algo-1, epoch=27, batch=5 train loss <loss>=0.869222670794\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:37 INFO 139741071263552] Epoch[27] Batch [5]#011Speed: 2971.33 samples/sec#011loss=0.869223\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:37 INFO 139741071263552] Epoch[27] Batch[10] avg_epoch_loss=0.837881\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:37 INFO 139741071263552] #quality_metric: host=algo-1, epoch=27, batch=10 train loss <loss>=0.800270104408\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:37 INFO 139741071263552] Epoch[27] Batch [10]#011Speed: 2737.87 samples/sec#011loss=0.800270\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:37 INFO 139741071263552] processed a total of 1303 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2028.6328792572021, \"sum\": 2028.6328792572021, \"min\": 2028.6328792572021}}, \"EndTime\": 1615065577.883973, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1615065575.854754}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:37 INFO 139741071263552] #throughput_metric: host=algo-1, train throughput=642.249007749 records/second\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:37 INFO 139741071263552] #progress_metric: host=algo-1, completed 14 % of epochs\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:37 INFO 139741071263552] #quality_metric: host=algo-1, epoch=27, train loss <loss>=0.837880595164\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:37 INFO 139741071263552] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:39 INFO 139741071263552] Epoch[28] Batch[0] avg_epoch_loss=0.907189\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:39 INFO 139741071263552] #quality_metric: host=algo-1, epoch=28, batch=0 train loss <loss>=0.907189369202\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:39 INFO 139741071263552] Epoch[28] Batch[5] avg_epoch_loss=0.768504\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:39 INFO 139741071263552] #quality_metric: host=algo-1, epoch=28, batch=5 train loss <loss>=0.768503988783\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:39 INFO 139741071263552] Epoch[28] Batch [5]#011Speed: 2271.96 samples/sec#011loss=0.768504\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:39 INFO 139741071263552] processed a total of 1269 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2113.2028102874756, \"sum\": 2113.2028102874756, \"min\": 2113.2028102874756}}, \"EndTime\": 1615065579.99798, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1615065577.88406}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:39 INFO 139741071263552] #throughput_metric: host=algo-1, train throughput=600.473013601 records/second\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:39 INFO 139741071263552] #progress_metric: host=algo-1, completed 14 % of epochs\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:39 INFO 139741071263552] #quality_metric: host=algo-1, epoch=28, train loss <loss>=0.835976740718\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:39 INFO 139741071263552] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:41 INFO 139741071263552] Epoch[29] Batch[0] avg_epoch_loss=0.925195\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:41 INFO 139741071263552] #quality_metric: host=algo-1, epoch=29, batch=0 train loss <loss>=0.925194621086\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:41 INFO 139741071263552] Epoch[29] Batch[5] avg_epoch_loss=0.767422\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:41 INFO 139741071263552] #quality_metric: host=algo-1, epoch=29, batch=5 train loss <loss>=0.767421931028\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:41 INFO 139741071263552] Epoch[29] Batch [5]#011Speed: 2723.41 samples/sec#011loss=0.767422\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:41 INFO 139741071263552] processed a total of 1274 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1988.6891841888428, \"sum\": 1988.6891841888428, \"min\": 1988.6891841888428}}, \"EndTime\": 1615065581.987319, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1615065579.998064}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:41 INFO 139741071263552] #throughput_metric: host=algo-1, train throughput=640.580822459 records/second\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:41 INFO 139741071263552] #progress_metric: host=algo-1, completed 15 % of epochs\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:41 INFO 139741071263552] #quality_metric: host=algo-1, epoch=29, train loss <loss>=0.746606403589\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:41 INFO 139741071263552] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:43 INFO 139741071263552] Epoch[30] Batch[0] avg_epoch_loss=0.787245\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:43 INFO 139741071263552] #quality_metric: host=algo-1, epoch=30, batch=0 train loss <loss>=0.787244856358\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:43 INFO 139741071263552] Epoch[30] Batch[5] avg_epoch_loss=0.943281\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:43 INFO 139741071263552] #quality_metric: host=algo-1, epoch=30, batch=5 train loss <loss>=0.943281253179\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:43 INFO 139741071263552] Epoch[30] Batch [5]#011Speed: 3114.63 samples/sec#011loss=0.943281\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:44 INFO 139741071263552] Epoch[30] Batch[10] avg_epoch_loss=0.901626\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:44 INFO 139741071263552] #quality_metric: host=algo-1, epoch=30, batch=10 train loss <loss>=0.85163949132\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:44 INFO 139741071263552] Epoch[30] Batch [10]#011Speed: 2690.08 samples/sec#011loss=0.851639\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:44 INFO 139741071263552] processed a total of 1335 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2043.2910919189453, \"sum\": 2043.2910919189453, \"min\": 2043.2910919189453}}, \"EndTime\": 1615065584.031375, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1615065581.987403}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:44 INFO 139741071263552] #throughput_metric: host=algo-1, train throughput=653.314263833 records/second\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:44 INFO 139741071263552] #progress_metric: host=algo-1, completed 15 % of epochs\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:44 INFO 139741071263552] #quality_metric: host=algo-1, epoch=30, train loss <loss>=0.901625906879\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:44 INFO 139741071263552] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:45 INFO 139741071263552] Epoch[31] Batch[0] avg_epoch_loss=1.230049\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:45 INFO 139741071263552] #quality_metric: host=algo-1, epoch=31, batch=0 train loss <loss>=1.23004901409\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:45 INFO 139741071263552] Epoch[31] Batch[5] avg_epoch_loss=0.997551\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:45 INFO 139741071263552] #quality_metric: host=algo-1, epoch=31, batch=5 train loss <loss>=0.997550984224\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:45 INFO 139741071263552] Epoch[31] Batch [5]#011Speed: 3174.41 samples/sec#011loss=0.997551\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:46 INFO 139741071263552] processed a total of 1268 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2006.9830417633057, \"sum\": 2006.9830417633057, \"min\": 2006.9830417633057}}, \"EndTime\": 1615065586.039076, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1615065584.031465}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:46 INFO 139741071263552] #throughput_metric: host=algo-1, train throughput=631.724510692 records/second\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:46 INFO 139741071263552] #progress_metric: host=algo-1, completed 16 % of epochs\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:46 INFO 139741071263552] #quality_metric: host=algo-1, epoch=31, train loss <loss>=0.812452694774\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:46 INFO 139741071263552] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:47 INFO 139741071263552] Epoch[32] Batch[0] avg_epoch_loss=0.722587\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:47 INFO 139741071263552] #quality_metric: host=algo-1, epoch=32, batch=0 train loss <loss>=0.722586870193\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:47 INFO 139741071263552] Epoch[32] Batch[5] avg_epoch_loss=0.662615\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:47 INFO 139741071263552] #quality_metric: host=algo-1, epoch=32, batch=5 train loss <loss>=0.662615249554\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:47 INFO 139741071263552] Epoch[32] Batch [5]#011Speed: 3121.89 samples/sec#011loss=0.662615\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:47 INFO 139741071263552] processed a total of 1241 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1954.003095626831, \"sum\": 1954.003095626831, \"min\": 1954.003095626831}}, \"EndTime\": 1615065587.994144, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1615065586.03921}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:47 INFO 139741071263552] #throughput_metric: host=algo-1, train throughput=635.062297857 records/second\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:47 INFO 139741071263552] #progress_metric: host=algo-1, completed 16 % of epochs\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:47 INFO 139741071263552] #quality_metric: host=algo-1, epoch=32, train loss <loss>=0.692673127353\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:47 INFO 139741071263552] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:49 INFO 139741071263552] Epoch[33] Batch[0] avg_epoch_loss=0.555447\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:49 INFO 139741071263552] #quality_metric: host=algo-1, epoch=33, batch=0 train loss <loss>=0.555446922779\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:49 INFO 139741071263552] Epoch[33] Batch[5] avg_epoch_loss=0.869199\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:49 INFO 139741071263552] #quality_metric: host=algo-1, epoch=33, batch=5 train loss <loss>=0.869198789199\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:49 INFO 139741071263552] Epoch[33] Batch [5]#011Speed: 3113.92 samples/sec#011loss=0.869199\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:50 INFO 139741071263552] Epoch[33] Batch[10] avg_epoch_loss=0.662161\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:50 INFO 139741071263552] #quality_metric: host=algo-1, epoch=33, batch=10 train loss <loss>=0.413716228306\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:50 INFO 139741071263552] Epoch[33] Batch [10]#011Speed: 2912.48 samples/sec#011loss=0.413716\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:50 INFO 139741071263552] processed a total of 1313 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2095.1950550079346, \"sum\": 2095.1950550079346, \"min\": 2095.1950550079346}}, \"EndTime\": 1615065590.089923, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1615065587.994238}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:50 INFO 139741071263552] #throughput_metric: host=algo-1, train throughput=626.633672348 records/second\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:50 INFO 139741071263552] #progress_metric: host=algo-1, completed 17 % of epochs\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:50 INFO 139741071263552] #quality_metric: host=algo-1, epoch=33, train loss <loss>=0.662161261521\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:50 INFO 139741071263552] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:51 INFO 139741071263552] Epoch[34] Batch[0] avg_epoch_loss=0.484276\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:51 INFO 139741071263552] #quality_metric: host=algo-1, epoch=34, batch=0 train loss <loss>=0.484275966883\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:51 INFO 139741071263552] Epoch[34] Batch[5] avg_epoch_loss=0.922969\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:51 INFO 139741071263552] #quality_metric: host=algo-1, epoch=34, batch=5 train loss <loss>=0.922968700528\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:51 INFO 139741071263552] Epoch[34] Batch [5]#011Speed: 3145.05 samples/sec#011loss=0.922969\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:52 INFO 139741071263552] processed a total of 1274 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1999.5949268341064, \"sum\": 1999.5949268341064, \"min\": 1999.5949268341064}}, \"EndTime\": 1615065592.090047, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1615065590.09001}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:52 INFO 139741071263552] #throughput_metric: host=algo-1, train throughput=637.090149165 records/second\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:52 INFO 139741071263552] #progress_metric: host=algo-1, completed 17 % of epochs\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:52 INFO 139741071263552] #quality_metric: host=algo-1, epoch=34, train loss <loss>=0.871377542615\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:52 INFO 139741071263552] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:53 INFO 139741071263552] Epoch[35] Batch[0] avg_epoch_loss=0.827002\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:53 INFO 139741071263552] #quality_metric: host=algo-1, epoch=35, batch=0 train loss <loss>=0.827001869678\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:53 INFO 139741071263552] Epoch[35] Batch[5] avg_epoch_loss=0.558196\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:53 INFO 139741071263552] #quality_metric: host=algo-1, epoch=35, batch=5 train loss <loss>=0.558195665479\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:53 INFO 139741071263552] Epoch[35] Batch [5]#011Speed: 2998.33 samples/sec#011loss=0.558196\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:54 INFO 139741071263552] Epoch[35] Batch[10] avg_epoch_loss=0.563044\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:54 INFO 139741071263552] #quality_metric: host=algo-1, epoch=35, batch=10 train loss <loss>=0.568862201273\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:54 INFO 139741071263552] Epoch[35] Batch [10]#011Speed: 2752.25 samples/sec#011loss=0.568862\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:54 INFO 139741071263552] processed a total of 1286 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2097.823143005371, \"sum\": 2097.823143005371, \"min\": 2097.823143005371}}, \"EndTime\": 1615065594.188433, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1615065592.090129}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:54 INFO 139741071263552] #throughput_metric: host=algo-1, train throughput=612.976416566 records/second\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:54 INFO 139741071263552] #progress_metric: host=algo-1, completed 18 % of epochs\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:54 INFO 139741071263552] #quality_metric: host=algo-1, epoch=35, train loss <loss>=0.56304409084\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:54 INFO 139741071263552] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:54 INFO 139741071263552] Saved checkpoint to \"/opt/ml/model/state_6341696e-8d37-48d7-94d1-e82f2c6375e4-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 8.957147598266602, \"sum\": 8.957147598266602, \"min\": 8.957147598266602}}, \"EndTime\": 1615065594.198085, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1615065594.188527}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:55 INFO 139741071263552] Epoch[36] Batch[0] avg_epoch_loss=0.508934\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:55 INFO 139741071263552] #quality_metric: host=algo-1, epoch=36, batch=0 train loss <loss>=0.508934080601\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:56 INFO 139741071263552] Epoch[36] Batch[5] avg_epoch_loss=0.656829\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:56 INFO 139741071263552] #quality_metric: host=algo-1, epoch=36, batch=5 train loss <loss>=0.656829461455\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:56 INFO 139741071263552] Epoch[36] Batch [5]#011Speed: 2781.06 samples/sec#011loss=0.656829\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:56 INFO 139741071263552] Epoch[36] Batch[10] avg_epoch_loss=0.865227\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:56 INFO 139741071263552] #quality_metric: host=algo-1, epoch=36, batch=10 train loss <loss>=1.11530507207\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:56 INFO 139741071263552] Epoch[36] Batch [10]#011Speed: 2834.38 samples/sec#011loss=1.115305\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:56 INFO 139741071263552] processed a total of 1311 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2112.771987915039, \"sum\": 2112.771987915039, \"min\": 2112.771987915039}}, \"EndTime\": 1615065596.310994, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1615065594.198157}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:56 INFO 139741071263552] #throughput_metric: host=algo-1, train throughput=620.473664131 records/second\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:56 INFO 139741071263552] #progress_metric: host=algo-1, completed 18 % of epochs\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:56 INFO 139741071263552] #quality_metric: host=algo-1, epoch=36, train loss <loss>=0.86522746628\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:56 INFO 139741071263552] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:57 INFO 139741071263552] Epoch[37] Batch[0] avg_epoch_loss=0.643658\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:57 INFO 139741071263552] #quality_metric: host=algo-1, epoch=37, batch=0 train loss <loss>=0.643658459187\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:58 INFO 139741071263552] Epoch[37] Batch[5] avg_epoch_loss=0.682402\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:58 INFO 139741071263552] #quality_metric: host=algo-1, epoch=37, batch=5 train loss <loss>=0.682401607434\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:58 INFO 139741071263552] Epoch[37] Batch [5]#011Speed: 2991.32 samples/sec#011loss=0.682402\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:58 INFO 139741071263552] Epoch[37] Batch[10] avg_epoch_loss=0.362565\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:58 INFO 139741071263552] #quality_metric: host=algo-1, epoch=37, batch=10 train loss <loss>=-0.0212380573153\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:58 INFO 139741071263552] Epoch[37] Batch [10]#011Speed: 2878.15 samples/sec#011loss=-0.021238\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:58 INFO 139741071263552] processed a total of 1305 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2032.5698852539062, \"sum\": 2032.5698852539062, \"min\": 2032.5698852539062}}, \"EndTime\": 1615065598.344214, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1615065596.311083}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:58 INFO 139741071263552] #throughput_metric: host=algo-1, train throughput=642.006088867 records/second\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:58 INFO 139741071263552] #progress_metric: host=algo-1, completed 19 % of epochs\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:58 INFO 139741071263552] #quality_metric: host=algo-1, epoch=37, train loss <loss>=0.362565396184\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:58 INFO 139741071263552] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:58 INFO 139741071263552] Saved checkpoint to \"/opt/ml/model/state_9ce65b53-e69c-404f-a488-02bb3ac5bd24-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 8.800983428955078, \"sum\": 8.800983428955078, \"min\": 8.800983428955078}}, \"EndTime\": 1615065598.353655, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1615065598.34429}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:59 INFO 139741071263552] Epoch[38] Batch[0] avg_epoch_loss=0.589981\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:19:59 INFO 139741071263552] #quality_metric: host=algo-1, epoch=38, batch=0 train loss <loss>=0.58998131752\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:00 INFO 139741071263552] Epoch[38] Batch[5] avg_epoch_loss=0.628423\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:00 INFO 139741071263552] #quality_metric: host=algo-1, epoch=38, batch=5 train loss <loss>=0.628422781825\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:00 INFO 139741071263552] Epoch[38] Batch [5]#011Speed: 3005.81 samples/sec#011loss=0.628423\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:00 INFO 139741071263552] Epoch[38] Batch[10] avg_epoch_loss=0.562321\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:00 INFO 139741071263552] #quality_metric: host=algo-1, epoch=38, batch=10 train loss <loss>=0.482999046147\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:00 INFO 139741071263552] Epoch[38] Batch [10]#011Speed: 2470.87 samples/sec#011loss=0.482999\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:00 INFO 139741071263552] processed a total of 1366 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2109.8649501800537, \"sum\": 2109.8649501800537, \"min\": 2109.8649501800537}}, \"EndTime\": 1615065600.463649, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1615065598.35372}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:00 INFO 139741071263552] #throughput_metric: host=algo-1, train throughput=647.394567549 records/second\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:00 INFO 139741071263552] #progress_metric: host=algo-1, completed 19 % of epochs\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:00 INFO 139741071263552] #quality_metric: host=algo-1, epoch=38, train loss <loss>=0.56232108379\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:00 INFO 139741071263552] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:02 INFO 139741071263552] Epoch[39] Batch[0] avg_epoch_loss=0.694884\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:02 INFO 139741071263552] #quality_metric: host=algo-1, epoch=39, batch=0 train loss <loss>=0.694884479046\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:02 INFO 139741071263552] Epoch[39] Batch[5] avg_epoch_loss=0.707718\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:02 INFO 139741071263552] #quality_metric: host=algo-1, epoch=39, batch=5 train loss <loss>=0.70771787564\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:02 INFO 139741071263552] Epoch[39] Batch [5]#011Speed: 2515.70 samples/sec#011loss=0.707718\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:02 INFO 139741071263552] processed a total of 1260 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2118.7961101531982, \"sum\": 2118.7961101531982, \"min\": 2118.7961101531982}}, \"EndTime\": 1615065602.583072, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1615065600.463738}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:02 INFO 139741071263552] #throughput_metric: host=algo-1, train throughput=594.641726209 records/second\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:02 INFO 139741071263552] #progress_metric: host=algo-1, completed 20 % of epochs\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:02 INFO 139741071263552] #quality_metric: host=algo-1, epoch=39, train loss <loss>=0.744080513716\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:02 INFO 139741071263552] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:04 INFO 139741071263552] Epoch[40] Batch[0] avg_epoch_loss=0.501155\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:04 INFO 139741071263552] #quality_metric: host=algo-1, epoch=40, batch=0 train loss <loss>=0.501154839993\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:04 INFO 139741071263552] Epoch[40] Batch[5] avg_epoch_loss=0.687234\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:04 INFO 139741071263552] #quality_metric: host=algo-1, epoch=40, batch=5 train loss <loss>=0.687233711282\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:04 INFO 139741071263552] Epoch[40] Batch [5]#011Speed: 1961.28 samples/sec#011loss=0.687234\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:05 INFO 139741071263552] Epoch[40] Batch[10] avg_epoch_loss=0.470616\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:05 INFO 139741071263552] #quality_metric: host=algo-1, epoch=40, batch=10 train loss <loss>=0.210675561428\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:05 INFO 139741071263552] Epoch[40] Batch [10]#011Speed: 2186.83 samples/sec#011loss=0.210676\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:05 INFO 139741071263552] processed a total of 1297 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2475.6059646606445, \"sum\": 2475.6059646606445, \"min\": 2475.6059646606445}}, \"EndTime\": 1615065605.059196, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1615065602.583158}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:05 INFO 139741071263552] #throughput_metric: host=algo-1, train throughput=523.886754044 records/second\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:05 INFO 139741071263552] #progress_metric: host=algo-1, completed 20 % of epochs\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:05 INFO 139741071263552] #quality_metric: host=algo-1, epoch=40, train loss <loss>=0.47061637044\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:05 INFO 139741071263552] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:06 INFO 139741071263552] Epoch[41] Batch[0] avg_epoch_loss=0.517365\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:06 INFO 139741071263552] #quality_metric: host=algo-1, epoch=41, batch=0 train loss <loss>=0.517364740372\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:07 INFO 139741071263552] Epoch[41] Batch[5] avg_epoch_loss=0.605392\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:07 INFO 139741071263552] #quality_metric: host=algo-1, epoch=41, batch=5 train loss <loss>=0.605392118295\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:07 INFO 139741071263552] Epoch[41] Batch [5]#011Speed: 3031.18 samples/sec#011loss=0.605392\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:07 INFO 139741071263552] processed a total of 1242 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2142.76385307312, \"sum\": 2142.76385307312, \"min\": 2142.76385307312}}, \"EndTime\": 1615065607.202455, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1615065605.059277}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:07 INFO 139741071263552] #throughput_metric: host=algo-1, train throughput=579.592215547 records/second\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:07 INFO 139741071263552] #progress_metric: host=algo-1, completed 21 % of epochs\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:07 INFO 139741071263552] #quality_metric: host=algo-1, epoch=41, train loss <loss>=0.618108043075\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:07 INFO 139741071263552] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:08 INFO 139741071263552] Epoch[42] Batch[0] avg_epoch_loss=0.400543\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:08 INFO 139741071263552] #quality_metric: host=algo-1, epoch=42, batch=0 train loss <loss>=0.400543004274\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:09 INFO 139741071263552] Epoch[42] Batch[5] avg_epoch_loss=0.547126\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:09 INFO 139741071263552] #quality_metric: host=algo-1, epoch=42, batch=5 train loss <loss>=0.547126476963\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:09 INFO 139741071263552] Epoch[42] Batch [5]#011Speed: 3127.38 samples/sec#011loss=0.547126\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:09 INFO 139741071263552] Epoch[42] Batch[10] avg_epoch_loss=0.619205\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:09 INFO 139741071263552] #quality_metric: host=algo-1, epoch=42, batch=10 train loss <loss>=0.705698662996\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:09 INFO 139741071263552] Epoch[42] Batch [10]#011Speed: 2892.96 samples/sec#011loss=0.705699\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:09 INFO 139741071263552] processed a total of 1288 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2048.6509799957275, \"sum\": 2048.6509799957275, \"min\": 2048.6509799957275}}, \"EndTime\": 1615065609.251637, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1615065607.202537}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:09 INFO 139741071263552] #throughput_metric: host=algo-1, train throughput=628.668363593 records/second\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:09 INFO 139741071263552] #progress_metric: host=algo-1, completed 21 % of epochs\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:09 INFO 139741071263552] #quality_metric: host=algo-1, epoch=42, train loss <loss>=0.619204743342\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:09 INFO 139741071263552] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:10 INFO 139741071263552] Epoch[43] Batch[0] avg_epoch_loss=0.418209\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:10 INFO 139741071263552] #quality_metric: host=algo-1, epoch=43, batch=0 train loss <loss>=0.418208688498\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:11 INFO 139741071263552] Epoch[43] Batch[5] avg_epoch_loss=0.526119\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:11 INFO 139741071263552] #quality_metric: host=algo-1, epoch=43, batch=5 train loss <loss>=0.526118993759\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:11 INFO 139741071263552] Epoch[43] Batch [5]#011Speed: 3135.06 samples/sec#011loss=0.526119\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:11 INFO 139741071263552] processed a total of 1277 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2065.993070602417, \"sum\": 2065.993070602417, \"min\": 2065.993070602417}}, \"EndTime\": 1615065611.318261, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1615065609.251725}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:11 INFO 139741071263552] #throughput_metric: host=algo-1, train throughput=618.07209071 records/second\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:11 INFO 139741071263552] #progress_metric: host=algo-1, completed 22 % of epochs\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:11 INFO 139741071263552] #quality_metric: host=algo-1, epoch=43, train loss <loss>=0.593924057484\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:11 INFO 139741071263552] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:12 INFO 139741071263552] Epoch[44] Batch[0] avg_epoch_loss=0.475752\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:12 INFO 139741071263552] #quality_metric: host=algo-1, epoch=44, batch=0 train loss <loss>=0.475751638412\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:13 INFO 139741071263552] Epoch[44] Batch[5] avg_epoch_loss=0.656385\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:13 INFO 139741071263552] #quality_metric: host=algo-1, epoch=44, batch=5 train loss <loss>=0.656384656827\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:13 INFO 139741071263552] Epoch[44] Batch [5]#011Speed: 3119.03 samples/sec#011loss=0.656385\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:13 INFO 139741071263552] Epoch[44] Batch[10] avg_epoch_loss=0.376515\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:13 INFO 139741071263552] #quality_metric: host=algo-1, epoch=44, batch=10 train loss <loss>=0.0406709015369\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:13 INFO 139741071263552] Epoch[44] Batch [10]#011Speed: 2946.82 samples/sec#011loss=0.040671\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:13 INFO 139741071263552] processed a total of 1288 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2017.9378986358643, \"sum\": 2017.9378986358643, \"min\": 2017.9378986358643}}, \"EndTime\": 1615065613.336838, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1615065611.31833}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:13 INFO 139741071263552] #throughput_metric: host=algo-1, train throughput=638.236355203 records/second\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:13 INFO 139741071263552] #progress_metric: host=algo-1, completed 22 % of epochs\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:13 INFO 139741071263552] #quality_metric: host=algo-1, epoch=44, train loss <loss>=0.376514768059\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:13 INFO 139741071263552] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:14 INFO 139741071263552] Epoch[45] Batch[0] avg_epoch_loss=1.218087\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:14 INFO 139741071263552] #quality_metric: host=algo-1, epoch=45, batch=0 train loss <loss>=1.2180866003\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:15 INFO 139741071263552] Epoch[45] Batch[5] avg_epoch_loss=0.963050\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:15 INFO 139741071263552] #quality_metric: host=algo-1, epoch=45, batch=5 train loss <loss>=0.963049868743\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:15 INFO 139741071263552] Epoch[45] Batch [5]#011Speed: 2869.12 samples/sec#011loss=0.963050\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:15 INFO 139741071263552] Epoch[45] Batch[10] avg_epoch_loss=0.806924\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:15 INFO 139741071263552] #quality_metric: host=algo-1, epoch=45, batch=10 train loss <loss>=0.619572210312\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:15 INFO 139741071263552] Epoch[45] Batch [10]#011Speed: 2811.49 samples/sec#011loss=0.619572\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:15 INFO 139741071263552] processed a total of 1306 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2077.1939754486084, \"sum\": 2077.1939754486084, \"min\": 2077.1939754486084}}, \"EndTime\": 1615065615.414675, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1615065613.336923}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:15 INFO 139741071263552] #throughput_metric: host=algo-1, train throughput=628.691892413 records/second\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:15 INFO 139741071263552] #progress_metric: host=algo-1, completed 23 % of epochs\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:15 INFO 139741071263552] #quality_metric: host=algo-1, epoch=45, train loss <loss>=0.806923660365\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:15 INFO 139741071263552] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:17 INFO 139741071263552] Epoch[46] Batch[0] avg_epoch_loss=0.520136\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:17 INFO 139741071263552] #quality_metric: host=algo-1, epoch=46, batch=0 train loss <loss>=0.520136117935\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:17 INFO 139741071263552] Epoch[46] Batch[5] avg_epoch_loss=0.665925\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:17 INFO 139741071263552] #quality_metric: host=algo-1, epoch=46, batch=5 train loss <loss>=0.66592516005\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:17 INFO 139741071263552] Epoch[46] Batch [5]#011Speed: 3123.22 samples/sec#011loss=0.665925\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:17 INFO 139741071263552] Epoch[46] Batch[10] avg_epoch_loss=0.795976\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:17 INFO 139741071263552] #quality_metric: host=algo-1, epoch=46, batch=10 train loss <loss>=0.952036511898\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:17 INFO 139741071263552] Epoch[46] Batch [10]#011Speed: 2973.20 samples/sec#011loss=0.952037\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:17 INFO 139741071263552] processed a total of 1316 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2035.499095916748, \"sum\": 2035.499095916748, \"min\": 2035.499095916748}}, \"EndTime\": 1615065617.450835, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1615065615.41477}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:17 INFO 139741071263552] #throughput_metric: host=algo-1, train throughput=646.483895246 records/second\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:17 INFO 139741071263552] #progress_metric: host=algo-1, completed 23 % of epochs\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:17 INFO 139741071263552] #quality_metric: host=algo-1, epoch=46, train loss <loss>=0.795975774527\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:17 INFO 139741071263552] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:19 INFO 139741071263552] Epoch[47] Batch[0] avg_epoch_loss=0.748539\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:19 INFO 139741071263552] #quality_metric: host=algo-1, epoch=47, batch=0 train loss <loss>=0.74853938818\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:19 INFO 139741071263552] Epoch[47] Batch[5] avg_epoch_loss=0.764739\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:19 INFO 139741071263552] #quality_metric: host=algo-1, epoch=47, batch=5 train loss <loss>=0.764738539855\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:19 INFO 139741071263552] Epoch[47] Batch [5]#011Speed: 3075.46 samples/sec#011loss=0.764739\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:19 INFO 139741071263552] Epoch[47] Batch[10] avg_epoch_loss=0.848678\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:19 INFO 139741071263552] #quality_metric: host=algo-1, epoch=47, batch=10 train loss <loss>=0.94940521121\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:19 INFO 139741071263552] Epoch[47] Batch [10]#011Speed: 2960.61 samples/sec#011loss=0.949405\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:19 INFO 139741071263552] processed a total of 1319 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2084.336042404175, \"sum\": 2084.336042404175, \"min\": 2084.336042404175}}, \"EndTime\": 1615065619.535696, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1615065617.450923}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:19 INFO 139741071263552] #throughput_metric: host=algo-1, train throughput=632.767799875 records/second\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:19 INFO 139741071263552] #progress_metric: host=algo-1, completed 24 % of epochs\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:19 INFO 139741071263552] #quality_metric: host=algo-1, epoch=47, train loss <loss>=0.848677935925\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:19 INFO 139741071263552] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:21 INFO 139741071263552] Epoch[48] Batch[0] avg_epoch_loss=0.518647\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:21 INFO 139741071263552] #quality_metric: host=algo-1, epoch=48, batch=0 train loss <loss>=0.518647074699\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:21 INFO 139741071263552] Epoch[48] Batch[5] avg_epoch_loss=0.474258\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:21 INFO 139741071263552] #quality_metric: host=algo-1, epoch=48, batch=5 train loss <loss>=0.474257724981\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:21 INFO 139741071263552] Epoch[48] Batch [5]#011Speed: 3111.81 samples/sec#011loss=0.474258\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:21 INFO 139741071263552] Epoch[48] Batch[10] avg_epoch_loss=0.272822\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:21 INFO 139741071263552] #quality_metric: host=algo-1, epoch=48, batch=10 train loss <loss>=0.0310988903046\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:21 INFO 139741071263552] Epoch[48] Batch [10]#011Speed: 2967.12 samples/sec#011loss=0.031099\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:21 INFO 139741071263552] processed a total of 1317 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2071.1610317230225, \"sum\": 2071.1610317230225, \"min\": 2071.1610317230225}}, \"EndTime\": 1615065621.607617, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1615065619.535814}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:21 INFO 139741071263552] #throughput_metric: host=algo-1, train throughput=635.83651196 records/second\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:21 INFO 139741071263552] #progress_metric: host=algo-1, completed 24 % of epochs\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:21 INFO 139741071263552] #quality_metric: host=algo-1, epoch=48, train loss <loss>=0.272821891037\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:21 INFO 139741071263552] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:21 INFO 139741071263552] Saved checkpoint to \"/opt/ml/model/state_4f66c87d-58bf-4de3-b905-cb33b9d0805d-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 10.781049728393555, \"sum\": 10.781049728393555, \"min\": 10.781049728393555}}, \"EndTime\": 1615065621.619002, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1615065621.607705}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:23 INFO 139741071263552] Epoch[49] Batch[0] avg_epoch_loss=0.541938\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:23 INFO 139741071263552] #quality_metric: host=algo-1, epoch=49, batch=0 train loss <loss>=0.541938185692\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:23 INFO 139741071263552] Epoch[49] Batch[5] avg_epoch_loss=0.714463\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:23 INFO 139741071263552] #quality_metric: host=algo-1, epoch=49, batch=5 train loss <loss>=0.714463363091\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:23 INFO 139741071263552] Epoch[49] Batch [5]#011Speed: 3157.30 samples/sec#011loss=0.714463\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:23 INFO 139741071263552] processed a total of 1232 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2028.2130241394043, \"sum\": 2028.2130241394043, \"min\": 2028.2130241394043}}, \"EndTime\": 1615065623.647331, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1615065621.619064}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:23 INFO 139741071263552] #throughput_metric: host=algo-1, train throughput=607.397134266 records/second\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:23 INFO 139741071263552] #progress_metric: host=algo-1, completed 25 % of epochs\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:23 INFO 139741071263552] #quality_metric: host=algo-1, epoch=49, train loss <loss>=0.684408126771\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:23 INFO 139741071263552] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:25 INFO 139741071263552] Epoch[50] Batch[0] avg_epoch_loss=0.781174\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:25 INFO 139741071263552] #quality_metric: host=algo-1, epoch=50, batch=0 train loss <loss>=0.781174004078\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:25 INFO 139741071263552] Epoch[50] Batch[5] avg_epoch_loss=0.812500\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:25 INFO 139741071263552] #quality_metric: host=algo-1, epoch=50, batch=5 train loss <loss>=0.812500188748\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:25 INFO 139741071263552] Epoch[50] Batch [5]#011Speed: 3080.43 samples/sec#011loss=0.812500\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:25 INFO 139741071263552] processed a total of 1279 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2027.0369052886963, \"sum\": 2027.0369052886963, \"min\": 2027.0369052886963}}, \"EndTime\": 1615065625.675032, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1615065623.647413}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:25 INFO 139741071263552] #throughput_metric: host=algo-1, train throughput=630.932856674 records/second\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:25 INFO 139741071263552] #progress_metric: host=algo-1, completed 25 % of epochs\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:25 INFO 139741071263552] #quality_metric: host=algo-1, epoch=50, train loss <loss>=0.763202768564\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:25 INFO 139741071263552] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:27 INFO 139741071263552] Epoch[51] Batch[0] avg_epoch_loss=0.449270\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:27 INFO 139741071263552] #quality_metric: host=algo-1, epoch=51, batch=0 train loss <loss>=0.449270367622\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:27 INFO 139741071263552] Epoch[51] Batch[5] avg_epoch_loss=0.618049\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:27 INFO 139741071263552] #quality_metric: host=algo-1, epoch=51, batch=5 train loss <loss>=0.618049184481\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:27 INFO 139741071263552] Epoch[51] Batch [5]#011Speed: 3160.41 samples/sec#011loss=0.618049\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:27 INFO 139741071263552] Epoch[51] Batch[10] avg_epoch_loss=0.583493\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:27 INFO 139741071263552] #quality_metric: host=algo-1, epoch=51, batch=10 train loss <loss>=0.542026165128\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:27 INFO 139741071263552] Epoch[51] Batch [10]#011Speed: 2999.61 samples/sec#011loss=0.542026\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:27 INFO 139741071263552] processed a total of 1296 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2023.3612060546875, \"sum\": 2023.3612060546875, \"min\": 2023.3612060546875}}, \"EndTime\": 1615065627.699051, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1615065625.675107}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:27 INFO 139741071263552] #throughput_metric: host=algo-1, train throughput=640.474436367 records/second\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:27 INFO 139741071263552] #progress_metric: host=algo-1, completed 26 % of epochs\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:27 INFO 139741071263552] #quality_metric: host=algo-1, epoch=51, train loss <loss>=0.583493266593\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:27 INFO 139741071263552] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:29 INFO 139741071263552] Epoch[52] Batch[0] avg_epoch_loss=0.496182\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:29 INFO 139741071263552] #quality_metric: host=algo-1, epoch=52, batch=0 train loss <loss>=0.496182233095\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:29 INFO 139741071263552] Epoch[52] Batch[5] avg_epoch_loss=0.545324\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:29 INFO 139741071263552] #quality_metric: host=algo-1, epoch=52, batch=5 train loss <loss>=0.545324499408\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:29 INFO 139741071263552] Epoch[52] Batch [5]#011Speed: 3134.23 samples/sec#011loss=0.545324\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:29 INFO 139741071263552] Epoch[52] Batch[10] avg_epoch_loss=0.429360\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:29 INFO 139741071263552] #quality_metric: host=algo-1, epoch=52, batch=10 train loss <loss>=0.290202063322\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:29 INFO 139741071263552] Epoch[52] Batch [10]#011Speed: 2957.00 samples/sec#011loss=0.290202\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:29 INFO 139741071263552] processed a total of 1304 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2023.7548351287842, \"sum\": 2023.7548351287842, \"min\": 2023.7548351287842}}, \"EndTime\": 1615065629.723413, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1615065627.699147}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:29 INFO 139741071263552] #throughput_metric: host=algo-1, train throughput=644.303785336 records/second\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:29 INFO 139741071263552] #progress_metric: host=algo-1, completed 26 % of epochs\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:29 INFO 139741071263552] #quality_metric: host=algo-1, epoch=52, train loss <loss>=0.429359755733\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:29 INFO 139741071263552] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:31 INFO 139741071263552] Epoch[53] Batch[0] avg_epoch_loss=0.346185\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:31 INFO 139741071263552] #quality_metric: host=algo-1, epoch=53, batch=0 train loss <loss>=0.346185445786\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:31 INFO 139741071263552] Epoch[53] Batch[5] avg_epoch_loss=0.410796\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:31 INFO 139741071263552] #quality_metric: host=algo-1, epoch=53, batch=5 train loss <loss>=0.410795986652\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:31 INFO 139741071263552] Epoch[53] Batch [5]#011Speed: 3113.58 samples/sec#011loss=0.410796\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:31 INFO 139741071263552] Epoch[53] Batch[10] avg_epoch_loss=0.767992\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:31 INFO 139741071263552] #quality_metric: host=algo-1, epoch=53, batch=10 train loss <loss>=1.19662618637\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:31 INFO 139741071263552] Epoch[53] Batch [10]#011Speed: 2985.94 samples/sec#011loss=1.196626\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:31 INFO 139741071263552] processed a total of 1283 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2054.741144180298, \"sum\": 2054.741144180298, \"min\": 2054.741144180298}}, \"EndTime\": 1615065631.778819, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1615065629.723505}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:31 INFO 139741071263552] #throughput_metric: host=algo-1, train throughput=624.367099113 records/second\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:31 INFO 139741071263552] #progress_metric: host=algo-1, completed 27 % of epochs\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:31 INFO 139741071263552] #quality_metric: host=algo-1, epoch=53, train loss <loss>=0.767991531979\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:31 INFO 139741071263552] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:33 INFO 139741071263552] Epoch[54] Batch[0] avg_epoch_loss=0.272434\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:33 INFO 139741071263552] #quality_metric: host=algo-1, epoch=54, batch=0 train loss <loss>=0.272434324026\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:33 INFO 139741071263552] Epoch[54] Batch[5] avg_epoch_loss=0.548723\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:33 INFO 139741071263552] #quality_metric: host=algo-1, epoch=54, batch=5 train loss <loss>=0.548723404606\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:33 INFO 139741071263552] Epoch[54] Batch [5]#011Speed: 2998.70 samples/sec#011loss=0.548723\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:33 INFO 139741071263552] Epoch[54] Batch[10] avg_epoch_loss=0.643513\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:33 INFO 139741071263552] #quality_metric: host=algo-1, epoch=54, batch=10 train loss <loss>=0.757260608673\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:33 INFO 139741071263552] Epoch[54] Batch [10]#011Speed: 2943.38 samples/sec#011loss=0.757261\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:33 INFO 139741071263552] processed a total of 1299 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2040.38405418396, \"sum\": 2040.38405418396, \"min\": 2040.38405418396}}, \"EndTime\": 1615065633.81987, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1615065631.778915}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:33 INFO 139741071263552] #throughput_metric: host=algo-1, train throughput=636.599920431 records/second\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:33 INFO 139741071263552] #progress_metric: host=algo-1, completed 27 % of epochs\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:33 INFO 139741071263552] #quality_metric: host=algo-1, epoch=54, train loss <loss>=0.643513042818\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:33 INFO 139741071263552] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:35 INFO 139741071263552] Epoch[55] Batch[0] avg_epoch_loss=1.103751\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:35 INFO 139741071263552] #quality_metric: host=algo-1, epoch=55, batch=0 train loss <loss>=1.10375106335\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:35 INFO 139741071263552] Epoch[55] Batch[5] avg_epoch_loss=0.964881\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:35 INFO 139741071263552] #quality_metric: host=algo-1, epoch=55, batch=5 train loss <loss>=0.96488070488\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:35 INFO 139741071263552] Epoch[55] Batch [5]#011Speed: 2968.19 samples/sec#011loss=0.964881\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:35 INFO 139741071263552] Epoch[55] Batch[10] avg_epoch_loss=0.875114\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:35 INFO 139741071263552] #quality_metric: host=algo-1, epoch=55, batch=10 train loss <loss>=0.767393413186\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:35 INFO 139741071263552] Epoch[55] Batch [10]#011Speed: 2858.51 samples/sec#011loss=0.767393\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:35 INFO 139741071263552] processed a total of 1296 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2092.0569896698, \"sum\": 2092.0569896698, \"min\": 2092.0569896698}}, \"EndTime\": 1615065635.912459, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1615065633.819975}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:35 INFO 139741071263552] #throughput_metric: host=algo-1, train throughput=619.4383416 records/second\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:35 INFO 139741071263552] #progress_metric: host=algo-1, completed 28 % of epochs\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:35 INFO 139741071263552] #quality_metric: host=algo-1, epoch=55, train loss <loss>=0.87511375411\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:35 INFO 139741071263552] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:37 INFO 139741071263552] Epoch[56] Batch[0] avg_epoch_loss=1.062415\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:37 INFO 139741071263552] #quality_metric: host=algo-1, epoch=56, batch=0 train loss <loss>=1.06241512299\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:37 INFO 139741071263552] Epoch[56] Batch[5] avg_epoch_loss=0.629967\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:37 INFO 139741071263552] #quality_metric: host=algo-1, epoch=56, batch=5 train loss <loss>=0.629967391491\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:37 INFO 139741071263552] Epoch[56] Batch [5]#011Speed: 2863.80 samples/sec#011loss=0.629967\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:37 INFO 139741071263552] processed a total of 1222 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1999.7339248657227, \"sum\": 1999.7339248657227, \"min\": 1999.7339248657227}}, \"EndTime\": 1615065637.912865, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1615065635.912574}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:37 INFO 139741071263552] #throughput_metric: host=algo-1, train throughput=611.030228799 records/second\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:37 INFO 139741071263552] #progress_metric: host=algo-1, completed 28 % of epochs\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:37 INFO 139741071263552] #quality_metric: host=algo-1, epoch=56, train loss <loss>=0.641120305657\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:37 INFO 139741071263552] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:39 INFO 139741071263552] Epoch[57] Batch[0] avg_epoch_loss=0.399002\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:39 INFO 139741071263552] #quality_metric: host=algo-1, epoch=57, batch=0 train loss <loss>=0.3990021348\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:39 INFO 139741071263552] Epoch[57] Batch[5] avg_epoch_loss=0.478066\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:39 INFO 139741071263552] #quality_metric: host=algo-1, epoch=57, batch=5 train loss <loss>=0.478066166242\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:39 INFO 139741071263552] Epoch[57] Batch [5]#011Speed: 3032.86 samples/sec#011loss=0.478066\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:39 INFO 139741071263552] Epoch[57] Batch[10] avg_epoch_loss=0.680140\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:39 INFO 139741071263552] #quality_metric: host=algo-1, epoch=57, batch=10 train loss <loss>=0.922628337145\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:39 INFO 139741071263552] Epoch[57] Batch [10]#011Speed: 2905.62 samples/sec#011loss=0.922628\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:39 INFO 139741071263552] processed a total of 1329 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2046.5340614318848, \"sum\": 2046.5340614318848, \"min\": 2046.5340614318848}}, \"EndTime\": 1615065639.960245, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1615065637.912983}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:39 INFO 139741071263552] #throughput_metric: host=algo-1, train throughput=649.3493804 records/second\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:39 INFO 139741071263552] #progress_metric: host=algo-1, completed 29 % of epochs\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:39 INFO 139741071263552] #quality_metric: host=algo-1, epoch=57, train loss <loss>=0.680139880289\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:39 INFO 139741071263552] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:41 INFO 139741071263552] Epoch[58] Batch[0] avg_epoch_loss=0.684795\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:41 INFO 139741071263552] #quality_metric: host=algo-1, epoch=58, batch=0 train loss <loss>=0.684794723988\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:41 INFO 139741071263552] Epoch[58] Batch[5] avg_epoch_loss=0.596665\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:41 INFO 139741071263552] #quality_metric: host=algo-1, epoch=58, batch=5 train loss <loss>=0.596665407221\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:41 INFO 139741071263552] Epoch[58] Batch [5]#011Speed: 2921.53 samples/sec#011loss=0.596665\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:42 INFO 139741071263552] Epoch[58] Batch[10] avg_epoch_loss=0.510742\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:42 INFO 139741071263552] #quality_metric: host=algo-1, epoch=58, batch=10 train loss <loss>=0.407634526491\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:42 INFO 139741071263552] Epoch[58] Batch [10]#011Speed: 2967.34 samples/sec#011loss=0.407635\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:42 INFO 139741071263552] processed a total of 1372 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2234.671115875244, \"sum\": 2234.671115875244, \"min\": 2234.671115875244}}, \"EndTime\": 1615065642.195426, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1615065639.960334}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:42 INFO 139741071263552] #throughput_metric: host=algo-1, train throughput=613.92626934 records/second\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:42 INFO 139741071263552] #progress_metric: host=algo-1, completed 29 % of epochs\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:42 INFO 139741071263552] #quality_metric: host=algo-1, epoch=58, train loss <loss>=0.510742279616\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:42 INFO 139741071263552] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:43 INFO 139741071263552] Epoch[59] Batch[0] avg_epoch_loss=0.457539\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:43 INFO 139741071263552] #quality_metric: host=algo-1, epoch=59, batch=0 train loss <loss>=0.457538962364\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:44 INFO 139741071263552] Epoch[59] Batch[5] avg_epoch_loss=0.560224\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:44 INFO 139741071263552] #quality_metric: host=algo-1, epoch=59, batch=5 train loss <loss>=0.560223663847\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:44 INFO 139741071263552] Epoch[59] Batch [5]#011Speed: 3030.29 samples/sec#011loss=0.560224\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:44 INFO 139741071263552] processed a total of 1249 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2050.906181335449, \"sum\": 2050.906181335449, \"min\": 2050.906181335449}}, \"EndTime\": 1615065644.246913, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1615065642.195513}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:44 INFO 139741071263552] #throughput_metric: host=algo-1, train throughput=608.962349827 records/second\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:44 INFO 139741071263552] #progress_metric: host=algo-1, completed 30 % of epochs\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:44 INFO 139741071263552] #quality_metric: host=algo-1, epoch=59, train loss <loss>=0.442899407446\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:44 INFO 139741071263552] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:45 INFO 139741071263552] Epoch[60] Batch[0] avg_epoch_loss=0.439085\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:45 INFO 139741071263552] #quality_metric: host=algo-1, epoch=60, batch=0 train loss <loss>=0.439085334539\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:46 INFO 139741071263552] Epoch[60] Batch[5] avg_epoch_loss=0.425914\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:46 INFO 139741071263552] #quality_metric: host=algo-1, epoch=60, batch=5 train loss <loss>=0.425914297501\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:46 INFO 139741071263552] Epoch[60] Batch [5]#011Speed: 2639.53 samples/sec#011loss=0.425914\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:46 INFO 139741071263552] processed a total of 1256 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2152.2860527038574, \"sum\": 2152.2860527038574, \"min\": 2152.2860527038574}}, \"EndTime\": 1615065646.399767, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1615065644.246995}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:46 INFO 139741071263552] #throughput_metric: host=algo-1, train throughput=583.530323762 records/second\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:46 INFO 139741071263552] #progress_metric: host=algo-1, completed 30 % of epochs\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:46 INFO 139741071263552] #quality_metric: host=algo-1, epoch=60, train loss <loss>=0.501162239909\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:46 INFO 139741071263552] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:48 INFO 139741071263552] Epoch[61] Batch[0] avg_epoch_loss=0.671213\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:48 INFO 139741071263552] #quality_metric: host=algo-1, epoch=61, batch=0 train loss <loss>=0.671212792397\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:48 INFO 139741071263552] Epoch[61] Batch[5] avg_epoch_loss=0.425138\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:48 INFO 139741071263552] #quality_metric: host=algo-1, epoch=61, batch=5 train loss <loss>=0.425137793024\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:48 INFO 139741071263552] Epoch[61] Batch [5]#011Speed: 3078.17 samples/sec#011loss=0.425138\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:48 INFO 139741071263552] Epoch[61] Batch[10] avg_epoch_loss=0.168020\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:48 INFO 139741071263552] #quality_metric: host=algo-1, epoch=61, batch=10 train loss <loss>=-0.140520393848\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:48 INFO 139741071263552] Epoch[61] Batch [10]#011Speed: 2991.37 samples/sec#011loss=-0.140520\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:48 INFO 139741071263552] processed a total of 1328 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2040.6529903411865, \"sum\": 2040.6529903411865, \"min\": 2040.6529903411865}}, \"EndTime\": 1615065648.441099, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1615065646.399844}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:48 INFO 139741071263552] #throughput_metric: host=algo-1, train throughput=650.727456293 records/second\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:48 INFO 139741071263552] #progress_metric: host=algo-1, completed 31 % of epochs\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:48 INFO 139741071263552] #quality_metric: host=algo-1, epoch=61, train loss <loss>=0.168020435355\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:48 INFO 139741071263552] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:48 INFO 139741071263552] Saved checkpoint to \"/opt/ml/model/state_6e4f4e04-9e48-4d2c-920e-eaaa855b87ca-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 8.78000259399414, \"sum\": 8.78000259399414, \"min\": 8.78000259399414}}, \"EndTime\": 1615065648.450556, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1615065648.441195}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:50 INFO 139741071263552] Epoch[62] Batch[0] avg_epoch_loss=0.581033\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:50 INFO 139741071263552] #quality_metric: host=algo-1, epoch=62, batch=0 train loss <loss>=0.581033229828\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:50 INFO 139741071263552] Epoch[62] Batch[5] avg_epoch_loss=0.551883\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:50 INFO 139741071263552] #quality_metric: host=algo-1, epoch=62, batch=5 train loss <loss>=0.551883126299\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:50 INFO 139741071263552] Epoch[62] Batch [5]#011Speed: 2962.72 samples/sec#011loss=0.551883\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:50 INFO 139741071263552] processed a total of 1277 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2033.397912979126, \"sum\": 2033.397912979126, \"min\": 2033.397912979126}}, \"EndTime\": 1615065650.484068, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1615065648.450603}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:50 INFO 139741071263552] #throughput_metric: host=algo-1, train throughput=627.977350671 records/second\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:50 INFO 139741071263552] #progress_metric: host=algo-1, completed 31 % of epochs\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:50 INFO 139741071263552] #quality_metric: host=algo-1, epoch=62, train loss <loss>=0.594155842066\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:50 INFO 139741071263552] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:52 INFO 139741071263552] Epoch[63] Batch[0] avg_epoch_loss=0.419844\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:52 INFO 139741071263552] #quality_metric: host=algo-1, epoch=63, batch=0 train loss <loss>=0.419844210148\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:52 INFO 139741071263552] Epoch[63] Batch[5] avg_epoch_loss=0.406830\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:52 INFO 139741071263552] #quality_metric: host=algo-1, epoch=63, batch=5 train loss <loss>=0.406829974304\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:52 INFO 139741071263552] Epoch[63] Batch [5]#011Speed: 2974.64 samples/sec#011loss=0.406830\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:52 INFO 139741071263552] processed a total of 1273 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1993.5657978057861, \"sum\": 1993.5657978057861, \"min\": 1993.5657978057861}}, \"EndTime\": 1615065652.478247, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1615065650.48414}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:52 INFO 139741071263552] #throughput_metric: host=algo-1, train throughput=638.516723225 records/second\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:52 INFO 139741071263552] #progress_metric: host=algo-1, completed 32 % of epochs\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:52 INFO 139741071263552] #quality_metric: host=algo-1, epoch=63, train loss <loss>=0.576884973794\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:52 INFO 139741071263552] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:54 INFO 139741071263552] Epoch[64] Batch[0] avg_epoch_loss=-0.075637\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:54 INFO 139741071263552] #quality_metric: host=algo-1, epoch=64, batch=0 train loss <loss>=-0.0756374448538\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:54 INFO 139741071263552] Epoch[64] Batch[5] avg_epoch_loss=0.530028\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:54 INFO 139741071263552] #quality_metric: host=algo-1, epoch=64, batch=5 train loss <loss>=0.530028300981\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:54 INFO 139741071263552] Epoch[64] Batch [5]#011Speed: 2834.88 samples/sec#011loss=0.530028\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:54 INFO 139741071263552] Epoch[64] Batch[10] avg_epoch_loss=0.816309\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:54 INFO 139741071263552] #quality_metric: host=algo-1, epoch=64, batch=10 train loss <loss>=1.15984649658\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:54 INFO 139741071263552] Epoch[64] Batch [10]#011Speed: 2901.93 samples/sec#011loss=1.159846\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:54 INFO 139741071263552] processed a total of 1300 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2053.131103515625, \"sum\": 2053.131103515625, \"min\": 2053.131103515625}}, \"EndTime\": 1615065654.532018, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1615065652.478326}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:54 INFO 139741071263552] #throughput_metric: host=algo-1, train throughput=633.139835708 records/second\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:54 INFO 139741071263552] #progress_metric: host=algo-1, completed 32 % of epochs\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:54 INFO 139741071263552] #quality_metric: host=algo-1, epoch=64, train loss <loss>=0.816309298981\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:54 INFO 139741071263552] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:56 INFO 139741071263552] Epoch[65] Batch[0] avg_epoch_loss=0.594970\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:56 INFO 139741071263552] #quality_metric: host=algo-1, epoch=65, batch=0 train loss <loss>=0.594970166683\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:56 INFO 139741071263552] Epoch[65] Batch[5] avg_epoch_loss=0.593178\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:56 INFO 139741071263552] #quality_metric: host=algo-1, epoch=65, batch=5 train loss <loss>=0.593177638948\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:56 INFO 139741071263552] Epoch[65] Batch [5]#011Speed: 2975.87 samples/sec#011loss=0.593178\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:56 INFO 139741071263552] Epoch[65] Batch[10] avg_epoch_loss=0.545163\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:56 INFO 139741071263552] #quality_metric: host=algo-1, epoch=65, batch=10 train loss <loss>=0.487544969283\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:56 INFO 139741071263552] Epoch[65] Batch [10]#011Speed: 2949.81 samples/sec#011loss=0.487545\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:56 INFO 139741071263552] processed a total of 1292 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2120.7621097564697, \"sum\": 2120.7621097564697, \"min\": 2120.7621097564697}}, \"EndTime\": 1615065656.653367, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1615065654.532105}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:56 INFO 139741071263552] #throughput_metric: host=algo-1, train throughput=609.179688557 records/second\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:56 INFO 139741071263552] #progress_metric: host=algo-1, completed 33 % of epochs\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:56 INFO 139741071263552] #quality_metric: host=algo-1, epoch=65, train loss <loss>=0.5451627891\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:56 INFO 139741071263552] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:58 INFO 139741071263552] Epoch[66] Batch[0] avg_epoch_loss=0.432818\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:58 INFO 139741071263552] #quality_metric: host=algo-1, epoch=66, batch=0 train loss <loss>=0.432817876339\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:58 INFO 139741071263552] Epoch[66] Batch[5] avg_epoch_loss=0.455494\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:58 INFO 139741071263552] #quality_metric: host=algo-1, epoch=66, batch=5 train loss <loss>=0.455494311949\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:58 INFO 139741071263552] Epoch[66] Batch [5]#011Speed: 2892.05 samples/sec#011loss=0.455494\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:58 INFO 139741071263552] processed a total of 1262 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2013.9179229736328, \"sum\": 2013.9179229736328, \"min\": 2013.9179229736328}}, \"EndTime\": 1615065658.66797, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1615065656.653451}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:58 INFO 139741071263552] #throughput_metric: host=algo-1, train throughput=626.603115654 records/second\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:58 INFO 139741071263552] #progress_metric: host=algo-1, completed 33 % of epochs\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:58 INFO 139741071263552] #quality_metric: host=algo-1, epoch=66, train loss <loss>=0.524182720482\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:20:58 INFO 139741071263552] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:00 INFO 139741071263552] Epoch[67] Batch[0] avg_epoch_loss=0.546986\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:00 INFO 139741071263552] #quality_metric: host=algo-1, epoch=67, batch=0 train loss <loss>=0.546985983849\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:00 INFO 139741071263552] Epoch[67] Batch[5] avg_epoch_loss=0.406511\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:00 INFO 139741071263552] #quality_metric: host=algo-1, epoch=67, batch=5 train loss <loss>=0.406510708233\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:00 INFO 139741071263552] Epoch[67] Batch [5]#011Speed: 2948.16 samples/sec#011loss=0.406511\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:00 INFO 139741071263552] processed a total of 1198 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2050.3499507904053, \"sum\": 2050.3499507904053, \"min\": 2050.3499507904053}}, \"EndTime\": 1615065660.718884, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1615065658.668046}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:00 INFO 139741071263552] #throughput_metric: host=algo-1, train throughput=584.252319876 records/second\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:00 INFO 139741071263552] #progress_metric: host=algo-1, completed 34 % of epochs\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:00 INFO 139741071263552] #quality_metric: host=algo-1, epoch=67, train loss <loss>=0.328097389638\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:00 INFO 139741071263552] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:02 INFO 139741071263552] Epoch[68] Batch[0] avg_epoch_loss=0.383758\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:02 INFO 139741071263552] #quality_metric: host=algo-1, epoch=68, batch=0 train loss <loss>=0.383758038282\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:02 INFO 139741071263552] Epoch[68] Batch[5] avg_epoch_loss=0.540737\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:02 INFO 139741071263552] #quality_metric: host=algo-1, epoch=68, batch=5 train loss <loss>=0.540736585855\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:02 INFO 139741071263552] Epoch[68] Batch [5]#011Speed: 3100.52 samples/sec#011loss=0.540737\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:02 INFO 139741071263552] processed a total of 1198 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2043.7500476837158, \"sum\": 2043.7500476837158, \"min\": 2043.7500476837158}}, \"EndTime\": 1615065662.763297, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1615065660.718972}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:02 INFO 139741071263552] #throughput_metric: host=algo-1, train throughput=586.13721891 records/second\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:02 INFO 139741071263552] #progress_metric: host=algo-1, completed 34 % of epochs\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:02 INFO 139741071263552] #quality_metric: host=algo-1, epoch=68, train loss <loss>=0.595469480753\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:02 INFO 139741071263552] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:04 INFO 139741071263552] Epoch[69] Batch[0] avg_epoch_loss=0.255009\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:04 INFO 139741071263552] #quality_metric: host=algo-1, epoch=69, batch=0 train loss <loss>=0.255008727312\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:04 INFO 139741071263552] Epoch[69] Batch[5] avg_epoch_loss=0.424437\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:04 INFO 139741071263552] #quality_metric: host=algo-1, epoch=69, batch=5 train loss <loss>=0.424437373877\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:04 INFO 139741071263552] Epoch[69] Batch [5]#011Speed: 2146.06 samples/sec#011loss=0.424437\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:05 INFO 139741071263552] processed a total of 1208 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2334.6810340881348, \"sum\": 2334.6810340881348, \"min\": 2334.6810340881348}}, \"EndTime\": 1615065665.098556, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1615065662.763394}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:05 INFO 139741071263552] #throughput_metric: host=algo-1, train throughput=517.385528822 records/second\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:05 INFO 139741071263552] #progress_metric: host=algo-1, completed 35 % of epochs\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:05 INFO 139741071263552] #quality_metric: host=algo-1, epoch=69, train loss <loss>=0.14585493803\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:05 INFO 139741071263552] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:05 INFO 139741071263552] Saved checkpoint to \"/opt/ml/model/state_df2c99e6-3847-4d32-9d8c-393812f27c4c-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 9.134054183959961, \"sum\": 9.134054183959961, \"min\": 9.134054183959961}}, \"EndTime\": 1615065665.108698, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1615065665.098647}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:07 INFO 139741071263552] Epoch[70] Batch[0] avg_epoch_loss=0.376920\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:07 INFO 139741071263552] #quality_metric: host=algo-1, epoch=70, batch=0 train loss <loss>=0.376920282841\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:07 INFO 139741071263552] Epoch[70] Batch[5] avg_epoch_loss=0.470745\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:07 INFO 139741071263552] #quality_metric: host=algo-1, epoch=70, batch=5 train loss <loss>=0.470745205879\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:07 INFO 139741071263552] Epoch[70] Batch [5]#011Speed: 3082.84 samples/sec#011loss=0.470745\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:07 INFO 139741071263552] Epoch[70] Batch[10] avg_epoch_loss=0.459140\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:07 INFO 139741071263552] #quality_metric: host=algo-1, epoch=70, batch=10 train loss <loss>=0.445213975012\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:07 INFO 139741071263552] Epoch[70] Batch [10]#011Speed: 2786.72 samples/sec#011loss=0.445214\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:07 INFO 139741071263552] processed a total of 1390 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2360.6929779052734, \"sum\": 2360.6929779052734, \"min\": 2360.6929779052734}}, \"EndTime\": 1615065667.469542, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1615065665.108776}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:07 INFO 139741071263552] #throughput_metric: host=algo-1, train throughput=588.778996702 records/second\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:07 INFO 139741071263552] #progress_metric: host=algo-1, completed 35 % of epochs\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:07 INFO 139741071263552] #quality_metric: host=algo-1, epoch=70, train loss <loss>=0.45914010094\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:07 INFO 139741071263552] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:09 INFO 139741071263552] Epoch[71] Batch[0] avg_epoch_loss=0.575665\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:09 INFO 139741071263552] #quality_metric: host=algo-1, epoch=71, batch=0 train loss <loss>=0.575664520264\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:09 INFO 139741071263552] Epoch[71] Batch[5] avg_epoch_loss=0.539482\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:09 INFO 139741071263552] #quality_metric: host=algo-1, epoch=71, batch=5 train loss <loss>=0.539482007424\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:09 INFO 139741071263552] Epoch[71] Batch [5]#011Speed: 2993.42 samples/sec#011loss=0.539482\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:09 INFO 139741071263552] processed a total of 1191 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1998.4118938446045, \"sum\": 1998.4118938446045, \"min\": 1998.4118938446045}}, \"EndTime\": 1615065669.468553, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1615065667.469624}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:09 INFO 139741071263552] #throughput_metric: host=algo-1, train throughput=595.935339422 records/second\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:09 INFO 139741071263552] #progress_metric: host=algo-1, completed 36 % of epochs\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:09 INFO 139741071263552] #quality_metric: host=algo-1, epoch=71, train loss <loss>=0.192448705435\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:09 INFO 139741071263552] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:11 INFO 139741071263552] Epoch[72] Batch[0] avg_epoch_loss=0.657271\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:11 INFO 139741071263552] #quality_metric: host=algo-1, epoch=72, batch=0 train loss <loss>=0.657270789146\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:11 INFO 139741071263552] Epoch[72] Batch[5] avg_epoch_loss=0.573568\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:11 INFO 139741071263552] #quality_metric: host=algo-1, epoch=72, batch=5 train loss <loss>=0.573568336666\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:11 INFO 139741071263552] Epoch[72] Batch [5]#011Speed: 2646.34 samples/sec#011loss=0.573568\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:11 INFO 139741071263552] Epoch[72] Batch[10] avg_epoch_loss=0.593080\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:11 INFO 139741071263552] #quality_metric: host=algo-1, epoch=72, batch=10 train loss <loss>=0.616494601965\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:11 INFO 139741071263552] Epoch[72] Batch [10]#011Speed: 2699.03 samples/sec#011loss=0.616495\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:11 INFO 139741071263552] processed a total of 1284 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2144.0179347991943, \"sum\": 2144.0179347991943, \"min\": 2144.0179347991943}}, \"EndTime\": 1615065671.613185, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1615065669.468644}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:11 INFO 139741071263552] #throughput_metric: host=algo-1, train throughput=598.839028369 records/second\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:11 INFO 139741071263552] #progress_metric: host=algo-1, completed 36 % of epochs\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:11 INFO 139741071263552] #quality_metric: host=algo-1, epoch=72, train loss <loss>=0.593080275438\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:11 INFO 139741071263552] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:13 INFO 139741071263552] Epoch[73] Batch[0] avg_epoch_loss=0.873509\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:13 INFO 139741071263552] #quality_metric: host=algo-1, epoch=73, batch=0 train loss <loss>=0.873508810997\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:13 INFO 139741071263552] Epoch[73] Batch[5] avg_epoch_loss=0.528146\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:13 INFO 139741071263552] #quality_metric: host=algo-1, epoch=73, batch=5 train loss <loss>=0.528146083156\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:13 INFO 139741071263552] Epoch[73] Batch [5]#011Speed: 3097.16 samples/sec#011loss=0.528146\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:13 INFO 139741071263552] Epoch[73] Batch[10] avg_epoch_loss=0.819626\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:13 INFO 139741071263552] #quality_metric: host=algo-1, epoch=73, batch=10 train loss <loss>=1.16940255165\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:13 INFO 139741071263552] Epoch[73] Batch [10]#011Speed: 2817.03 samples/sec#011loss=1.169403\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:13 INFO 139741071263552] processed a total of 1284 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2049.1080284118652, \"sum\": 2049.1080284118652, \"min\": 2049.1080284118652}}, \"EndTime\": 1615065673.662796, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1615065671.61327}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:13 INFO 139741071263552] #throughput_metric: host=algo-1, train throughput=626.571022596 records/second\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:13 INFO 139741071263552] #progress_metric: host=algo-1, completed 37 % of epochs\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:13 INFO 139741071263552] #quality_metric: host=algo-1, epoch=73, train loss <loss>=0.819626296108\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:13 INFO 139741071263552] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:15 INFO 139741071263552] Epoch[74] Batch[0] avg_epoch_loss=0.371606\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:15 INFO 139741071263552] #quality_metric: host=algo-1, epoch=74, batch=0 train loss <loss>=0.371605634689\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:15 INFO 139741071263552] Epoch[74] Batch[5] avg_epoch_loss=0.432343\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:15 INFO 139741071263552] #quality_metric: host=algo-1, epoch=74, batch=5 train loss <loss>=0.432343242069\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:15 INFO 139741071263552] Epoch[74] Batch [5]#011Speed: 2883.13 samples/sec#011loss=0.432343\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:15 INFO 139741071263552] Epoch[74] Batch[10] avg_epoch_loss=0.650975\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:15 INFO 139741071263552] #quality_metric: host=algo-1, epoch=74, batch=10 train loss <loss>=0.913334077597\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:15 INFO 139741071263552] Epoch[74] Batch [10]#011Speed: 2937.17 samples/sec#011loss=0.913334\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:15 INFO 139741071263552] processed a total of 1326 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2062.8180503845215, \"sum\": 2062.8180503845215, \"min\": 2062.8180503845215}}, \"EndTime\": 1615065675.726259, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1615065673.662893}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:15 INFO 139741071263552] #throughput_metric: host=algo-1, train throughput=642.769848599 records/second\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:15 INFO 139741071263552] #progress_metric: host=algo-1, completed 37 % of epochs\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:15 INFO 139741071263552] #quality_metric: host=algo-1, epoch=74, train loss <loss>=0.650975440036\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:15 INFO 139741071263552] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:17 INFO 139741071263552] Epoch[75] Batch[0] avg_epoch_loss=0.431552\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:17 INFO 139741071263552] #quality_metric: host=algo-1, epoch=75, batch=0 train loss <loss>=0.431552439928\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:17 INFO 139741071263552] Epoch[75] Batch[5] avg_epoch_loss=0.514407\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:17 INFO 139741071263552] #quality_metric: host=algo-1, epoch=75, batch=5 train loss <loss>=0.514407078425\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:17 INFO 139741071263552] Epoch[75] Batch [5]#011Speed: 3073.18 samples/sec#011loss=0.514407\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:17 INFO 139741071263552] processed a total of 1270 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2035.8710289001465, \"sum\": 2035.8710289001465, \"min\": 2035.8710289001465}}, \"EndTime\": 1615065677.76264, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1615065675.726348}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:17 INFO 139741071263552] #throughput_metric: host=algo-1, train throughput=623.774508553 records/second\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:17 INFO 139741071263552] #progress_metric: host=algo-1, completed 38 % of epochs\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:17 INFO 139741071263552] #quality_metric: host=algo-1, epoch=75, train loss <loss>=0.458681496978\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:17 INFO 139741071263552] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:19 INFO 139741071263552] Epoch[76] Batch[0] avg_epoch_loss=0.622826\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:19 INFO 139741071263552] #quality_metric: host=algo-1, epoch=76, batch=0 train loss <loss>=0.622825622559\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:19 INFO 139741071263552] Epoch[76] Batch[5] avg_epoch_loss=0.524958\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:19 INFO 139741071263552] #quality_metric: host=algo-1, epoch=76, batch=5 train loss <loss>=0.524957622091\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:19 INFO 139741071263552] Epoch[76] Batch [5]#011Speed: 3003.02 samples/sec#011loss=0.524958\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:19 INFO 139741071263552] processed a total of 1251 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2002.7010440826416, \"sum\": 2002.7010440826416, \"min\": 2002.7010440826416}}, \"EndTime\": 1615065679.76595, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1615065677.76272}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:19 INFO 139741071263552] #throughput_metric: host=algo-1, train throughput=624.620843613 records/second\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:19 INFO 139741071263552] #progress_metric: host=algo-1, completed 38 % of epochs\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:19 INFO 139741071263552] #quality_metric: host=algo-1, epoch=76, train loss <loss>=0.47237970829\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:19 INFO 139741071263552] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:21 INFO 139741071263552] Epoch[77] Batch[0] avg_epoch_loss=0.462099\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:21 INFO 139741071263552] #quality_metric: host=algo-1, epoch=77, batch=0 train loss <loss>=0.462098836899\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:21 INFO 139741071263552] Epoch[77] Batch[5] avg_epoch_loss=0.497964\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:21 INFO 139741071263552] #quality_metric: host=algo-1, epoch=77, batch=5 train loss <loss>=0.49796406428\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:21 INFO 139741071263552] Epoch[77] Batch [5]#011Speed: 2910.36 samples/sec#011loss=0.497964\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:21 INFO 139741071263552] Epoch[77] Batch[10] avg_epoch_loss=0.497648\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:21 INFO 139741071263552] #quality_metric: host=algo-1, epoch=77, batch=10 train loss <loss>=0.497268775105\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:21 INFO 139741071263552] Epoch[77] Batch [10]#011Speed: 2986.83 samples/sec#011loss=0.497269\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:21 INFO 139741071263552] processed a total of 1335 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2079.648017883301, \"sum\": 2079.648017883301, \"min\": 2079.648017883301}}, \"EndTime\": 1615065681.846251, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1615065679.766021}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:21 INFO 139741071263552] #throughput_metric: host=algo-1, train throughput=641.89699193 records/second\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:21 INFO 139741071263552] #progress_metric: host=algo-1, completed 39 % of epochs\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:21 INFO 139741071263552] #quality_metric: host=algo-1, epoch=77, train loss <loss>=0.497648023746\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:21 INFO 139741071263552] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:23 INFO 139741071263552] Epoch[78] Batch[0] avg_epoch_loss=0.621722\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:23 INFO 139741071263552] #quality_metric: host=algo-1, epoch=78, batch=0 train loss <loss>=0.621721804142\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:23 INFO 139741071263552] Epoch[78] Batch[5] avg_epoch_loss=0.401649\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:23 INFO 139741071263552] #quality_metric: host=algo-1, epoch=78, batch=5 train loss <loss>=0.401648697754\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:23 INFO 139741071263552] Epoch[78] Batch [5]#011Speed: 2952.21 samples/sec#011loss=0.401649\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:23 INFO 139741071263552] Epoch[78] Batch[10] avg_epoch_loss=0.609824\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:23 INFO 139741071263552] #quality_metric: host=algo-1, epoch=78, batch=10 train loss <loss>=0.859635353088\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:23 INFO 139741071263552] Epoch[78] Batch [10]#011Speed: 2980.15 samples/sec#011loss=0.859635\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:23 INFO 139741071263552] processed a total of 1301 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2092.8640365600586, \"sum\": 2092.8640365600586, \"min\": 2092.8640365600586}}, \"EndTime\": 1615065683.939639, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1615065681.846336}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:23 INFO 139741071263552] #throughput_metric: host=algo-1, train throughput=621.594611044 records/second\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:23 INFO 139741071263552] #progress_metric: host=algo-1, completed 39 % of epochs\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:23 INFO 139741071263552] #quality_metric: host=algo-1, epoch=78, train loss <loss>=0.609824450179\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:23 INFO 139741071263552] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:25 INFO 139741071263552] Epoch[79] Batch[0] avg_epoch_loss=0.779550\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:25 INFO 139741071263552] #quality_metric: host=algo-1, epoch=79, batch=0 train loss <loss>=0.779550015926\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:25 INFO 139741071263552] Epoch[79] Batch[5] avg_epoch_loss=0.532506\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:25 INFO 139741071263552] #quality_metric: host=algo-1, epoch=79, batch=5 train loss <loss>=0.532505996525\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:25 INFO 139741071263552] Epoch[79] Batch [5]#011Speed: 3025.76 samples/sec#011loss=0.532506\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:25 INFO 139741071263552] processed a total of 1273 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2002.1131038665771, \"sum\": 2002.1131038665771, \"min\": 2002.1131038665771}}, \"EndTime\": 1615065685.942446, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1615065683.939735}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:25 INFO 139741071263552] #throughput_metric: host=algo-1, train throughput=635.793992396 records/second\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:25 INFO 139741071263552] #progress_metric: host=algo-1, completed 40 % of epochs\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:25 INFO 139741071263552] #quality_metric: host=algo-1, epoch=79, train loss <loss>=0.516977547109\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:25 INFO 139741071263552] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:27 INFO 139741071263552] Epoch[80] Batch[0] avg_epoch_loss=0.502880\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:27 INFO 139741071263552] #quality_metric: host=algo-1, epoch=80, batch=0 train loss <loss>=0.502880215645\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:27 INFO 139741071263552] Epoch[80] Batch[5] avg_epoch_loss=0.482854\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:27 INFO 139741071263552] #quality_metric: host=algo-1, epoch=80, batch=5 train loss <loss>=0.482854499171\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:27 INFO 139741071263552] Epoch[80] Batch [5]#011Speed: 2944.72 samples/sec#011loss=0.482854\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:28 INFO 139741071263552] Epoch[80] Batch[10] avg_epoch_loss=0.515760\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:28 INFO 139741071263552] #quality_metric: host=algo-1, epoch=80, batch=10 train loss <loss>=0.55524661839\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:28 INFO 139741071263552] Epoch[80] Batch [10]#011Speed: 2962.83 samples/sec#011loss=0.555247\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:28 INFO 139741071263552] processed a total of 1296 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2076.7970085144043, \"sum\": 2076.7970085144043, \"min\": 2076.7970085144043}}, \"EndTime\": 1615065688.019916, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1615065685.942515}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:28 INFO 139741071263552] #throughput_metric: host=algo-1, train throughput=623.988952408 records/second\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:28 INFO 139741071263552] #progress_metric: host=algo-1, completed 40 % of epochs\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:28 INFO 139741071263552] #quality_metric: host=algo-1, epoch=80, train loss <loss>=0.515760007907\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:28 INFO 139741071263552] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:29 INFO 139741071263552] Epoch[81] Batch[0] avg_epoch_loss=0.962682\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:29 INFO 139741071263552] #quality_metric: host=algo-1, epoch=81, batch=0 train loss <loss>=0.96268171072\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:29 INFO 139741071263552] Epoch[81] Batch[5] avg_epoch_loss=0.728409\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:29 INFO 139741071263552] #quality_metric: host=algo-1, epoch=81, batch=5 train loss <loss>=0.728408594926\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:29 INFO 139741071263552] Epoch[81] Batch [5]#011Speed: 2942.61 samples/sec#011loss=0.728409\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:30 INFO 139741071263552] processed a total of 1264 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2030.8809280395508, \"sum\": 2030.8809280395508, \"min\": 2030.8809280395508}}, \"EndTime\": 1615065690.051352, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1615065688.020006}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:30 INFO 139741071263552] #throughput_metric: host=algo-1, train throughput=622.35106745 records/second\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:30 INFO 139741071263552] #progress_metric: host=algo-1, completed 41 % of epochs\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:30 INFO 139741071263552] #quality_metric: host=algo-1, epoch=81, train loss <loss>=0.669788002968\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:30 INFO 139741071263552] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:31 INFO 139741071263552] Epoch[82] Batch[0] avg_epoch_loss=1.102073\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:31 INFO 139741071263552] #quality_metric: host=algo-1, epoch=82, batch=0 train loss <loss>=1.10207307339\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:31 INFO 139741071263552] Epoch[82] Batch[5] avg_epoch_loss=0.602231\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:31 INFO 139741071263552] #quality_metric: host=algo-1, epoch=82, batch=5 train loss <loss>=0.602230918904\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:31 INFO 139741071263552] Epoch[82] Batch [5]#011Speed: 3042.46 samples/sec#011loss=0.602231\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:32 INFO 139741071263552] Epoch[82] Batch[10] avg_epoch_loss=0.485571\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:32 INFO 139741071263552] #quality_metric: host=algo-1, epoch=82, batch=10 train loss <loss>=0.34557929635\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:32 INFO 139741071263552] Epoch[82] Batch [10]#011Speed: 2861.51 samples/sec#011loss=0.345579\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:32 INFO 139741071263552] processed a total of 1303 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2150.0580310821533, \"sum\": 2150.0580310821533, \"min\": 2150.0580310821533}}, \"EndTime\": 1615065692.20216, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1615065690.051437}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:32 INFO 139741071263552] #throughput_metric: host=algo-1, train throughput=605.992052837 records/second\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:32 INFO 139741071263552] #progress_metric: host=algo-1, completed 41 % of epochs\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:32 INFO 139741071263552] #quality_metric: host=algo-1, epoch=82, train loss <loss>=0.485571090471\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:32 INFO 139741071263552] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:33 INFO 139741071263552] Epoch[83] Batch[0] avg_epoch_loss=0.882661\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:33 INFO 139741071263552] #quality_metric: host=algo-1, epoch=83, batch=0 train loss <loss>=0.882660925388\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:34 INFO 139741071263552] Epoch[83] Batch[5] avg_epoch_loss=0.674193\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:34 INFO 139741071263552] #quality_metric: host=algo-1, epoch=83, batch=5 train loss <loss>=0.674192845821\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:34 INFO 139741071263552] Epoch[83] Batch [5]#011Speed: 3112.98 samples/sec#011loss=0.674193\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:34 INFO 139741071263552] Epoch[83] Batch[10] avg_epoch_loss=0.872560\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:34 INFO 139741071263552] #quality_metric: host=algo-1, epoch=83, batch=10 train loss <loss>=1.11059980392\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:34 INFO 139741071263552] Epoch[83] Batch [10]#011Speed: 2902.36 samples/sec#011loss=1.110600\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:34 INFO 139741071263552] processed a total of 1311 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2046.8101501464844, \"sum\": 2046.8101501464844, \"min\": 2046.8101501464844}}, \"EndTime\": 1615065694.249545, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1615065692.202252}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:34 INFO 139741071263552] #throughput_metric: host=algo-1, train throughput=640.470421442 records/second\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:34 INFO 139741071263552] #progress_metric: host=algo-1, completed 42 % of epochs\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:34 INFO 139741071263552] #quality_metric: host=algo-1, epoch=83, train loss <loss>=0.872559644959\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:34 INFO 139741071263552] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:35 INFO 139741071263552] Epoch[84] Batch[0] avg_epoch_loss=0.703440\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:35 INFO 139741071263552] #quality_metric: host=algo-1, epoch=84, batch=0 train loss <loss>=0.703440010548\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:36 INFO 139741071263552] Epoch[84] Batch[5] avg_epoch_loss=0.644102\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:36 INFO 139741071263552] #quality_metric: host=algo-1, epoch=84, batch=5 train loss <loss>=0.644102270404\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:36 INFO 139741071263552] Epoch[84] Batch [5]#011Speed: 3062.67 samples/sec#011loss=0.644102\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:36 INFO 139741071263552] processed a total of 1278 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2106.7450046539307, \"sum\": 2106.7450046539307, \"min\": 2106.7450046539307}}, \"EndTime\": 1615065696.356803, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1615065694.249631}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:36 INFO 139741071263552] #throughput_metric: host=algo-1, train throughput=606.414590368 records/second\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:36 INFO 139741071263552] #progress_metric: host=algo-1, completed 42 % of epochs\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:36 INFO 139741071263552] #quality_metric: host=algo-1, epoch=84, train loss <loss>=0.510813726485\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:36 INFO 139741071263552] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:38 INFO 139741071263552] Epoch[85] Batch[0] avg_epoch_loss=0.395721\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:38 INFO 139741071263552] #quality_metric: host=algo-1, epoch=85, batch=0 train loss <loss>=0.395721375942\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:38 INFO 139741071263552] Epoch[85] Batch[5] avg_epoch_loss=0.351768\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:38 INFO 139741071263552] #quality_metric: host=algo-1, epoch=85, batch=5 train loss <loss>=0.351768049101\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:38 INFO 139741071263552] Epoch[85] Batch [5]#011Speed: 2532.02 samples/sec#011loss=0.351768\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:38 INFO 139741071263552] processed a total of 1223 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2066.2529468536377, \"sum\": 2066.2529468536377, \"min\": 2066.2529468536377}}, \"EndTime\": 1615065698.426865, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1615065696.35748}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:38 INFO 139741071263552] #throughput_metric: host=algo-1, train throughput=591.858877848 records/second\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:38 INFO 139741071263552] #progress_metric: host=algo-1, completed 43 % of epochs\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:38 INFO 139741071263552] #quality_metric: host=algo-1, epoch=85, train loss <loss>=0.375413419306\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:38 INFO 139741071263552] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:40 INFO 139741071263552] Epoch[86] Batch[0] avg_epoch_loss=0.242907\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:40 INFO 139741071263552] #quality_metric: host=algo-1, epoch=86, batch=0 train loss <loss>=0.242907181382\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:40 INFO 139741071263552] Epoch[86] Batch[5] avg_epoch_loss=0.373640\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:40 INFO 139741071263552] #quality_metric: host=algo-1, epoch=86, batch=5 train loss <loss>=0.37363995115\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:40 INFO 139741071263552] Epoch[86] Batch [5]#011Speed: 3065.83 samples/sec#011loss=0.373640\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:40 INFO 139741071263552] Epoch[86] Batch[10] avg_epoch_loss=0.387293\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:40 INFO 139741071263552] #quality_metric: host=algo-1, epoch=86, batch=10 train loss <loss>=0.403676125407\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:40 INFO 139741071263552] Epoch[86] Batch [10]#011Speed: 2902.36 samples/sec#011loss=0.403676\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:40 INFO 139741071263552] processed a total of 1305 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2054.9991130828857, \"sum\": 2054.9991130828857, \"min\": 2054.9991130828857}}, \"EndTime\": 1615065700.482477, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1615065698.426947}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:40 INFO 139741071263552] #throughput_metric: host=algo-1, train throughput=634.997282428 records/second\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:40 INFO 139741071263552] #progress_metric: host=algo-1, completed 43 % of epochs\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:40 INFO 139741071263552] #quality_metric: host=algo-1, epoch=86, train loss <loss>=0.38729275763\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:40 INFO 139741071263552] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:42 INFO 139741071263552] Epoch[87] Batch[0] avg_epoch_loss=0.767535\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:42 INFO 139741071263552] #quality_metric: host=algo-1, epoch=87, batch=0 train loss <loss>=0.767534971237\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:42 INFO 139741071263552] Epoch[87] Batch[5] avg_epoch_loss=0.571197\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:42 INFO 139741071263552] #quality_metric: host=algo-1, epoch=87, batch=5 train loss <loss>=0.571196913719\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:42 INFO 139741071263552] Epoch[87] Batch [5]#011Speed: 3080.97 samples/sec#011loss=0.571197\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:42 INFO 139741071263552] Epoch[87] Batch[10] avg_epoch_loss=0.673752\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:42 INFO 139741071263552] #quality_metric: host=algo-1, epoch=87, batch=10 train loss <loss>=0.79681811817\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:42 INFO 139741071263552] Epoch[87] Batch [10]#011Speed: 2673.77 samples/sec#011loss=0.796818\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:42 INFO 139741071263552] processed a total of 1284 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2174.647092819214, \"sum\": 2174.647092819214, \"min\": 2174.647092819214}}, \"EndTime\": 1615065702.65766, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1615065700.482564}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:42 INFO 139741071263552] #throughput_metric: host=algo-1, train throughput=590.406453037 records/second\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:42 INFO 139741071263552] #progress_metric: host=algo-1, completed 44 % of epochs\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:42 INFO 139741071263552] #quality_metric: host=algo-1, epoch=87, train loss <loss>=0.673752006651\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:42 INFO 139741071263552] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:44 INFO 139741071263552] Epoch[88] Batch[0] avg_epoch_loss=0.674957\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:44 INFO 139741071263552] #quality_metric: host=algo-1, epoch=88, batch=0 train loss <loss>=0.674957096577\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:44 INFO 139741071263552] Epoch[88] Batch[5] avg_epoch_loss=0.637428\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:44 INFO 139741071263552] #quality_metric: host=algo-1, epoch=88, batch=5 train loss <loss>=0.637428407868\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:44 INFO 139741071263552] Epoch[88] Batch [5]#011Speed: 2684.62 samples/sec#011loss=0.637428\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:44 INFO 139741071263552] processed a total of 1224 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2093.238115310669, \"sum\": 2093.238115310669, \"min\": 2093.238115310669}}, \"EndTime\": 1615065704.751417, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1615065702.657746}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:44 INFO 139741071263552] #throughput_metric: host=algo-1, train throughput=584.705076342 records/second\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:44 INFO 139741071263552] #progress_metric: host=algo-1, completed 44 % of epochs\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:44 INFO 139741071263552] #quality_metric: host=algo-1, epoch=88, train loss <loss>=0.532086453959\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:44 INFO 139741071263552] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:46 INFO 139741071263552] Epoch[89] Batch[0] avg_epoch_loss=0.849136\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:46 INFO 139741071263552] #quality_metric: host=algo-1, epoch=89, batch=0 train loss <loss>=0.84913611412\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:46 INFO 139741071263552] Epoch[89] Batch[5] avg_epoch_loss=0.451330\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:46 INFO 139741071263552] #quality_metric: host=algo-1, epoch=89, batch=5 train loss <loss>=0.451329739764\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:46 INFO 139741071263552] Epoch[89] Batch [5]#011Speed: 2673.48 samples/sec#011loss=0.451330\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:46 INFO 139741071263552] processed a total of 1266 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2018.7718868255615, \"sum\": 2018.7718868255615, \"min\": 2018.7718868255615}}, \"EndTime\": 1615065706.770779, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1615065704.751501}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:46 INFO 139741071263552] #throughput_metric: host=algo-1, train throughput=627.075359737 records/second\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:46 INFO 139741071263552] #progress_metric: host=algo-1, completed 45 % of epochs\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:46 INFO 139741071263552] #quality_metric: host=algo-1, epoch=89, train loss <loss>=0.44112293832\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:46 INFO 139741071263552] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:48 INFO 139741071263552] Epoch[90] Batch[0] avg_epoch_loss=0.529410\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:48 INFO 139741071263552] #quality_metric: host=algo-1, epoch=90, batch=0 train loss <loss>=0.529410421848\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:48 INFO 139741071263552] Epoch[90] Batch[5] avg_epoch_loss=0.334957\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:48 INFO 139741071263552] #quality_metric: host=algo-1, epoch=90, batch=5 train loss <loss>=0.334957187374\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:48 INFO 139741071263552] Epoch[90] Batch [5]#011Speed: 3152.32 samples/sec#011loss=0.334957\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:48 INFO 139741071263552] processed a total of 1254 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2020.0490951538086, \"sum\": 2020.0490951538086, \"min\": 2020.0490951538086}}, \"EndTime\": 1615065708.791364, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1615065706.770862}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:48 INFO 139741071263552] #throughput_metric: host=algo-1, train throughput=620.740799278 records/second\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:48 INFO 139741071263552] #progress_metric: host=algo-1, completed 45 % of epochs\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:48 INFO 139741071263552] #quality_metric: host=algo-1, epoch=90, train loss <loss>=0.452694669366\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:48 INFO 139741071263552] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:50 INFO 139741071263552] Epoch[91] Batch[0] avg_epoch_loss=0.357076\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:50 INFO 139741071263552] #quality_metric: host=algo-1, epoch=91, batch=0 train loss <loss>=0.357076019049\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:50 INFO 139741071263552] Epoch[91] Batch[5] avg_epoch_loss=0.394971\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:50 INFO 139741071263552] #quality_metric: host=algo-1, epoch=91, batch=5 train loss <loss>=0.394971340895\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:50 INFO 139741071263552] Epoch[91] Batch [5]#011Speed: 3101.39 samples/sec#011loss=0.394971\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:50 INFO 139741071263552] processed a total of 1203 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1975.3990173339844, \"sum\": 1975.3990173339844, \"min\": 1975.3990173339844}}, \"EndTime\": 1615065710.767425, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1615065708.791445}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:50 INFO 139741071263552] #throughput_metric: host=algo-1, train throughput=608.949582102 records/second\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:50 INFO 139741071263552] #progress_metric: host=algo-1, completed 46 % of epochs\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:50 INFO 139741071263552] #quality_metric: host=algo-1, epoch=91, train loss <loss>=0.345829442143\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:50 INFO 139741071263552] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:52 INFO 139741071263552] Epoch[92] Batch[0] avg_epoch_loss=0.626589\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:52 INFO 139741071263552] #quality_metric: host=algo-1, epoch=92, batch=0 train loss <loss>=0.62658905983\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:52 INFO 139741071263552] Epoch[92] Batch[5] avg_epoch_loss=0.509061\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:52 INFO 139741071263552] #quality_metric: host=algo-1, epoch=92, batch=5 train loss <loss>=0.509061382463\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:52 INFO 139741071263552] Epoch[92] Batch [5]#011Speed: 3153.15 samples/sec#011loss=0.509061\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:52 INFO 139741071263552] processed a total of 1224 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2004.3301582336426, \"sum\": 2004.3301582336426, \"min\": 2004.3301582336426}}, \"EndTime\": 1615065712.772332, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1615065710.767516}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:52 INFO 139741071263552] #throughput_metric: host=algo-1, train throughput=610.639482031 records/second\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:52 INFO 139741071263552] #progress_metric: host=algo-1, completed 46 % of epochs\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:52 INFO 139741071263552] #quality_metric: host=algo-1, epoch=92, train loss <loss>=0.560776413232\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:52 INFO 139741071263552] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:54 INFO 139741071263552] Epoch[93] Batch[0] avg_epoch_loss=0.306929\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:54 INFO 139741071263552] #quality_metric: host=algo-1, epoch=93, batch=0 train loss <loss>=0.306928515434\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:54 INFO 139741071263552] Epoch[93] Batch[5] avg_epoch_loss=0.492476\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:54 INFO 139741071263552] #quality_metric: host=algo-1, epoch=93, batch=5 train loss <loss>=0.492476016283\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:54 INFO 139741071263552] Epoch[93] Batch [5]#011Speed: 3137.89 samples/sec#011loss=0.492476\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:54 INFO 139741071263552] processed a total of 1279 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2029.5140743255615, \"sum\": 2029.5140743255615, \"min\": 2029.5140743255615}}, \"EndTime\": 1615065714.802395, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1615065712.772416}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:54 INFO 139741071263552] #throughput_metric: host=algo-1, train throughput=630.160730305 records/second\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:54 INFO 139741071263552] #progress_metric: host=algo-1, completed 47 % of epochs\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:54 INFO 139741071263552] #quality_metric: host=algo-1, epoch=93, train loss <loss>=0.482313656807\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:54 INFO 139741071263552] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:56 INFO 139741071263552] Epoch[94] Batch[0] avg_epoch_loss=0.103010\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:56 INFO 139741071263552] #quality_metric: host=algo-1, epoch=94, batch=0 train loss <loss>=0.103009715676\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:56 INFO 139741071263552] Epoch[94] Batch[5] avg_epoch_loss=0.459981\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:56 INFO 139741071263552] #quality_metric: host=algo-1, epoch=94, batch=5 train loss <loss>=0.459981290003\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:56 INFO 139741071263552] Epoch[94] Batch [5]#011Speed: 3146.06 samples/sec#011loss=0.459981\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:56 INFO 139741071263552] Epoch[94] Batch[10] avg_epoch_loss=0.320535\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:56 INFO 139741071263552] #quality_metric: host=algo-1, epoch=94, batch=10 train loss <loss>=0.153198361397\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:56 INFO 139741071263552] Epoch[94] Batch [10]#011Speed: 2383.89 samples/sec#011loss=0.153198\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:56 INFO 139741071263552] processed a total of 1307 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2090.9109115600586, \"sum\": 2090.9109115600586, \"min\": 2090.9109115600586}}, \"EndTime\": 1615065716.893935, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1615065714.802479}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:56 INFO 139741071263552] #throughput_metric: host=algo-1, train throughput=625.044860959 records/second\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:56 INFO 139741071263552] #progress_metric: host=algo-1, completed 47 % of epochs\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:56 INFO 139741071263552] #quality_metric: host=algo-1, epoch=94, train loss <loss>=0.320534504273\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:56 INFO 139741071263552] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:58 INFO 139741071263552] Epoch[95] Batch[0] avg_epoch_loss=0.505048\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:58 INFO 139741071263552] #quality_metric: host=algo-1, epoch=95, batch=0 train loss <loss>=0.505047619343\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:58 INFO 139741071263552] Epoch[95] Batch[5] avg_epoch_loss=0.499153\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:58 INFO 139741071263552] #quality_metric: host=algo-1, epoch=95, batch=5 train loss <loss>=0.499153271317\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:58 INFO 139741071263552] Epoch[95] Batch [5]#011Speed: 3056.16 samples/sec#011loss=0.499153\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:58 INFO 139741071263552] processed a total of 1222 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1990.6117916107178, \"sum\": 1990.6117916107178, \"min\": 1990.6117916107178}}, \"EndTime\": 1615065718.885097, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1615065716.89403}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:58 INFO 139741071263552] #throughput_metric: host=algo-1, train throughput=613.840820352 records/second\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:58 INFO 139741071263552] #progress_metric: host=algo-1, completed 48 % of epochs\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:58 INFO 139741071263552] #quality_metric: host=algo-1, epoch=95, train loss <loss>=0.416323080659\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:21:58 INFO 139741071263552] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:00 INFO 139741071263552] Epoch[96] Batch[0] avg_epoch_loss=0.179292\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:00 INFO 139741071263552] #quality_metric: host=algo-1, epoch=96, batch=0 train loss <loss>=0.179292231798\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:00 INFO 139741071263552] Epoch[96] Batch[5] avg_epoch_loss=0.512628\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:00 INFO 139741071263552] #quality_metric: host=algo-1, epoch=96, batch=5 train loss <loss>=0.512628460924\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:00 INFO 139741071263552] Epoch[96] Batch [5]#011Speed: 2930.62 samples/sec#011loss=0.512628\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:01 INFO 139741071263552] Epoch[96] Batch[10] avg_epoch_loss=0.663996\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:01 INFO 139741071263552] #quality_metric: host=algo-1, epoch=96, batch=10 train loss <loss>=0.845637312531\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:01 INFO 139741071263552] Epoch[96] Batch [10]#011Speed: 2836.24 samples/sec#011loss=0.845637\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:01 INFO 139741071263552] processed a total of 1321 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2133.6941719055176, \"sum\": 2133.6941719055176, \"min\": 2133.6941719055176}}, \"EndTime\": 1615065721.019377, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1615065718.885187}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:01 INFO 139741071263552] #throughput_metric: host=algo-1, train throughput=619.076053829 records/second\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:01 INFO 139741071263552] #progress_metric: host=algo-1, completed 48 % of epochs\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:01 INFO 139741071263552] #quality_metric: host=algo-1, epoch=96, train loss <loss>=0.663996120745\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:01 INFO 139741071263552] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:02 INFO 139741071263552] Epoch[97] Batch[0] avg_epoch_loss=0.811963\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:02 INFO 139741071263552] #quality_metric: host=algo-1, epoch=97, batch=0 train loss <loss>=0.811963319778\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:03 INFO 139741071263552] Epoch[97] Batch[5] avg_epoch_loss=0.530524\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:03 INFO 139741071263552] #quality_metric: host=algo-1, epoch=97, batch=5 train loss <loss>=0.530524104834\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:03 INFO 139741071263552] Epoch[97] Batch [5]#011Speed: 2261.82 samples/sec#011loss=0.530524\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:03 INFO 139741071263552] processed a total of 1265 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2305.889129638672, \"sum\": 2305.889129638672, \"min\": 2305.889129638672}}, \"EndTime\": 1615065723.325836, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1615065721.019468}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:03 INFO 139741071263552] #throughput_metric: host=algo-1, train throughput=548.55681479 records/second\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:03 INFO 139741071263552] #progress_metric: host=algo-1, completed 49 % of epochs\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:03 INFO 139741071263552] #quality_metric: host=algo-1, epoch=97, train loss <loss>=0.575685226917\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:03 INFO 139741071263552] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:05 INFO 139741071263552] Epoch[98] Batch[0] avg_epoch_loss=0.750570\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:05 INFO 139741071263552] #quality_metric: host=algo-1, epoch=98, batch=0 train loss <loss>=0.750570118427\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:05 INFO 139741071263552] Epoch[98] Batch[5] avg_epoch_loss=0.579878\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:05 INFO 139741071263552] #quality_metric: host=algo-1, epoch=98, batch=5 train loss <loss>=0.579877972603\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:05 INFO 139741071263552] Epoch[98] Batch [5]#011Speed: 2078.79 samples/sec#011loss=0.579878\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:05 INFO 139741071263552] processed a total of 1229 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2492.340087890625, \"sum\": 2492.340087890625, \"min\": 2492.340087890625}}, \"EndTime\": 1615065725.819189, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1615065723.325955}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:05 INFO 139741071263552] #throughput_metric: host=algo-1, train throughput=493.084790063 records/second\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:05 INFO 139741071263552] #progress_metric: host=algo-1, completed 49 % of epochs\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:05 INFO 139741071263552] #quality_metric: host=algo-1, epoch=98, train loss <loss>=0.635505124927\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:05 INFO 139741071263552] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:07 INFO 139741071263552] Epoch[99] Batch[0] avg_epoch_loss=0.420413\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:07 INFO 139741071263552] #quality_metric: host=algo-1, epoch=99, batch=0 train loss <loss>=0.420413404703\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:07 INFO 139741071263552] Epoch[99] Batch[5] avg_epoch_loss=0.454759\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:07 INFO 139741071263552] #quality_metric: host=algo-1, epoch=99, batch=5 train loss <loss>=0.454758858929\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:07 INFO 139741071263552] Epoch[99] Batch [5]#011Speed: 3094.18 samples/sec#011loss=0.454759\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:07 INFO 139741071263552] processed a total of 1275 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2022.7699279785156, \"sum\": 2022.7699279785156, \"min\": 2022.7699279785156}}, \"EndTime\": 1615065727.842569, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1615065725.819276}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:07 INFO 139741071263552] #throughput_metric: host=algo-1, train throughput=630.28389271 records/second\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:07 INFO 139741071263552] #progress_metric: host=algo-1, completed 50 % of epochs\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:07 INFO 139741071263552] #quality_metric: host=algo-1, epoch=99, train loss <loss>=0.447358877212\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:07 INFO 139741071263552] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:09 INFO 139741071263552] Epoch[100] Batch[0] avg_epoch_loss=0.537061\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:09 INFO 139741071263552] #quality_metric: host=algo-1, epoch=100, batch=0 train loss <loss>=0.537061274052\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:09 INFO 139741071263552] Epoch[100] Batch[5] avg_epoch_loss=0.477374\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:09 INFO 139741071263552] #quality_metric: host=algo-1, epoch=100, batch=5 train loss <loss>=0.477373828491\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:09 INFO 139741071263552] Epoch[100] Batch [5]#011Speed: 3093.63 samples/sec#011loss=0.477374\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:09 INFO 139741071263552] processed a total of 1224 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2043.8709259033203, \"sum\": 2043.8709259033203, \"min\": 2043.8709259033203}}, \"EndTime\": 1615065729.886997, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1615065727.842654}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:09 INFO 139741071263552] #throughput_metric: host=algo-1, train throughput=598.821736907 records/second\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:09 INFO 139741071263552] #progress_metric: host=algo-1, completed 50 % of epochs\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:09 INFO 139741071263552] #quality_metric: host=algo-1, epoch=100, train loss <loss>=0.576853251457\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:09 INFO 139741071263552] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:11 INFO 139741071263552] Epoch[101] Batch[0] avg_epoch_loss=0.567809\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:11 INFO 139741071263552] #quality_metric: host=algo-1, epoch=101, batch=0 train loss <loss>=0.567808628082\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:11 INFO 139741071263552] Epoch[101] Batch[5] avg_epoch_loss=0.292033\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:11 INFO 139741071263552] #quality_metric: host=algo-1, epoch=101, batch=5 train loss <loss>=0.292033139616\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:11 INFO 139741071263552] Epoch[101] Batch [5]#011Speed: 2882.39 samples/sec#011loss=0.292033\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:11 INFO 139741071263552] processed a total of 1257 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2093.2528972625732, \"sum\": 2093.2528972625732, \"min\": 2093.2528972625732}}, \"EndTime\": 1615065731.980826, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1615065729.887095}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:11 INFO 139741071263552] #throughput_metric: host=algo-1, train throughput=600.390888595 records/second\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:11 INFO 139741071263552] #progress_metric: host=algo-1, completed 51 % of epochs\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:11 INFO 139741071263552] #quality_metric: host=algo-1, epoch=101, train loss <loss>=0.326438651979\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:11 INFO 139741071263552] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:13 INFO 139741071263552] Epoch[102] Batch[0] avg_epoch_loss=0.169893\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:13 INFO 139741071263552] #quality_metric: host=algo-1, epoch=102, batch=0 train loss <loss>=0.169893145561\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:13 INFO 139741071263552] Epoch[102] Batch[5] avg_epoch_loss=0.570326\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:13 INFO 139741071263552] #quality_metric: host=algo-1, epoch=102, batch=5 train loss <loss>=0.570325985551\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:13 INFO 139741071263552] Epoch[102] Batch [5]#011Speed: 3099.48 samples/sec#011loss=0.570326\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:14 INFO 139741071263552] processed a total of 1250 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2086.0819816589355, \"sum\": 2086.0819816589355, \"min\": 2086.0819816589355}}, \"EndTime\": 1615065734.068388, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1615065731.981163}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:14 INFO 139741071263552] #throughput_metric: host=algo-1, train throughput=599.171768638 records/second\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:14 INFO 139741071263552] #progress_metric: host=algo-1, completed 51 % of epochs\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:14 INFO 139741071263552] #quality_metric: host=algo-1, epoch=102, train loss <loss>=0.616239216924\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:14 INFO 139741071263552] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:15 INFO 139741071263552] Epoch[103] Batch[0] avg_epoch_loss=0.570558\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:15 INFO 139741071263552] #quality_metric: host=algo-1, epoch=103, batch=0 train loss <loss>=0.570558190346\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:15 INFO 139741071263552] Epoch[103] Batch[5] avg_epoch_loss=0.614032\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:15 INFO 139741071263552] #quality_metric: host=algo-1, epoch=103, batch=5 train loss <loss>=0.614031657577\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:15 INFO 139741071263552] Epoch[103] Batch [5]#011Speed: 3179.16 samples/sec#011loss=0.614032\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:16 INFO 139741071263552] Epoch[103] Batch[10] avg_epoch_loss=0.705836\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:16 INFO 139741071263552] #quality_metric: host=algo-1, epoch=103, batch=10 train loss <loss>=0.816001029313\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:16 INFO 139741071263552] Epoch[103] Batch [10]#011Speed: 2722.15 samples/sec#011loss=0.816001\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:16 INFO 139741071263552] processed a total of 1298 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2074.730157852173, \"sum\": 2074.730157852173, \"min\": 2074.730157852173}}, \"EndTime\": 1615065736.143695, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1615065734.068476}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:16 INFO 139741071263552] #throughput_metric: host=algo-1, train throughput=625.5840603 records/second\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:16 INFO 139741071263552] #progress_metric: host=algo-1, completed 52 % of epochs\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:16 INFO 139741071263552] #quality_metric: host=algo-1, epoch=103, train loss <loss>=0.705835917457\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:16 INFO 139741071263552] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:17 INFO 139741071263552] Epoch[104] Batch[0] avg_epoch_loss=0.641977\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:17 INFO 139741071263552] #quality_metric: host=algo-1, epoch=104, batch=0 train loss <loss>=0.641976773739\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:17 INFO 139741071263552] Epoch[104] Batch[5] avg_epoch_loss=0.446882\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:17 INFO 139741071263552] #quality_metric: host=algo-1, epoch=104, batch=5 train loss <loss>=0.446881656845\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:17 INFO 139741071263552] Epoch[104] Batch [5]#011Speed: 3064.62 samples/sec#011loss=0.446882\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:18 INFO 139741071263552] processed a total of 1266 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2021.009922027588, \"sum\": 2021.009922027588, \"min\": 2021.009922027588}}, \"EndTime\": 1615065738.165351, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1615065736.143782}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:18 INFO 139741071263552] #throughput_metric: host=algo-1, train throughput=626.381358285 records/second\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:18 INFO 139741071263552] #progress_metric: host=algo-1, completed 52 % of epochs\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:18 INFO 139741071263552] #quality_metric: host=algo-1, epoch=104, train loss <loss>=0.490272817016\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:18 INFO 139741071263552] loss did not improve\u001b[0m\n",
      "\n",
      "2021-03-06 21:22:32 Uploading - Uploading generated training model\u001b[34m[03/06/2021 21:22:19 INFO 139741071263552] Epoch[105] Batch[0] avg_epoch_loss=0.537737\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:19 INFO 139741071263552] #quality_metric: host=algo-1, epoch=105, batch=0 train loss <loss>=0.537736833096\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:20 INFO 139741071263552] Epoch[105] Batch[5] avg_epoch_loss=0.377882\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:20 INFO 139741071263552] #quality_metric: host=algo-1, epoch=105, batch=5 train loss <loss>=0.377881650502\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:20 INFO 139741071263552] Epoch[105] Batch [5]#011Speed: 2966.84 samples/sec#011loss=0.377882\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:20 INFO 139741071263552] processed a total of 1274 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2009.5579624176025, \"sum\": 2009.5579624176025, \"min\": 2009.5579624176025}}, \"EndTime\": 1615065740.175471, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1615065738.165432}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:20 INFO 139741071263552] #throughput_metric: host=algo-1, train throughput=633.913184433 records/second\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:20 INFO 139741071263552] #progress_metric: host=algo-1, completed 53 % of epochs\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:20 INFO 139741071263552] #quality_metric: host=algo-1, epoch=105, train loss <loss>=0.42403565906\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:20 INFO 139741071263552] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:21 INFO 139741071263552] Epoch[106] Batch[0] avg_epoch_loss=0.291051\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:21 INFO 139741071263552] #quality_metric: host=algo-1, epoch=106, batch=0 train loss <loss>=0.291051089764\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:21 INFO 139741071263552] Epoch[106] Batch[5] avg_epoch_loss=0.275968\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:21 INFO 139741071263552] #quality_metric: host=algo-1, epoch=106, batch=5 train loss <loss>=0.275968156755\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:21 INFO 139741071263552] Epoch[106] Batch [5]#011Speed: 3166.77 samples/sec#011loss=0.275968\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:22 INFO 139741071263552] Epoch[106] Batch[10] avg_epoch_loss=0.305231\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:22 INFO 139741071263552] #quality_metric: host=algo-1, epoch=106, batch=10 train loss <loss>=0.340346771479\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:22 INFO 139741071263552] Epoch[106] Batch [10]#011Speed: 2960.76 samples/sec#011loss=0.340347\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:22 INFO 139741071263552] processed a total of 1304 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2024.134874343872, \"sum\": 2024.134874343872, \"min\": 2024.134874343872}}, \"EndTime\": 1615065742.200326, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1615065740.175569}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:22 INFO 139741071263552] #throughput_metric: host=algo-1, train throughput=644.186616483 records/second\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:22 INFO 139741071263552] #progress_metric: host=algo-1, completed 53 % of epochs\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:22 INFO 139741071263552] #quality_metric: host=algo-1, epoch=106, train loss <loss>=0.305231163448\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:22 INFO 139741071263552] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:23 INFO 139741071263552] Epoch[107] Batch[0] avg_epoch_loss=0.599583\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:23 INFO 139741071263552] #quality_metric: host=algo-1, epoch=107, batch=0 train loss <loss>=0.599582910538\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:24 INFO 139741071263552] Epoch[107] Batch[5] avg_epoch_loss=0.423422\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:24 INFO 139741071263552] #quality_metric: host=algo-1, epoch=107, batch=5 train loss <loss>=0.423422334095\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:24 INFO 139741071263552] Epoch[107] Batch [5]#011Speed: 3173.80 samples/sec#011loss=0.423422\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:24 INFO 139741071263552] Epoch[107] Batch[10] avg_epoch_loss=0.539000\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:24 INFO 139741071263552] #quality_metric: host=algo-1, epoch=107, batch=10 train loss <loss>=0.67769382\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:24 INFO 139741071263552] Epoch[107] Batch [10]#011Speed: 2913.94 samples/sec#011loss=0.677694\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:24 INFO 139741071263552] processed a total of 1343 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2112.776041030884, \"sum\": 2112.776041030884, \"min\": 2112.776041030884}}, \"EndTime\": 1615065744.313641, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1615065742.200411}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:24 INFO 139741071263552] #throughput_metric: host=algo-1, train throughput=635.615412382 records/second\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:24 INFO 139741071263552] #progress_metric: host=algo-1, completed 54 % of epochs\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:24 INFO 139741071263552] #quality_metric: host=algo-1, epoch=107, train loss <loss>=0.539000282233\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:24 INFO 139741071263552] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:25 INFO 139741071263552] Epoch[108] Batch[0] avg_epoch_loss=0.435427\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:25 INFO 139741071263552] #quality_metric: host=algo-1, epoch=108, batch=0 train loss <loss>=0.435426741838\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:26 INFO 139741071263552] Epoch[108] Batch[5] avg_epoch_loss=0.377416\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:26 INFO 139741071263552] #quality_metric: host=algo-1, epoch=108, batch=5 train loss <loss>=0.377416133881\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:26 INFO 139741071263552] Epoch[108] Batch [5]#011Speed: 3145.44 samples/sec#011loss=0.377416\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:26 INFO 139741071263552] Epoch[108] Batch[10] avg_epoch_loss=0.440257\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:26 INFO 139741071263552] #quality_metric: host=algo-1, epoch=108, batch=10 train loss <loss>=0.515665763617\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:26 INFO 139741071263552] Epoch[108] Batch [10]#011Speed: 2678.10 samples/sec#011loss=0.515666\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:26 INFO 139741071263552] processed a total of 1313 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2072.4329948425293, \"sum\": 2072.4329948425293, \"min\": 2072.4329948425293}}, \"EndTime\": 1615065746.386714, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1615065744.313734}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:26 INFO 139741071263552] #throughput_metric: host=algo-1, train throughput=633.515724637 records/second\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:26 INFO 139741071263552] #progress_metric: host=algo-1, completed 54 % of epochs\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:26 INFO 139741071263552] #quality_metric: host=algo-1, epoch=108, train loss <loss>=0.44025687467\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:26 INFO 139741071263552] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:28 INFO 139741071263552] Epoch[109] Batch[0] avg_epoch_loss=0.599237\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:28 INFO 139741071263552] #quality_metric: host=algo-1, epoch=109, batch=0 train loss <loss>=0.599237024784\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:28 INFO 139741071263552] Epoch[109] Batch[5] avg_epoch_loss=0.467049\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:28 INFO 139741071263552] #quality_metric: host=algo-1, epoch=109, batch=5 train loss <loss>=0.467048915724\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:28 INFO 139741071263552] Epoch[109] Batch [5]#011Speed: 3053.40 samples/sec#011loss=0.467049\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:28 INFO 139741071263552] Epoch[109] Batch[10] avg_epoch_loss=0.232220\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:28 INFO 139741071263552] #quality_metric: host=algo-1, epoch=109, batch=10 train loss <loss>=-0.0495755881071\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:28 INFO 139741071263552] Epoch[109] Batch [10]#011Speed: 2910.73 samples/sec#011loss=-0.049576\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:28 INFO 139741071263552] processed a total of 1309 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2097.8291034698486, \"sum\": 2097.8291034698486, \"min\": 2097.8291034698486}}, \"EndTime\": 1615065748.485077, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1615065746.386801}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:28 INFO 139741071263552] #throughput_metric: host=algo-1, train throughput=623.943063591 records/second\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:28 INFO 139741071263552] #progress_metric: host=algo-1, completed 55 % of epochs\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:28 INFO 139741071263552] #quality_metric: host=algo-1, epoch=109, train loss <loss>=0.232219595801\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:28 INFO 139741071263552] loss did not improve\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:28 INFO 139741071263552] Loading parameters from best epoch (69)\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.deserialize.time\": {\"count\": 1, \"max\": 3.628969192504883, \"sum\": 3.628969192504883, \"min\": 3.628969192504883}}, \"EndTime\": 1615065748.489382, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1615065748.485156}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:28 INFO 139741071263552] stopping training now\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:28 INFO 139741071263552] #progress_metric: host=algo-1, completed 100 % of epochs\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:28 INFO 139741071263552] Final loss: 0.14585493803 (occurred at epoch 69)\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:28 INFO 139741071263552] #quality_metric: host=algo-1, train final_loss <loss>=0.14585493803\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:28 INFO 139741071263552] Worker algo-1 finished training.\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:28 WARNING 139741071263552] wait_for_all_workers will not sync workers since the kv store is not running distributed\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:28 INFO 139741071263552] All workers finished. Serializing model for prediction.\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"get_graph.time\": {\"count\": 1, \"max\": 75.70409774780273, \"sum\": 75.70409774780273, \"min\": 75.70409774780273}}, \"EndTime\": 1615065748.565918, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1615065748.489429}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:28 INFO 139741071263552] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"finalize.time\": {\"count\": 1, \"max\": 99.1659164428711, \"sum\": 99.1659164428711, \"min\": 99.1659164428711}}, \"EndTime\": 1615065748.589341, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1615065748.565992}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:28 INFO 139741071263552] Serializing to /opt/ml/model/model_algo-1\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:28 INFO 139741071263552] Saved checkpoint to \"/opt/ml/model/model_algo-1-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"model.serialize.time\": {\"count\": 1, \"max\": 3.612041473388672, \"sum\": 3.612041473388672, \"min\": 3.612041473388672}}, \"EndTime\": 1615065748.593063, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1615065748.589407}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:28 INFO 139741071263552] Successfully serialized the model for prediction.\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:28 INFO 139741071263552] Evaluating model accuracy on testset using 100 samples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"model.bind.time\": {\"count\": 1, \"max\": 0.04100799560546875, \"sum\": 0.04100799560546875, \"min\": 0.04100799560546875}}, \"EndTime\": 1615065748.593841, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1615065748.593108}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"model.score.time\": {\"count\": 1, \"max\": 688.8108253479004, \"sum\": 688.8108253479004, \"min\": 688.8108253479004}}, \"EndTime\": 1615065749.282613, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1615065748.593905}\n",
      "\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:29 INFO 139741071263552] #test_score (algo-1, RMSE): 82.4032838319\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:29 INFO 139741071263552] #test_score (algo-1, mean_absolute_QuantileLoss): 1013.1339519924586\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:29 INFO 139741071263552] #test_score (algo-1, mean_wQuantileLoss): 0.030043778517367156\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:29 INFO 139741071263552] #test_score (algo-1, wQuantileLoss[0.1]): 0.015028442951873621\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:29 INFO 139741071263552] #test_score (algo-1, wQuantileLoss[0.2]): 0.027495335981630318\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:29 INFO 139741071263552] #test_score (algo-1, wQuantileLoss[0.3]): 0.03334740867351507\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:29 INFO 139741071263552] #test_score (algo-1, wQuantileLoss[0.4]): 0.0371939104800093\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:29 INFO 139741071263552] #test_score (algo-1, wQuantileLoss[0.5]): 0.03968796723079629\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:29 INFO 139741071263552] #test_score (algo-1, wQuantileLoss[0.6]): 0.03530292141120993\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:29 INFO 139741071263552] #test_score (algo-1, wQuantileLoss[0.7]): 0.03354169841327485\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:29 INFO 139741071263552] #test_score (algo-1, wQuantileLoss[0.8]): 0.029096829739544885\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:29 INFO 139741071263552] #test_score (algo-1, wQuantileLoss[0.9]): 0.019699491774450198\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:29 INFO 139741071263552] #quality_metric: host=algo-1, test mean_wQuantileLoss <loss>=0.0300437785174\u001b[0m\n",
      "\u001b[34m[03/06/2021 21:22:29 INFO 139741071263552] #quality_metric: host=algo-1, test RMSE <loss>=82.4032838319\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"totaltime\": {\"count\": 1, \"max\": 229245.45407295227, \"sum\": 229245.45407295227, \"min\": 229245.45407295227}, \"setuptime\": {\"count\": 1, \"max\": 9.654045104980469, \"sum\": 9.654045104980469, \"min\": 9.654045104980469}}, \"EndTime\": 1615065749.29666, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1615065749.282707}\n",
      "\u001b[0m\n",
      "\n",
      "2021-03-06 21:22:59 Completed - Training job completed\n",
      "ProfilerReport-1615065330: NoIssuesFound\n",
      "Training seconds: 269\n",
      "Billable seconds: 269\n",
      "CPU times: user 1.25 s, sys: 84.1 ms, total: 1.34 s\n",
      "Wall time: 7min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# train and test channels\n",
    "data_channels = {\n",
    "    \"train\": input_data_train,\n",
    "    \"test\": input_data_test\n",
    "}\n",
    "\n",
    "# fit the estimator\n",
    "estimator.fit(inputs=data_channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Predictions\n",
    "\n",
    "According to the [inference format](https://docs.aws.amazon.com/sagemaker/latest/dg/deepar-in-formats.html) for DeepAR, the `predictor` expects to see input data in a JSON format, with the following keys:\n",
    "* **instances**: A list of JSON-formatted time series that should be forecast by the model.\n",
    "* **configuration** (optional): A dictionary of configuration information for the type of response desired by the request.\n",
    "\n",
    "Within configuration the following keys can be configured:\n",
    "* **num_samples**: An integer specifying the number of samples that the model generates when making a probabilistic prediction.\n",
    "* **output_types**: A list specifying the type of response. We'll ask for **quantiles**, which look at the list of num_samples generated by the model, and generate [quantile estimates](https://en.wikipedia.org/wiki/Quantile) for each time point based on these values.\n",
    "* **quantiles**: A list that specified which quantiles estimates are generated and returned in the response.\n",
    "\n",
    "\n",
    "Below is an example of what a JSON query to a DeepAR model endpoint might look like.\n",
    "\n",
    "```\n",
    "{\n",
    " \"instances\": [\n",
    "  { \"start\": \"2009-11-01 00:00:00\", \"target\": [4.0, 10.0, 50.0, 100.0, 113.0] },\n",
    "  { \"start\": \"1999-01-30\", \"target\": [2.0, 1.0] }\n",
    " ],\n",
    " \"configuration\": {\n",
    "  \"num_samples\": 50,\n",
    "  \"output_types\": [\"quantiles\"],\n",
    "  \"quantiles\": [\"0.5\", \"0.9\"]\n",
    " }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JSON Prediction Request\n",
    "\n",
    "The code below accepts a **list** of time series as input and some configuration parameters. It then formats that series into a JSON instance and converts the input into an appropriately formatted JSON_input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_predictor_input(input_ts, num_samples=50, quantiles=['0.1', '0.5', '0.9']):\n",
    "    '''Accepts a list of input time series and produces a formatted input.\n",
    "       :input_ts: An list of input time series.\n",
    "       :num_samples: Number of samples to calculate metrics with.\n",
    "       :quantiles: A list of quantiles to return in the predicted output.\n",
    "       :return: The JSON-formatted input.\n",
    "       '''\n",
    "    # request data is made of JSON objects (instances)\n",
    "    # and an output configuration that details the type of data/quantiles we want\n",
    "    \n",
    "    instances = []\n",
    "    for k in range(len(input_ts)):\n",
    "        # get JSON objects for input time series\n",
    "        instances.append(series_to_json_obj(input_ts[k]))\n",
    "\n",
    "    # specify the output quantiles and samples\n",
    "    configuration = {\"num_samples\": num_samples, \n",
    "                     \"output_types\": [\"quantiles\"], \n",
    "                     \"quantiles\": quantiles}\n",
    "\n",
    "    request_data = {\"instances\": instances, \n",
    "                    \"configuration\": configuration}\n",
    "\n",
    "    json_request = json.dumps(request_data).encode('utf-8')\n",
    "    \n",
    "    return json_request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get a Prediction\n",
    "\n",
    "We can then use this function to get a prediction for a formatted time series!\n",
    "\n",
    "In the next cell, I'm getting an input time series and known target, and passing the formatted input into the predictor endpoint to get a resultant prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date\n",
       "2019-12-27    126.834549\n",
       "2019-12-30    124.527962\n",
       "2019-12-31    125.681244\n",
       "2020-01-02    126.975204\n",
       "2020-01-03    125.962540\n",
       "Name: Adj Close, dtype: float64"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ibm_valid['Adj Close'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all input and target (test) time series\n",
    "input_ts = df_ibm_valid['Adj Close']\n",
    "target_ts = time_series\n",
    "\n",
    "# get formatted input time series\n",
    "json_input_ts = json_predictor_input(input_ts)\n",
    "\n",
    "# get the prediction from the predictor\n",
    "json_prediction = predictor.predict(json_input_ts)\n",
    "\n",
    "#print(json_prediction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
