{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Stock Prices Predictions with Machine Learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Notebook will contain the modeling phases needed to predict stock prices using a deep learning model.\n",
    "The stocks analyzed will be the following:\n",
    "* IBM\n",
    "* AAPL (Apple Inc.)\n",
    "* AMZN (Amazon Inc.)\n",
    "* GOOGL (Alphabet Inc.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data must be prepared in order to be processed by DeepAR model:\n",
    "* Train/test set split\n",
    "* Save Data locally\n",
    "* Upload to S3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Data Locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'stock_deepar'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir_csv = os.path.join(data_dir, 'csv') # The folder we will use for storing data\n",
    "if not os.path.exists(data_dir_csv): # Make sure that the folder exists\n",
    "    os.makedirs(data_dir_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "interval ='D'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IBM\n",
    "df_ibm_train.to_csv(os.path.join(data_dir_csv, 'ibm_train.csv'), header=True, index=True)\n",
    "df_ibm_test.to_csv(os.path.join(data_dir_csv, 'ibm_test.csv'), header=True, index=True)\n",
    "df_ibm_valid.to_csv(os.path.join(data_dir_csv, 'ibm_valid.csv'), header=True, index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apple Inc.\n",
    "df_aapl_train.to_csv(os.path.join(data_dir_csv, 'aapl_train.csv'), header=True, index=True)\n",
    "df_aapl_test.to_csv(os.path.join(data_dir_csv, 'aapl_test.csv'), header=True, index=True)\n",
    "df_aapl_valid.to_csv(os.path.join(data_dir_csv, 'aapl_valid.csv'), header=True, index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Amazon.com\n",
    "df_amzn_train.to_csv(os.path.join(data_dir_csv, 'amzn_train.csv'), header=True, index=True)\n",
    "df_amzn_test.to_csv(os.path.join(data_dir_csv, 'amzn_test.csv'), header=True, index=True)\n",
    "df_amzn_valid.to_csv(os.path.join(data_dir_csv, 'amzn_valid.csv'), header=True, index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alphabet Inc.\n",
    "df_googl_train.to_csv(os.path.join(data_dir_csv, 'googl_train.csv'), header=True, index=True)\n",
    "df_googl_test.to_csv(os.path.join(data_dir_csv, 'googl_test.csv'), header=True, index=True)\n",
    "df_googl_valid.to_csv(os.path.join(data_dir_csv, 'googl_valid.csv'), header=True, index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSON serialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to feed DeepAR model, JSON files must be prepared from data.\n",
    "I'll dispose two kind of JSON inputs:\n",
    "* one with \"dynamic features\", to use a DeepAR API terminology: all dataset features except for target column and related one ('Adj Close', 'Close');\n",
    "* one without \"dynamic features: only 'Adj Close' column will be fed to DeepAR model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSON files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I'm going to convert data to JSON file format, in order to feed the DeepAR model correctly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As already announced, I will create two kind of time series, one with a list of dynamic features `dyn_feat`and the other one with only the target column (`Adj Close`) time series. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing train/test dataframe lists to iterate on them\n",
    "dfs_train = [df_ibm_train, df_aapl_train, df_amzn_train, df_googl_train]\n",
    "dfs_test = [df_ibm_test, df_aapl_test, df_amzn_test, df_googl_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating local storage path:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir_json = os.path.join(data_dir, 'json')\n",
    "if not os.path.exists(data_dir_json): # Make sure that the folder exists\n",
    "    os.makedirs(data_dir_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Serializing data to json files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "from source_deepar.deepar_utils import ts2json_serialize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset with the `Adj Close` time series alone:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir_json_train = os.path.join(data_dir_json, 'train') # The folder we will use for storing data\n",
    "if not os.path.exists(data_dir_json_train): # Make sure that the folder exists\n",
    "    os.makedirs(data_dir_json_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df, m in zip(dfs_train, mnemonics):\n",
    "    ts2json_serialize(df, data_dir_json_train, m+'.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir_json_test = os.path.join(data_dir_json, 'test') # The folder we will use for storing data\n",
    "if not os.path.exists(data_dir_json_test): # Make sure that the folder exists\n",
    "    os.makedirs(data_dir_json_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df, m in zip(dfs_test, mnemonics):\n",
    "    ts2json_serialize(df, data_dir_json_test, m+'.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset containing dynamic features:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir_json_dyn_feat = os.path.join(data_dir_json, 'w_dyn_feat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir_json_train_dyn_feat = os.path.join(data_dir_json_dyn_feat, 'train') # The folder we will use for storing data\n",
    "if not os.path.exists(data_dir_json_train_dyn_feat): # Make sure that the folder exists\n",
    "    os.makedirs(data_dir_json_train_dyn_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df, m in zip(dfs_train, mnemonics):\n",
    "    ts2json_serialize(df, data_dir_json_train_dyn_feat, m+'.json', dyn_feat=['Open'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir_json_test_dyn_feat = os.path.join(data_dir_json_dyn_feat, 'test') # The folder we will use for storing data\n",
    "if not os.path.exists(data_dir_json_test_dyn_feat): # Make sure that the folder exists\n",
    "    os.makedirs(data_dir_json_test_dyn_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df, m in zip(dfs_test, mnemonics):\n",
    "    ts2json_serialize(df, data_dir_json_test_dyn_feat, m+'.json', dyn_feat=['Open'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir_json_test_dyn_feat = os.path.join(data_dir_json_dyn_feat, 'test') # The folder we will use for storing data\n",
    "if not os.path.exists(data_dir_json_test_dyn_feat): # Make sure that the folder exists\n",
    "    os.makedirs(data_dir_json_test_dyn_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df, m in zip(dfs_test, mnemonics):\n",
    "    ts2json_serialize(df, data_dir_json_test_dyn_feat, m+'.json', dyn_feat=['Open'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uploading data to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "# Define IAM role and session\n",
    "role = sagemaker.get_execution_role()\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "#Define training data location\n",
    "s3_data_key = 'stock_deepar/train_artifacts'\n",
    "s3_bucket = sagemaker_session.default_bucket()\n",
    "interval = 'D' #Use D or H\n",
    "s3_output_path = \"s3://{}/{}/{}/output\".format(s3_bucket, s3_data_key, interval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *unique* train/test prefixes\n",
    "train_prefix   = '{}/{}'.format(data_dir_json, 'train')\n",
    "test_prefix    = '{}/{}'.format(data_dir_json, 'test')\n",
    "train_prefix_dyn_feat   = '{}/{}'.format(data_dir_json_dyn_feat, 'train')\n",
    "test_prefix_dyn_feat    = '{}/{}'.format(data_dir_json_dyn_feat, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_train = sagemaker_session.upload_data(path=data_dir_json_train, bucket=s3_bucket, key_prefix=train_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_test = sagemaker_session.upload_data(path=data_dir_json_test, bucket=s3_bucket, key_prefix=test_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_train_dyn_feat = sagemaker_session.upload_data(path=data_dir_json_train, bucket=s3_bucket, key_prefix=train_prefix_dyn_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_test_dyn_feat = sagemaker_session.upload_data(path=data_dir_json_test, bucket=s3_bucket, key_prefix=test_prefix_dyn_feat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set DeepAR specific hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "from source_deepar import deepar_utils\n",
    "\n",
    "# setting target columns\n",
    "target_column = 'Adj Close'\n",
    "    \n",
    "    \n",
    "hyperparameters = {\n",
    "    \"prediction_length\": str(prediction_length[1]), #number of time-steps model is trained to predict, always generates forecasts with this length\n",
    "    \"context_length\": str(context_length[1]), #number of time-points that the model gets to see before making the prediction, should be about same as the prediction_length\n",
    "    \"time_freq\": interval, #granularity of the time series in the dataset\n",
    "    \"epochs\": \"200\", #maximum number of passes over the training data\n",
    "    \"early_stopping_patience\": \"40\", #training stops when no progress is made within the specified number of epochs\n",
    "    \"num_layers\": \"2\", #number of hidden layers in the RNN, typically range from 1 to 4    \n",
    "    \"num_cells\": \"40\", #number of cells to use in each hidden layer of the RNN, typically range from 30 to 100\n",
    "    \"mini_batch_size\": \"128\", #size of mini-batches used during training, typically values range from 32 to 512\n",
    "    \"learning_rate\": \"1e-3\", #learning rate used in training. Typical values range from 1e-4 to 1e-1\n",
    "    \"dropout_rate\": \"0.1\", # dropout rate to use for regularization, typically less than 0.2. \n",
    "    \"likelihood\": \"gaussian\" #noise model used for uncertainty estimates - gaussian/beta/negative-binomial/student-T/deterministic-L1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The method get_image_uri has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "Defaulting to the only supported framework/algorithm version: 1. Ignoring framework/algorithm version: latest.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model will be trained using container image : 495149712605.dkr.ecr.eu-central-1.amazonaws.com/forecasting-deepar:1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define IAM role and session\n",
    "role = sagemaker.get_execution_role()\n",
    "session = sagemaker.Session()\n",
    "\n",
    "#Obtain container image URI for SageMaker-DeepAR algorithm, based on region\n",
    "region = session.boto_region_name\n",
    "image_name = sagemaker.amazon.amazon_estimator.get_image_uri(region, \"forecasting-deepar\", \"latest\")\n",
    "print(\"Model will be trained using container image : {}\".format(image_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimator Instantiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train_instance_count has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "train_instance_type has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.estimator import Estimator\n",
    "\n",
    "# instantiate a DeepAR estimator\n",
    "estimator = Estimator(image_uri=image_name,\n",
    "                      sagemaker_session=sagemaker_session,\n",
    "                      image_name=image_name,\n",
    "                      role=role,\n",
    "                      train_instance_count=1,\n",
    "                      train_instance_type='ml.c4.xlarge',\n",
    "                      output_path=s3_output_path,\n",
    "                      hyperparameters=hyperparameters\n",
    "                      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Job Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creation of a training job with stand alone time series (no dynamic features provided)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# train and test channels\n",
    "data_channels = {\n",
    "    \"train\": input_data_train,\n",
    "    \"test\": input_data_test\n",
    "}\n",
    "\n",
    "# fit the estimator\n",
    "estimator.fit(inputs=data_channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy and Create a Predictor\n",
    "\n",
    "Now that we have trained a model, we can use it to perform predictions by deploying it to a predictor endpoint.\n",
    "\n",
    "Remember to **delete the endpoint** at the end of this notebook. A cell at the very bottom of this notebook will be provided, but it is always good to keep, front-of-mind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name = 'DeepAR-ml-spp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a predictor\n",
    "\n",
    "from sagemaker.predictor import json_serializer, json_deserializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run it once, then update the endpoint\n",
    "%%time\n",
    "predictor = estimator.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.t2.medium',\n",
    "    endpoint_name=endpoint_name,\n",
    "    #content_type=\"application/json\" # specify that it will accept/produce JSON\n",
    "    update_endpoint=True,\n",
    "    serializer=json_serializer,\n",
    "    deserializer=json_deserializer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------!CPU times: user 281 ms, sys: 20 ms, total: 301 ms\n",
      "Wall time: 8min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# update an endpoint\n",
    "\n",
    "predictor = estimator.update_endpoint(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.t2.medium',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "from source_deepar.deepar_utils import DeepARPredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The class RealTimePredictor has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    }
   ],
   "source": [
    "json_predictor = DeepARPredictor(endpoint_name=endpoint_name, sagemaker_session=sagemaker_session)\n",
    "json_predictor.set_prediction_parameters(interval, prediction_length[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Predictions\n",
    "\n",
    "According to the [inference format](https://docs.aws.amazon.com/sagemaker/latest/dg/deepar-in-formats.html) for DeepAR, the `predictor` expects to see input data in a JSON format, with the following keys:\n",
    "* **instances**: A list of JSON-formatted time series that should be forecast by the model.\n",
    "* **configuration** (optional): A dictionary of configuration information for the type of response desired by the request.\n",
    "\n",
    "Within configuration the following keys can be configured:\n",
    "* **num_samples**: An integer specifying the number of samples that the model generates when making a probabilistic prediction.\n",
    "* **output_types**: A list specifying the type of response. We'll ask for **quantiles**, which look at the list of num_samples generated by the model, and generate [quantile estimates](https://en.wikipedia.org/wiki/Quantile) for each time point based on these values.\n",
    "* **quantiles**: A list that specified which quantiles estimates are generated and returned in the response.\n",
    "\n",
    "\n",
    "Below is an example of what a JSON query to a DeepAR model endpoint might look like.\n",
    "\n",
    "```\n",
    "{\n",
    " \"instances\": [\n",
    "  { \"start\": \"2009-11-01 00:00:00\", \"target\": [4.0, 10.0, 50.0, 100.0, 113.0] },\n",
    "  { \"start\": \"1999-01-30\", \"target\": [2.0, 1.0] }\n",
    " ],\n",
    " \"configuration\": {\n",
    "  \"num_samples\": 50,\n",
    "  \"output_types\": [\"quantiles\"],\n",
    "  \"quantiles\": [\"0.5\", \"0.9\"]\n",
    " }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JSON Prediction Request\n",
    "\n",
    "The code below accepts a **list** of time series as input and some configuration parameters. It then formats that series into a JSON instance and converts the input into an appropriately formatted JSON_input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "from source_deepar.deepar_utils import __series_to_json_obj\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_predictor_input(input_ts, target_column='Adj Close', dyn_feat=[], start=None, num_samples=50, quantiles=['0.1', '0.5', '0.9']):\n",
    "    '''Accepts a list of input time series and produces a formatted input.\n",
    "       :input_ts: An list of input time series.\n",
    "       :num_samples: Number of samples to calculate metrics with.\n",
    "       :quantiles: A list of quantiles to return in the predicted output.\n",
    "       :return: The JSON-formatted input.\n",
    "       '''\n",
    "    # request data is made of JSON objects (instances)\n",
    "    # and an output configuration that details the type of data/quantiles we want\n",
    "    \n",
    "    instances = []\n",
    "    for k in range(len(input_ts)):\n",
    "        # get JSON objects for input time series\n",
    "        instances.append(__series_to_json_obj(input_ts[k], target_column=target_column, dyn_feat=dyn_feat, start=start))\n",
    "\n",
    "    # specify the output quantiles and samples\n",
    "    '''configuration = {\"num_samples\": num_samples, \n",
    "                     \"output_types\": [\"quantiles\"], \n",
    "                     \"quantiles\": quantiles}\n",
    "\n",
    "    request_data = {\"instances\": instances, \n",
    "                    \"configuration\": configuration}\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    configuration = {\n",
    "            \"num_samples\": num_samples,\n",
    "            \"output_types\": [\"quantiles\"],\n",
    "            \"quantiles\": quantiles\n",
    "    }\n",
    "\n",
    "    request_data = {\n",
    "            \"instances\": [instances],\n",
    "            \"configuration\": configuration\n",
    "    }\n",
    "    \n",
    "    json_request = json.dumps(request_data)\n",
    "    \n",
    "    return json_request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get a Prediction\n",
    "\n",
    "We can then use this function to get a prediction for a formatted time series!\n",
    "\n",
    "In the next cell, I'm getting an input time series and known target, and passing the formatted input into the predictor endpoint to get a resultant prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all input and target (test) time series\n",
    "# input_ts = [df_ibm_train, df_aapl_train, df_amzn_train, df_googl_train]\n",
    "# target_ts = [df_ibm_test, df_aapl_test, df_amzn_test, df_googl_test]\n",
    "\n",
    "input_ts = [df_ibm_train]\n",
    "target_ts = [df_ibm_test]\n",
    "\n",
    "# get formatted input time series\n",
    "json_input_ts = json_predictor_input(input_ts)\n",
    "\n",
    "# get the prediction from the predictor\n",
    "json_prediction = predictor.predict(json_input_ts)\n",
    "\n",
    "#print(json_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting IBM Stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all input and target (test) time series\n",
    "# input_ts = [df_ibm_train, df_aapl_train, df_amzn_train, df_googl_train]\n",
    "# target_ts = [df_ibm_test, df_aapl_test, df_amzn_test, df_googl_test]\n",
    "\n",
    "input_ts = [df_ibm_train]\n",
    "target_ts = [df_ibm_test]\n",
    "\n",
    "# get the prediction from the predictor\n",
    "json_prediction = json_predictor.predict(input_ts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[                   0.1         0.9         0.5\n",
      "2021-01-07  121.835075  127.992470  125.059486\n",
      "2021-01-08  122.355553  128.646957  124.954491\n",
      "2021-01-09  119.964478  128.169891  124.520325\n",
      "2021-01-10  121.023933  128.718262  124.628288\n",
      "2021-01-11  119.816162  128.747482  124.592346\n",
      "2021-01-12  120.824181  129.297180  125.081215\n",
      "2021-01-13  119.372238  128.318588  124.544899\n",
      "2021-01-14  119.224167  128.399811  123.815605\n",
      "2021-01-15  118.401733  128.649597  123.272163\n",
      "2021-01-16  117.649498  128.446579  122.900276\n",
      "2021-01-17  119.322922  130.186584  123.523125\n",
      "2021-01-18  117.754303  128.946533  122.647331\n",
      "2021-01-19  118.098427  129.045151  123.872612\n",
      "2021-01-20  117.775459  129.269470  122.622276\n",
      "2021-01-21  116.197449  129.602325  123.174355\n",
      "2021-01-22  117.485069  129.664856  123.424995\n",
      "2021-01-23  116.223900  130.558914  123.748756\n",
      "2021-01-24  115.928848  130.304184  124.099792\n",
      "2021-01-25  116.006180  130.210938  123.257805\n",
      "2021-01-26  116.588120  130.897644  123.646027]\n"
     ]
    }
   ],
   "source": [
    "print(json_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ground truth:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date\n",
      "2021-01-07    127.289062\n",
      "2021-01-08    126.835121\n",
      "2021-01-11    126.884468\n",
      "2021-01-12    127.506165\n",
      "2021-01-13    125.246353\n",
      "2021-01-14    127.269318\n",
      "2021-01-15    126.696968\n",
      "2021-01-19    127.318665\n",
      "2021-01-20    128.364685\n",
      "2021-01-21    129.913971\n",
      "2021-01-22    117.045937\n",
      "2021-01-25    117.016335\n",
      "2021-01-26    120.874763\n",
      "2021-01-27    120.855034\n",
      "2021-01-28    118.496552\n",
      "2021-01-29    117.539337\n",
      "2021-02-01    118.950485\n",
      "2021-02-02    117.864990\n",
      "2021-02-03    117.549210\n",
      "2021-02-04    119.424149\n",
      "Name: Adj Close, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(df_ibm_test.iloc[-(prediction_length[1]):]['Adj Close'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting Apple Stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all input and target (test) time series\n",
    "# input_ts = [df_ibm_train, df_aapl_train, df_amzn_train, df_googl_train]\n",
    "# target_ts = [df_ibm_test, df_aapl_test, df_amzn_test, df_googl_test]\n",
    "\n",
    "input_ts = [df_aapl_train]\n",
    "target_ts = [df_aapl_test]\n",
    "\n",
    "# get the prediction from the predictor\n",
    "json_prediction = json_predictor.predict(input_ts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[                   0.1         0.9         0.5\n",
      "2021-01-07  125.093613  132.763382  129.110001\n",
      "2021-01-08  125.319176  133.149612  128.552902\n",
      "2021-01-09  123.105209  133.478683  128.995026\n",
      "2021-01-10  124.570038  134.422852  129.220093\n",
      "2021-01-11  123.412041  134.891541  129.760132\n",
      "2021-01-12  125.122711  136.418060  130.931061\n",
      "2021-01-13  123.706871  135.310989  130.589737\n",
      "2021-01-14  124.589447  136.117706  130.782318\n",
      "2021-01-15  123.423264  137.367249  130.409927\n",
      "2021-01-16  123.571732  138.207718  131.052139\n",
      "2021-01-17  125.403740  139.851944  131.624786\n",
      "2021-01-18  123.532669  139.180222  130.335739\n",
      "2021-01-19  124.197906  138.609848  131.049896\n",
      "2021-01-20  124.423309  140.104279  130.706589\n",
      "2021-01-21  122.195633  140.038956  131.790817\n",
      "2021-01-22  124.620186  140.600830  132.093918\n",
      "2021-01-23  122.096970  141.768021  132.417282\n",
      "2021-01-24  121.327774  141.081955  133.163788\n",
      "2021-01-25  122.702339  141.449966  132.317459\n",
      "2021-01-26  121.980942  141.936066  131.625092]\n"
     ]
    }
   ],
   "source": [
    "print(json_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ground truth:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date\n",
      "2021-01-07    130.724655\n",
      "2021-01-08    131.852966\n",
      "2021-01-11    128.787552\n",
      "2021-01-12    128.607819\n",
      "2021-01-13    130.694702\n",
      "2021-01-14    128.717667\n",
      "2021-01-15    126.950294\n",
      "2021-01-19    127.639267\n",
      "2021-01-20    131.832993\n",
      "2021-01-21    136.665771\n",
      "2021-01-22    138.862503\n",
      "2021-01-25    142.706757\n",
      "2021-01-26    142.946396\n",
      "2021-01-27    141.848038\n",
      "2021-01-28    136.885452\n",
      "2021-01-29    131.763107\n",
      "2021-02-01    133.939850\n",
      "2021-02-02    134.788589\n",
      "2021-02-03    133.740158\n",
      "2021-02-04    137.184998\n",
      "Name: Adj Close, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(df_aapl_test.iloc[-(prediction_length[1]):]['Adj Close'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting Amazon.com Stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all input and target (test) time series\n",
    "# input_ts = [df_ibm_train, df_aapl_train, df_amzn_train, df_googl_train]\n",
    "# target_ts = [df_ibm_test, df_aapl_test, df_amzn_test, df_googl_test]\n",
    "\n",
    "input_ts = [df_amzn_train]\n",
    "target_ts = [df_amzn_test]\n",
    "\n",
    "# get the prediction from the predictor\n",
    "json_prediction = json_predictor.predict(input_ts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[                    0.1          0.9          0.5\n",
      "2021-01-07  3094.219482  3265.937744  3186.451416\n",
      "2021-01-08  3091.069092  3266.278809  3184.170898\n",
      "2021-01-09  3084.891113  3272.152832  3176.452393\n",
      "2021-01-10  3108.677979  3284.607910  3177.629883\n",
      "2021-01-11  3093.076904  3285.515381  3173.991943\n",
      "2021-01-12  3102.929443  3308.193115  3195.529541\n",
      "2021-01-13  3092.083496  3309.724121  3194.941162\n",
      "2021-01-14  3103.580078  3332.495117  3209.557617\n",
      "2021-01-15  3094.831787  3315.429932  3202.861816\n",
      "2021-01-16  3106.655029  3335.878174  3203.782471\n",
      "2021-01-17  3081.158447  3334.591553  3210.318604\n",
      "2021-01-18  3087.839600  3347.077637  3209.628418\n",
      "2021-01-19  3084.959961  3369.511963  3210.942627\n",
      "2021-01-20  3106.575195  3355.424316  3228.671631\n",
      "2021-01-21  3113.171143  3378.174072  3235.158447\n",
      "2021-01-22  3131.904053  3404.039795  3229.553955\n",
      "2021-01-23  3125.084961  3356.024658  3236.537354\n",
      "2021-01-24  3119.146484  3369.903076  3231.877686\n",
      "2021-01-25  3096.519775  3349.702393  3219.257324\n",
      "2021-01-26  3132.143066  3350.390869  3249.314697]\n"
     ]
    }
   ],
   "source": [
    "print(json_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ground truth:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date\n",
      "2021-01-07    3162.159912\n",
      "2021-01-08    3182.699951\n",
      "2021-01-11    3114.209961\n",
      "2021-01-12    3120.830078\n",
      "2021-01-13    3165.889893\n",
      "2021-01-14    3127.469971\n",
      "2021-01-15    3104.250000\n",
      "2021-01-19    3120.760010\n",
      "2021-01-20    3263.379883\n",
      "2021-01-21    3306.989990\n",
      "2021-01-22    3292.229980\n",
      "2021-01-25    3294.000000\n",
      "2021-01-26    3326.129883\n",
      "2021-01-27    3232.580078\n",
      "2021-01-28    3237.620117\n",
      "2021-01-29    3206.199951\n",
      "2021-02-01    3342.879883\n",
      "2021-02-02    3380.000000\n",
      "2021-02-03    3312.530029\n",
      "2021-02-04    3331.000000\n",
      "Name: Adj Close, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(df_amzn_test.iloc[-(prediction_length[1]):]['Adj Close'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting Alphabet Inc. Stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all input and target (test) time series\n",
    "# input_ts = [df_ibm_train, df_aapl_train, df_amzn_train, df_googl_train]\n",
    "# target_ts = [df_ibm_test, df_aapl_test, df_amzn_test, df_googl_test]\n",
    "\n",
    "input_ts = [df_googl_train]\n",
    "target_ts = [df_googl_test]\n",
    "\n",
    "# get the prediction from the predictor\n",
    "json_prediction = json_predictor.predict(input_ts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[                    0.1          0.9          0.5\n",
      "2021-01-07  1675.028687  1786.291382  1732.208374\n",
      "2021-01-08  1668.661255  1782.718994  1733.668701\n",
      "2021-01-09  1677.693115  1785.289429  1730.733521\n",
      "2021-01-10  1684.309937  1793.493652  1733.876587\n",
      "2021-01-11  1673.302368  1784.088989  1732.772705\n",
      "2021-01-12  1666.367798  1798.504395  1736.104614\n",
      "2021-01-13  1673.352051  1815.874634  1725.143311\n",
      "2021-01-14  1668.871460  1817.390625  1741.362305\n",
      "2021-01-15  1667.125122  1802.829712  1738.277466\n",
      "2021-01-16  1668.462524  1815.500122  1737.886963\n",
      "2021-01-17  1685.492798  1819.410645  1749.690918\n",
      "2021-01-18  1677.694702  1810.656616  1745.091187\n",
      "2021-01-19  1680.247192  1837.997070  1773.494385\n",
      "2021-01-20  1692.805298  1827.459229  1767.529419\n",
      "2021-01-21  1698.426025  1845.270996  1760.774292\n",
      "2021-01-22  1707.861084  1840.332886  1764.793457\n",
      "2021-01-23  1715.499634  1863.902222  1773.113525\n",
      "2021-01-24  1719.811523  1857.586914  1782.431396\n",
      "2021-01-25  1714.488770  1855.269043  1781.574951\n",
      "2021-01-26  1721.883057  1860.849243  1785.219360]\n"
     ]
    }
   ],
   "source": [
    "print(json_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ground truth:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date\n",
      "2021-01-07    1774.339966\n",
      "2021-01-08    1797.829956\n",
      "2021-01-11    1756.290039\n",
      "2021-01-12    1737.430054\n",
      "2021-01-13    1747.250000\n",
      "2021-01-14    1730.920044\n",
      "2021-01-15    1727.619995\n",
      "2021-01-19    1784.469971\n",
      "2021-01-20    1880.069946\n",
      "2021-01-21    1884.150024\n",
      "2021-01-22    1892.560059\n",
      "2021-01-25    1894.280029\n",
      "2021-01-26    1907.949951\n",
      "2021-01-27    1818.939941\n",
      "2021-01-28    1853.199951\n",
      "2021-01-29    1827.359985\n",
      "2021-02-01    1893.069946\n",
      "2021-02-02    1919.119995\n",
      "2021-02-03    2058.879883\n",
      "2021-02-04    2053.629883\n",
      "Name: Adj Close, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(df_googl_test.iloc[-(prediction_length[1]):]['Adj Close'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get a Prediction\n",
    "\n",
    "We can then use this function to get a prediction for a formatted time series!\n",
    "\n",
    "In the next cell, I'm getting an input time series and known target, and passing the formatted input into the predictor endpoint to get a resultant prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date\n",
       "2019-12-27    126.834549\n",
       "2019-12-30    124.527962\n",
       "2019-12-31    125.681244\n",
       "2020-01-02    126.975204\n",
       "2020-01-03    125.962540\n",
       "Name: Adj Close, dtype: float64"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ibm_valid['Adj Close'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all input and target (test) time series\n",
    "input_ts = df_ibm_valid['Adj Close']\n",
    "target_ts = time_series\n",
    "\n",
    "# get formatted input time series\n",
    "json_input_ts = json_predictor_input(input_ts)\n",
    "\n",
    "# get the prediction from the predictor\n",
    "json_prediction = predictor.predict(json_input_ts)\n",
    "\n",
    "#print(json_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete the Endpoint\n",
    "\n",
    "Try your code out on different time series. You may want to tweak your DeepAR hyperparameters and see if you can improve the performance of this predictor.\n",
    "\n",
    "When you're done with evaluating the predictor (any predictor), make sure to delete the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: delete the endpoint\n",
    "predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
