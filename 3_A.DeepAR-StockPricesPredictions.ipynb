{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Stock Prices Predictions with DeepAR**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Notebook will contain the modeling phases needed to predict stock prices using a deep learning model.\n",
    "The stocks analyzed will be the following:\n",
    "* IBM\n",
    "* AAPL (Apple Inc.)\n",
    "* AMZN (Amazon Inc.)\n",
    "* GOOGL (Alphabet Inc.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data must be prepared in order to be processed by DeepAR model:\n",
    "* Train/test set split\n",
    "* Save Data locally\n",
    "* Upload to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'stock_deepar'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The folder we will be used to store csv data\n",
    "data_dir_csv = os.path.join(data_dir, 'csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The folder we will be used to store json data\n",
    "data_dir_json = os.path.join(data_dir, 'json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The folder we will be used to store training data in json format\n",
    "data_dir_json_train = os.path.join(data_dir_json, 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The folder we will be used to store test data in json format\n",
    "data_dir_json_test = os.path.join(data_dir_json, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The folder we will be used to store validation data in json format\n",
    "data_dir_json_valid = os.path.join(data_dir_json, 'validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing train/test dataframe lists to iterate on them\n",
    "dfs_train = [df_ibm_train, df_aapl_train, df_amzn_train, df_googl_train]\n",
    "dfs_test = [df_ibm_test, df_aapl_test, df_amzn_test, df_googl_test]\n",
    "dfs_valid = [df_ibm_valid, df_aapl_valid, df_amzn_valid, df_googl_valid]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Data Locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(data_dir_csv): # Make sure that the folder exists\n",
    "    os.makedirs(data_dir_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IBM\n",
    "df_ibm_train.to_csv(os.path.join(data_dir_csv, 'ibm_train.csv'), header=True, index=True)\n",
    "df_ibm_test.to_csv(os.path.join(data_dir_csv, 'ibm_test.csv'), header=True, index=True)\n",
    "df_ibm_valid.to_csv(os.path.join(data_dir_csv, 'ibm_valid.csv'), header=True, index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apple Inc.\n",
    "df_aapl_train.to_csv(os.path.join(data_dir_csv, 'aapl_train.csv'), header=True, index=True)\n",
    "df_aapl_test.to_csv(os.path.join(data_dir_csv, 'aapl_test.csv'), header=True, index=True)\n",
    "df_aapl_valid.to_csv(os.path.join(data_dir_csv, 'aapl_valid.csv'), header=True, index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Amazon.com\n",
    "df_amzn_train.to_csv(os.path.join(data_dir_csv, 'amzn_train.csv'), header=True, index=True)\n",
    "df_amzn_test.to_csv(os.path.join(data_dir_csv, 'amzn_test.csv'), header=True, index=True)\n",
    "df_amzn_valid.to_csv(os.path.join(data_dir_csv, 'amzn_valid.csv'), header=True, index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alphabet Inc.\n",
    "df_googl_train.to_csv(os.path.join(data_dir_csv, 'googl_train.csv'), header=True, index=True)\n",
    "df_googl_test.to_csv(os.path.join(data_dir_csv, 'googl_test.csv'), header=True, index=True)\n",
    "df_googl_valid.to_csv(os.path.join(data_dir_csv, 'googl_valid.csv'), header=True, index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSON serialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to feed DeepAR model, JSON files must be prepared from data.\n",
    "I'll dispose two kind of JSON inputs:\n",
    "* one with \"dynamic features\", to use a DeepAR API terminology: all dataset features except for target column and related one ('Adj Close', 'Close');\n",
    "* one without \"dynamic features: only 'Adj Close' column will be fed to DeepAR model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DataFrame to JSON conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I'm going to convert data to JSON file format, in order to feed the DeepAR model correctly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As already announced, I will create two kind of time series, one with a list of dynamic features `dyn_feat`and the other one with only the target column (`Adj Close`) time series. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating local storage path:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(data_dir_json): # Make sure that the folder exists\n",
    "    os.makedirs(data_dir_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Serializing data to json files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "from source_deepar.deepar_utils import ts2DeepARjson_serialize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset with the `Adj Close` time series alone:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(data_dir_json_train): # Make sure that the folder exists\n",
    "    os.makedirs(data_dir_json_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df, m in zip(dfs_train, mnemonics):\n",
    "    ts2DeepARjson_serialize(df, data_dir_json_train, m+'.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(data_dir_json_test): # Make sure that the folder exists\n",
    "    os.makedirs(data_dir_json_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df, m in zip(dfs_test, mnemonics):\n",
    "    ts2DeepARjson_serialize(df, data_dir_json_test, m+'.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(data_dir_json_valid): # Make sure that the folder exists\n",
    "    os.makedirs(data_dir_json_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df, m in zip(dfs_valid, mnemonics):\n",
    "    ts2DeepARjson_serialize(df, data_dir_json_valid, m+'.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AWS declarations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining training data Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define IAM role and session\n",
    "role = sagemaker.get_execution_role()\n",
    "sagemaker_session = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "interval ='D'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define training data location\n",
    "s3_data_key = 'train_artifacts'\n",
    "s3_bucket = sagemaker_session.default_bucket()\n",
    "s3_output_path = \"s3://{}/{}/{}/{}/output\".format(s3_bucket, data_dir, s3_data_key, interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model image : 495149712605.dkr.ecr.eu-central-1.amazonaws.com/forecasting-deepar:1\n"
     ]
    }
   ],
   "source": [
    "#Obtain container image URI for SageMaker-DeepAR algorithm, based on region\n",
    "region = sagemaker_session.boto_region_name\n",
    "image_name = sagemaker.image_uris.retrieve(\"forecasting-deepar\", region)\n",
    "print(\"Model image : {}\".format(image_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload data to S3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training input preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *unique* train/test prefixes\n",
    "train_prefix   = '{}/{}'.format(data_dir_json, 'train')\n",
    "test_prefix    = '{}/{}'.format(data_dir_json, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_train = sagemaker_session.upload_data(path=data_dir_json_train, bucket=s3_bucket, key_prefix=train_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_test = sagemaker_session.upload_data(path=data_dir_json_test, bucket=s3_bucket, key_prefix=test_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set DeepAR specific hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "from source_deepar import deepar_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting target columns\n",
    "target_column = 'Adj Close'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DeepAR estimator parameters    \n",
    "hyperparameters = {\n",
    "    \"prediction_length\": str(prediction_length[1]), #number of time-steps model is trained to predict, always generates forecasts with this length\n",
    "    \"context_length\": str(context_length[1]), #number of time-points that the model gets to see before making the prediction, should be about same as the prediction_length\n",
    "    \"time_freq\": interval, #granularity of the time series in the dataset\n",
    "    \"epochs\": \"200\", #maximum number of passes over the training data\n",
    "    \"early_stopping_patience\": \"40\", #training stops when no progress is made within the specified number of epochs\n",
    "    \"num_layers\": \"2\", #number of hidden layers in the RNN, typically range from 1 to 4    \n",
    "    \"num_cells\": \"40\", #number of cells to use in each hidden layer of the RNN, typically range from 30 to 100\n",
    "    \"mini_batch_size\": \"128\", #size of mini-batches used during training, typically values range from 32 to 512\n",
    "    \"learning_rate\": \"1e-3\", #learning rate used in training. Typical values range from 1e-4 to 1e-1\n",
    "    \"dropout_rate\": \"0.1\", # dropout rate to use for regularization, typically less than 0.2. \n",
    "    \"likelihood\": \"gaussian\" #noise model used for uncertainty estimates - gaussian/beta/negative-binomial/student-T/deterministic-L1\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimator Instantiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.estimator import Estimator\n",
    "\n",
    "# instantiate a DeepAR estimator\n",
    "estimator = Estimator(image_uri=image_name,\n",
    "                      sagemaker_session=sagemaker_session,\n",
    "                      #image_name=image_name,\n",
    "                      role=role,\n",
    "                      instance_count=1,\n",
    "                      instance_type='ml.c4.xlarge',\n",
    "                      output_path=s3_output_path,\n",
    "                      hyperparameters=hyperparameters\n",
    "                      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Job Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creation of a training job with stand alone time series (no dynamic features provided). Run only if no model has already been trained before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "# train and test channels\n",
    "data_channels = {\n",
    "    \"train\": input_data_train,\n",
    "    \"test\": input_data_test\n",
    "}\n",
    "\n",
    "# fit the estimator\n",
    "estimator.fit(inputs=data_channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Existing Model Instantiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instatiation of a model from existing training artifacts (run only if a model has already been trained before)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sagemaker.model.Model(\n",
    "    model_data='{}/{}/model.tar.gz'.format(s3_output_path, 'forecasting-deepar-2021-03-07-20-20-06-397/output'),\n",
    "    image_uri= image_name,\n",
    "    #image=image_name,  # example path for the semantic segmentation in eu-west-1\n",
    "    role=role)  # your role here; could be different name\n",
    "\n",
    "#trainedmodel.deploy(initial_instance_count=1, instance_type='ml.c4.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy and Create a Predictor\n",
    "\n",
    "Now that we have trained a model, we can use it to perform predictions by deploying it to a predictor endpoint.\n",
    "\n",
    "Remember to **delete the endpoint** at the end of this notebook. A cell at the very bottom of this notebook will be provided, but it is always good to keep, front-of-mind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name = 'DeepAR-ml-spp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a predictor\n",
    "\n",
    "from sagemaker.predictor import json_serializer, json_deserializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run it once, then update the endpoint if needed\n",
    "%time\n",
    "\n",
    "endpoint = model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.t2.medium',\n",
    "    endpoint_name=endpoint_name,\n",
    "    serializer=json_serializer,\n",
    "    deserializer=json_deserializer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### update endpoint if needed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "\n",
    "# update an endpoint\n",
    "\n",
    "predictor = estimator.update_endpoint(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.t2.medium',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Predictions\n",
    "\n",
    "According to the [inference format](https://docs.aws.amazon.com/sagemaker/latest/dg/deepar-in-formats.html) for DeepAR, the `predictor` expects to see input data in a JSON format, with the following keys:\n",
    "* **instances**: A list of JSON-formatted time series that should be forecast by the model.\n",
    "* **configuration** (optional): A dictionary of configuration information for the type of response desired by the request.\n",
    "\n",
    "Within configuration the following keys can be configured:\n",
    "* **num_samples**: An integer specifying the number of samples that the model generates when making a probabilistic prediction.\n",
    "* **output_types**: A list specifying the type of response. We'll ask for **quantiles**, which look at the list of num_samples generated by the model, and generate [quantile estimates](https://en.wikipedia.org/wiki/Quantile) for each time point based on these values.\n",
    "* **quantiles**: A list that specified which quantiles estimates are generated and returned in the response.\n",
    "\n",
    "\n",
    "Below is an example of what a JSON query to a DeepAR model endpoint might look like.\n",
    "\n",
    "```\n",
    "{\n",
    " \"instances\": [\n",
    "  { \"start\": \"2009-11-01 00:00:00\", \"target\": [4.0, 10.0, 50.0, 100.0, 113.0] },\n",
    "  { \"start\": \"1999-01-30\", \"target\": [2.0, 1.0] }\n",
    " ],\n",
    " \"configuration\": {\n",
    "  \"num_samples\": 50,\n",
    "  \"output_types\": [\"quantiles\"],\n",
    "  \"quantiles\": [\"0.5\", \"0.9\"]\n",
    " }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JSON Predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a predictor that accepts JSON as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "from source_deepar.deepar_utils import DeepARPredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_predictor = DeepARPredictor(endpoint_name=endpoint_name, sagemaker_session=sagemaker_session)\n",
    "json_predictor.set_prediction_parameters(interval, prediction_length[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Predictions\n",
    "\n",
    "We can now use the model to get a predictions for input time series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting IBM stock price"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ground truth:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_gt = df_ibm_test.iloc[-prediction_length[1]:]['Adj Close']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all input and target (test) time series\n",
    "# input_ts = [df_ibm_train, df_aapl_train, df_amzn_train, df_googl_train]\n",
    "# target_ts = [df_ibm_test, df_aapl_test, df_amzn_test, df_googl_test]\n",
    "\n",
    "input_ts = [df_ibm_train]\n",
    "#target_ts = [df_ibm]\n",
    "\n",
    "# get the prediction from the predictor\n",
    "json_prediction = json_predictor.predict(input_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0.1</th>\n",
       "      <th>0.5</th>\n",
       "      <th>0.9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021-01-22</th>\n",
       "      <td>125.714218</td>\n",
       "      <td>129.027008</td>\n",
       "      <td>132.164368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-01-23</th>\n",
       "      <td>125.244308</td>\n",
       "      <td>127.819000</td>\n",
       "      <td>132.071564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-01-24</th>\n",
       "      <td>125.774399</td>\n",
       "      <td>128.861832</td>\n",
       "      <td>132.511368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-01-25</th>\n",
       "      <td>125.350494</td>\n",
       "      <td>128.672256</td>\n",
       "      <td>134.421585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-01-26</th>\n",
       "      <td>124.133377</td>\n",
       "      <td>128.197189</td>\n",
       "      <td>133.200897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-01-27</th>\n",
       "      <td>122.965141</td>\n",
       "      <td>128.347977</td>\n",
       "      <td>134.279800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-01-28</th>\n",
       "      <td>124.080360</td>\n",
       "      <td>128.219116</td>\n",
       "      <td>133.931549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-01-29</th>\n",
       "      <td>122.933479</td>\n",
       "      <td>128.175446</td>\n",
       "      <td>132.553558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-01-30</th>\n",
       "      <td>123.798706</td>\n",
       "      <td>128.011948</td>\n",
       "      <td>134.314545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-01-31</th>\n",
       "      <td>123.482536</td>\n",
       "      <td>128.177979</td>\n",
       "      <td>134.009583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-01</th>\n",
       "      <td>122.868126</td>\n",
       "      <td>128.005920</td>\n",
       "      <td>133.146561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-02</th>\n",
       "      <td>123.081825</td>\n",
       "      <td>127.503738</td>\n",
       "      <td>133.631790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-03</th>\n",
       "      <td>123.101173</td>\n",
       "      <td>128.158554</td>\n",
       "      <td>133.804062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-04</th>\n",
       "      <td>121.966820</td>\n",
       "      <td>127.899025</td>\n",
       "      <td>133.881317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-05</th>\n",
       "      <td>121.586388</td>\n",
       "      <td>126.814400</td>\n",
       "      <td>132.328766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-06</th>\n",
       "      <td>121.484062</td>\n",
       "      <td>126.828659</td>\n",
       "      <td>133.207291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-07</th>\n",
       "      <td>121.823380</td>\n",
       "      <td>126.255951</td>\n",
       "      <td>134.051804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-08</th>\n",
       "      <td>119.063873</td>\n",
       "      <td>125.782776</td>\n",
       "      <td>133.071014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-09</th>\n",
       "      <td>118.382004</td>\n",
       "      <td>124.462196</td>\n",
       "      <td>130.457840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-10</th>\n",
       "      <td>118.046997</td>\n",
       "      <td>124.427368</td>\n",
       "      <td>130.561768</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0.1         0.5         0.9\n",
       "2021-01-22  125.714218  129.027008  132.164368\n",
       "2021-01-23  125.244308  127.819000  132.071564\n",
       "2021-01-24  125.774399  128.861832  132.511368\n",
       "2021-01-25  125.350494  128.672256  134.421585\n",
       "2021-01-26  124.133377  128.197189  133.200897\n",
       "2021-01-27  122.965141  128.347977  134.279800\n",
       "2021-01-28  124.080360  128.219116  133.931549\n",
       "2021-01-29  122.933479  128.175446  132.553558\n",
       "2021-01-30  123.798706  128.011948  134.314545\n",
       "2021-01-31  123.482536  128.177979  134.009583\n",
       "2021-02-01  122.868126  128.005920  133.146561\n",
       "2021-02-02  123.081825  127.503738  133.631790\n",
       "2021-02-03  123.101173  128.158554  133.804062\n",
       "2021-02-04  121.966820  127.899025  133.881317\n",
       "2021-02-05  121.586388  126.814400  132.328766\n",
       "2021-02-06  121.484062  126.828659  133.207291\n",
       "2021-02-07  121.823380  126.255951  134.051804\n",
       "2021-02-08  119.063873  125.782776  133.071014\n",
       "2021-02-09  118.382004  124.462196  130.457840\n",
       "2021-02-10  118.046997  124.427368  130.561768"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_prediction[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, index are just progressing of one day each row, wich is not stock price progression scheme in real life (e.g.: weekends are not trading days), so I'm going fix the index before going on with the analysis of results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_prediction = json_prediction[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_prediction.index = test_gt.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save data locally:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir_json_prediction = os.path.join(data_dir_json, 'prediction') # The folder we will use for storing data\n",
    "if not os.path.exists(data_dir_json_prediction): # Make sure that the folder exists\n",
    "    os.makedirs(data_dir_json_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = str(single_prediction.index[0].date())\n",
    "end_date = str(single_prediction.index[-1].date())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction serialization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_prediction.to_json(os.path.join(data_dir_json_prediction, \"IBM_{} - {}.json\".format(start_date, end_date)),\n",
    "                          orient='columns',date_format='iso')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction de-serialization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_single_prediction = pd.read_json(os.path.join(data_dir_json_prediction, \"IBM_{} - {}.json\".format(start_date, end_date)),\n",
    "                                   orient='columns', convert_axes=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, index normalization using target index, before using deserialized data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_single_prediction.index = test_gt.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting the Future -  validation data predictions\n",
    "\n",
    "Now that we've tested our estimator on test set, we would like to see how it behaves on new data.\n",
    "So we'll feed it with validation data we set apart before starting the training phase.\n",
    "Create a formatted input to send to the deployed `endpoint` passing usual parameters for \"configuration\". The \"instances\" will, in this case, just be one instance, defined by the following:\n",
    "* **start**: The start time from wich we would like to make a prediction.\n",
    "* **target**: The target will be an empty list because this time period has no, complete associated time series.\n",
    "```\n",
    "{\"start\": start_time, \"target\": []} # empty target\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_gt = df_ibm_valid['Adj Close']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requesting prediction for 2021-02-22 00:00:00\n"
     ]
    }
   ],
   "source": [
    "# formatting start_date\n",
    "start_time = str(df_ibm_valid.index[0])\n",
    "\n",
    "# formatting request_data\n",
    "# this instance has an empty target!\n",
    "request_data = {\"instances\": [{\"start\": start_time, \"target\": []}],\n",
    "                \"configuration\": {\"num_samples\": 50,\n",
    "                                  \"output_types\": [\"quantiles\"],\n",
    "                                  \"quantiles\": ['0.1', '0.5', '0.9']}\n",
    "                }\n",
    "\n",
    "input_ts = pd.DataFrame()\n",
    "\n",
    "print('Requesting prediction for '+start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieving predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get prediction response\n",
    "json_prediction = json_predictor.predict_future([df_ibm_valid.index[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_prediction = json_prediction[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_prediction.index = test_gt.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save data locally:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir_json_prediction = os.path.join(data_dir_json, 'prediction') # The folder we will use for storing data\n",
    "if not os.path.exists(data_dir_json_prediction): # Make sure that the folder exists\n",
    "    os.makedirs(data_dir_json_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = str(single_prediction.index[0].date())\n",
    "end_date = str(single_prediction.index[-1].date())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction serialization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_prediction.to_json(os.path.join(data_dir_json_prediction, \"IBM_valid{} - {}.json\".format(start_date, end_date)),\n",
    "                          orient='columns',date_format='iso')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction de-serialization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_single_prediction = pd.read_json(os.path.join(data_dir_json_prediction, \"IBM_valid{} - {}.json\".format(start_date, end_date)),\n",
    "                                   orient='columns', convert_axes=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, index normalization using target index, before using deserialized data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_single_prediction.index = test_gt.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0.1</th>\n",
       "      <th>0.5</th>\n",
       "      <th>0.9</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021-02-22</th>\n",
       "      <td>-0.368397</td>\n",
       "      <td>64.829224</td>\n",
       "      <td>126.120926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-23</th>\n",
       "      <td>20.297901</td>\n",
       "      <td>77.605095</td>\n",
       "      <td>155.923462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-24</th>\n",
       "      <td>59.721714</td>\n",
       "      <td>99.626595</td>\n",
       "      <td>144.338852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-25</th>\n",
       "      <td>69.641335</td>\n",
       "      <td>114.135666</td>\n",
       "      <td>155.885162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-26</th>\n",
       "      <td>83.316910</td>\n",
       "      <td>125.863640</td>\n",
       "      <td>170.498642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-03-01</th>\n",
       "      <td>104.771149</td>\n",
       "      <td>147.467026</td>\n",
       "      <td>191.276611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-03-02</th>\n",
       "      <td>127.716080</td>\n",
       "      <td>163.987610</td>\n",
       "      <td>196.198410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-03-03</th>\n",
       "      <td>121.278679</td>\n",
       "      <td>153.129974</td>\n",
       "      <td>182.344376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-03-04</th>\n",
       "      <td>129.271698</td>\n",
       "      <td>158.558273</td>\n",
       "      <td>189.969421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-03-05</th>\n",
       "      <td>130.038193</td>\n",
       "      <td>163.651077</td>\n",
       "      <td>184.936966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-03-08</th>\n",
       "      <td>140.005219</td>\n",
       "      <td>166.687897</td>\n",
       "      <td>189.988632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-03-09</th>\n",
       "      <td>141.058060</td>\n",
       "      <td>166.927155</td>\n",
       "      <td>184.173798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-03-10</th>\n",
       "      <td>144.655701</td>\n",
       "      <td>170.053558</td>\n",
       "      <td>186.051559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-03-11</th>\n",
       "      <td>150.162415</td>\n",
       "      <td>176.373398</td>\n",
       "      <td>190.311554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-03-12</th>\n",
       "      <td>154.298340</td>\n",
       "      <td>179.334885</td>\n",
       "      <td>197.960052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-03-15</th>\n",
       "      <td>161.404465</td>\n",
       "      <td>185.756943</td>\n",
       "      <td>207.628906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-03-16</th>\n",
       "      <td>169.427673</td>\n",
       "      <td>193.929291</td>\n",
       "      <td>210.613434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-03-17</th>\n",
       "      <td>179.375824</td>\n",
       "      <td>202.626007</td>\n",
       "      <td>216.220139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-03-18</th>\n",
       "      <td>188.700165</td>\n",
       "      <td>212.902908</td>\n",
       "      <td>225.105576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-03-19</th>\n",
       "      <td>200.757172</td>\n",
       "      <td>219.766052</td>\n",
       "      <td>233.025681</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0.1         0.5         0.9\n",
       "Date                                          \n",
       "2021-02-22   -0.368397   64.829224  126.120926\n",
       "2021-02-23   20.297901   77.605095  155.923462\n",
       "2021-02-24   59.721714   99.626595  144.338852\n",
       "2021-02-25   69.641335  114.135666  155.885162\n",
       "2021-02-26   83.316910  125.863640  170.498642\n",
       "2021-03-01  104.771149  147.467026  191.276611\n",
       "2021-03-02  127.716080  163.987610  196.198410\n",
       "2021-03-03  121.278679  153.129974  182.344376\n",
       "2021-03-04  129.271698  158.558273  189.969421\n",
       "2021-03-05  130.038193  163.651077  184.936966\n",
       "2021-03-08  140.005219  166.687897  189.988632\n",
       "2021-03-09  141.058060  166.927155  184.173798\n",
       "2021-03-10  144.655701  170.053558  186.051559\n",
       "2021-03-11  150.162415  176.373398  190.311554\n",
       "2021-03-12  154.298340  179.334885  197.960052\n",
       "2021-03-15  161.404465  185.756943  207.628906\n",
       "2021-03-16  169.427673  193.929291  210.613434\n",
       "2021-03-17  179.375824  202.626007  216.220139\n",
       "2021-03-18  188.700165  212.902908  225.105576\n",
       "2021-03-19  200.757172  219.766052  233.025681"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_single_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting IBM stock price"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ground truth:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_gt = df_ibm_valid.iloc[-prediction_length[1]:]['Adj Close']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all input and target (test) time series\n",
    "# input_ts = [df_ibm_train, df_aapl_train, df_amzn_train, df_googl_train]\n",
    "# target_ts = [df_ibm_test, df_aapl_test, df_amzn_test, df_googl_test]\n",
    "\n",
    "input_ts = [df_ibm_valid]\n",
    "target_ts = [df_ibm_valid]\n",
    "\n",
    "# get the prediction from the predictor\n",
    "json_prediction = json_predictor.predict(input_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_prediction[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, index are just progressing of one day each row, wich is not stock price progression scheme in real life (e.g.: weekends are not trading days), so I'm going fix the index before going on with the analysis of results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_prediction = json_prediction[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_prediction.index = test_gt.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save data locally:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir_json_prediction = os.path.join(data_dir_json, 'prediction') # The folder we will use for storing data\n",
    "if not os.path.exists(data_dir_json_prediction): # Make sure that the folder exists\n",
    "    os.makedirs(data_dir_json_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = str(single_prediction.index[0].date())\n",
    "end_date = str(single_prediction.index[-1].date())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction serialization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_prediction.to_json(os.path.join(data_dir_json_prediction, \"IBM_{} - {}_valid.json\".format(start_date, end_date)),\n",
    "                          orient='columns',date_format='iso')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction de-serialization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_single_prediction = pd.read_json(os.path.join(data_dir_json_prediction, \"IBM_{} - {}_valid.json\".format(start_date, end_date)),\n",
    "                                   orient='columns', convert_axes=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, index normalization using target index, before using deserialized data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_single_prediction.index = test_gt.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have predictions on validation dataset, we can compute our metrics and compare it to benchmakr model performaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IBM Stock prices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean Absolute Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ibm_dar_mae_loss = mean_absolute_error(test_gt, json_prediction[0]['0.5'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ibm_dar_mae_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Root Mean Squared Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ibm_dar_mse_loss = mean_squared_error(test_gt, json_prediction[0]['0.5'], squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ibm_dar_mse_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean Absolute Percentage Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ibm_dar_map_loss = mean_absolute_percentage_error(test_gt, json_prediction[0]['0.5'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ibm_dar_map_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R<sup>2</sup> score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ibm_dar_r2_score = r2_score(test_gt, json_prediction[0]['0.5'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ibm_dar_r2_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display the Results\n",
    "\n",
    "The quantile data will give us all we need to see the results of our prediction.\n",
    "* Quantiles 0.1 and 0.9 represent higher and lower bounds for the predicted values.\n",
    "* Quantile 0.5 represents the median of all sample predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the prediction median against the actual data\n",
    "def display_quantiles(prediction, prediction_length, target_ts=None):\n",
    "    \"\"\" show predictions for input time series \"\"\" \n",
    "    plt.figure(figsize=(12,6))\n",
    "    # get the target month of data\n",
    "    if target_ts is not None:\n",
    "        #target = target_ts[:]\n",
    "        #plt.plot(range(len(target)), target, label='target')\n",
    "        target_ts.plot()\n",
    "    # get the quantile values at 10 and 90%\n",
    "    p10 = prediction['0.1']\n",
    "    p90 = prediction['0.9']\n",
    "    # fill the 80% confidence interval\n",
    "    plt.fill_between(p10.index, p10, p90, color='y', alpha=0.5, label='80% confidence interval')\n",
    "    # plot the median prediction line\n",
    "    prediction['0.5'].plot(label='prediction median')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display predictions\n",
    "display_quantiles(d_single_prediction, prediction_length[1], test_gt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting Apple stock price"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ground truth:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_gt = df_aapl_test.iloc[-prediction_length[1]:]['Adj Close']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all input and target (test) time series\n",
    "# input_ts = [df_ibm_train, df_aapl_train, df_amzn_train, df_googl_train]\n",
    "# target_ts = [df_ibm_test, df_aapl_test, df_amzn_test, df_googl_test]\n",
    "\n",
    "input_ts = [df_aapl_train]\n",
    "target_ts = [df_aapl]\n",
    "\n",
    "# get the prediction from the predictor\n",
    "json_prediction = json_predictor.predict(input_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_prediction[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, index are just progressing of one day each row, wich is not stock price progression scheme in real life (e.g.: weekends are not trading days), so I'm going fix the index before going on with the analysis of results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_prediction = json_prediction[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_prediction.index = test_gt.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save data locally:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir_json_prediction = os.path.join(data_dir_json, 'prediction') # The folder we will use for storing data\n",
    "if not os.path.exists(data_dir_json_prediction): # Make sure that the folder exists\n",
    "    os.makedirs(data_dir_json_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = str(single_prediction.index[0].date())\n",
    "end_date = str(single_prediction.index[-1].date())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction serialization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_prediction.to_json(os.path.join(data_dir_json_prediction, \"AAPL_{} - {}.json\".format(start_date, end_date)),\n",
    "                          orient='columns',date_format='iso')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction de-serialization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_single_prediction = pd.read_json(os.path.join(data_dir_json_prediction, \"AAPL_{} - {}.json\".format(start_date, end_date)),\n",
    "                                   orient='columns', convert_axes=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simplifying date string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_single_prediction.index = d_single_prediction.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, index normalization using target index, before using deserialized data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_single_prediction.index = test_gt.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metrics computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean Absolute Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aapl_dar_mae_loss = mean_absolute_error(test_gt, json_prediction[0]['0.5'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(aapl_dar_mae_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Root Mean Squared Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aapl_dar_mse_loss = mean_squared_error(test_gt, json_prediction[0]['0.5'], squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(aapl_dar_mse_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean Absolute Percentage Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aapl_dar_map_loss = mean_absolute_percentage_error(test_gt, json_prediction[0]['0.5'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(aapl_dar_map_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R<sup>2</sup> score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aapl_dar_r2_score = r2_score(test_gt, json_prediction[0]['0.5'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(aapl_dar_r2_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display the Results\n",
    "\n",
    "The quantile data will give us all we need to see the results of our prediction.\n",
    "* Quantiles 0.1 and 0.9 represent higher and lower bounds for the predicted values.\n",
    "* Quantile 0.5 represents the median of all sample predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display predictions\n",
    "display_quantiles(d_single_prediction, prediction_length[1], test_gt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting Amazon stock price"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ground truth:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_gt = df_amzn_test.iloc[-prediction_length[1]:]['Adj Close']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all input and target (test) time series\n",
    "# input_ts = [df_ibm_train, df_aapl_train, df_amzn_train, df_googl_train]\n",
    "# target_ts = [df_ibm_test, df_aapl_test, df_amzn_test, df_googl_test]\n",
    "\n",
    "input_ts = [df_amzn_train]\n",
    "target_ts = [df_amzn]\n",
    "\n",
    "# get the prediction from the predictor\n",
    "json_prediction = json_predictor.predict(input_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_prediction[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, index are just progressing of one day each row, wich is not stock price progression scheme in real life (e.g.: weekends are not trading days), so I'm going fix the index before going on with the analysis of results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_prediction = json_prediction[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_prediction.index = test_gt.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save data locally:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir_json_prediction = os.path.join(data_dir_json, 'prediction') # The folder we will use for storing data\n",
    "if not os.path.exists(data_dir_json_prediction): # Make sure that the folder exists\n",
    "    os.makedirs(data_dir_json_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = str(single_prediction.index[0].date())\n",
    "end_date = str(single_prediction.index[-1].date())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction serialization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_prediction.to_json(os.path.join(data_dir_json_prediction, \"AMZN_{} - {}.json\".format(start_date, end_date)),\n",
    "                          orient='columns',date_format='iso')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction de-serialization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_single_prediction = pd.read_json(os.path.join(data_dir_json_prediction, \"AMZN_{} - {}.json\".format(start_date, end_date)),\n",
    "                                   orient='columns', convert_axes=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, index normalization using target index, before using deserialized data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_single_prediction.index = test_gt.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metrics computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean Absolute Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amzn_dar_mae_loss = mean_absolute_error(test_gt, json_prediction[0]['0.5'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(amzn_dar_mae_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Root Mean Squared Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amzn_dar_mse_loss = mean_squared_error(test_gt, json_prediction[0]['0.5'], squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(amzn_dar_mse_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean Absolute Percentage Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amzn_dar_map_loss = mean_absolute_percentage_error(test_gt, json_prediction[0]['0.5'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(amzn_dar_map_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R<sup>2</sup> score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amzn_dar_r2_score = r2_score(test_gt, json_prediction[0]['0.5'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(amzn_dar_r2_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display the Results\n",
    "\n",
    "The quantile data will give us all we need to see the results of our prediction.\n",
    "* Quantiles 0.1 and 0.9 represent higher and lower bounds for the predicted values.\n",
    "* Quantile 0.5 represents the median of all sample predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display predictions\n",
    "display_quantiles(d_single_prediction, prediction_length[1], test_gt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting Alphabet stock price"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ground truth:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_gt = df_googl_test.iloc[-prediction_length[1]:]['Adj Close']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all input and target (test) time series\n",
    "# input_ts = [df_ibm_train, df_aapl_train, df_amzn_train, df_googl_train]\n",
    "# target_ts = [df_ibm_test, df_aapl_test, df_amzn_test, df_googl_test]\n",
    "\n",
    "input_ts = [df_googl_train]\n",
    "target_ts = [df_googl]\n",
    "\n",
    "# get the prediction from the predictor\n",
    "json_prediction = json_predictor.predict(input_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_prediction[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, index are just progressing of one day each row, wich is not stock price progression scheme in real life (e.g.: weekends are not trading days), so I'm going fix the index before going on with the analysis of results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_prediction = json_prediction[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_prediction.index = test_gt.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save data locally:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir_json_prediction = os.path.join(data_dir_json, 'prediction') # The folder we will use for storing data\n",
    "if not os.path.exists(data_dir_json_prediction): # Make sure that the folder exists\n",
    "    os.makedirs(data_dir_json_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = str(single_prediction.index[0].date())\n",
    "end_date = str(single_prediction.index[-1].date())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction serialization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_prediction.to_json(os.path.join(data_dir_json_prediction, \"GOOGL_{} - {}.json\".format(start_date, end_date)),\n",
    "                          orient='columns',date_format='iso')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction de-serialization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_single_prediction = pd.read_json(os.path.join(data_dir_json_prediction, \"GOOGL_{} - {}.json\".format(start_date, end_date)),\n",
    "                                   orient='columns', convert_axes=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, index normalization using target index, before using deserialized data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_single_prediction.index = test_gt.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metrics computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean Absolute Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "googl_dar_mae_loss = mean_absolute_error(test_gt, json_prediction[0]['0.5'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(googl_dar_mae_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Root Mean Squared Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "googl_ma_mse_loss = mean_squared_error(test_gt, json_prediction[0]['0.5'], squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(googl_ma_mse_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean Absolute Percentage Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "googl_dar_map_loss = mean_absolute_percentage_error(test_gt, json_prediction[0]['0.5'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(googl_dar_map_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R<sup>2</sup> score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "googl_dar_r2_score = r2_score(test_gt, json_prediction[0]['0.5'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(googl_dar_r2_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display the Results\n",
    "\n",
    "The quantile data will give us all we need to see the results of our prediction.\n",
    "* Quantiles 0.1 and 0.9 represent higher and lower bounds for the predicted values.\n",
    "* Quantile 0.5 represents the median of all sample predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display predictions\n",
    "display_quantiles(d_single_prediction, prediction_length[1], test_gt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete the Endpoint\n",
    "\n",
    "Try your code out on different time series. You may want to tweak your DeepAR hyperparameters and see if you can improve the performance of this predictor.\n",
    "\n",
    "When you're done with evaluating the predictor (any predictor), make sure to delete the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: delete the endpoint\n",
    "json_predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
